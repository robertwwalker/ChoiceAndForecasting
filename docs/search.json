[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "TLDR: The course is what I think of as the natural successor for Data Analysis, Modeling, and Decision Making that extends basic regression to a broader universe of data types and the particular troubles in forecasting data observed over time.\nIt relies on two texts that exist in both paper and digital [with free access] forms. The first half of the course is built around The Handbook of Regression Modeling in People Analytics: With Examples in R, Python, and Julia by Keith McNulty [Global Director of Talent Sciences at McKinsey and Company] that covers models for discrete data types [binary, ordered, nominal choices, counts of events, and survival/duration analysis].\nThe second half of the course relies on the excellent Forecasting, Principles and Practice, 3rd Edition, by Rob J. Hyndman and George Athanasopoulos of Monash University in Australia that is entirely supported by R libraries for time series problems.\nThe lectures will focus on intuition and the mathematical logic but the goal is to put the tools into practice. To this end, the expectations are weekly homework exercises to insure that we can actually do what we are presented but there are two key summary deliverables: a project employing a detailed application of choice models near the middle and a project in time series forecasting due at the end of the term. Both are to be presented at the end of the term on date to be arranged to make up for the cancellation the first week."
  },
  {
    "objectID": "about.html#instructor",
    "href": "about.html#instructor",
    "title": "About",
    "section": "Instructor",
    "text": "Instructor\nRobert W. Walker is Associate Professor of Quantitative Methods in the Atkinson Graduate School of Management at Willamette University. He earned a Ph. D. in political science from the University of Rochester in 2005 and has previously held teaching positions at Dartmouth College, Rice University, Texas A&M University, and Washington University in Saint Louis. His current research develops and applies semi-Markov processes to time-series, cross-section data in international relations and international/comparative political economy. He teaches courses in quantitative methods/applied statistics and microeconomic strategy and previously taught four iterations in the U. S. National Science Foundation funded Empirical Implications of Theoretical Models sequence at Washington University in Saint Louis. His work with Curt Signorino and Muhammet Bas was awarded the Miller Prize for the best article in Political Analysis in 2009."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Models of Choice and Forecasting",
    "section": "",
    "text": "R\n\n\n\n\n\n\n\nRobert W. Walker\n\n\nAug 29, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\nRobert W. Walker\n\n\nAug 26, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/week-1/index.html",
    "href": "posts/week-1/index.html",
    "title": "Week 1: R, Inference, and Regression: A Review",
    "section": "",
    "text": "The slides are here.\nOur first class meeting will focus on Chapters 1, 2, 3; I suspect we will leave Chapter 4 of Handbook of Regression Modeling in People Analytics for next time."
  },
  {
    "objectID": "posts/week-1/index.html#hypothesis-tests-and-confidence-intervals",
    "href": "posts/week-1/index.html#hypothesis-tests-and-confidence-intervals",
    "title": "Week 1: R, Inference, and Regression: A Review",
    "section": "Hypothesis Tests and Confidence Intervals",
    "text": "Hypothesis Tests and Confidence Intervals"
  },
  {
    "objectID": "posts/week-1/index.html#an-hypothesis-test",
    "href": "posts/week-1/index.html#an-hypothesis-test",
    "title": "Week 1: R, Inference, and Regression: A Review",
    "section": "An Hypothesis Test",
    "text": "An Hypothesis Test\nI will work with the speed variable. The hypothesis to advance is that 17 or greater is the true average speed. The alternative must then be that the average speed is less than 17. Knowing only the sample size, I can figure out what \\(t\\) must be to reject 17 or greater and conclude that the true average must be less with 90% probability. The sample mean would have to be at least qt(0.1, 49) standard errors below 17 to rule out a mean of 17 or greater. Now let’s see what we have. Let me skim the data for the relevant information.\n\nlibrary(skimr)\nskim(cars)\n\n\nData summary\n\n\nName\ncars\n\n\nNumber of rows\n50\n\n\nNumber of columns\n2\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nspeed\n0\n1\n15.40\n5.29\n4\n12\n15\n19\n25\n▂▅▇▇▃\n\n\ndist\n0\n1\n42.98\n25.77\n2\n26\n36\n56\n120\n▅▇▅▂▁\n\n\n\n\n\nDoing the math by hand, I get:\n\\[ t = \\frac{\\overline{x} - \\mu}{\\frac{s}{\\sqrt{n}}} = \\frac{15.4 - 17}{\\frac{5.29}{\\sqrt{50}}} = -2.14 \\]\nInterpreting the result, the sample mean is 2.14 standard errors below the hypothetical mean of 17. The probability of a sample mean of 15.4 [or smaller] given a true average of 17, this standard deviation and sample size is pt(-2.14, 49) = 0.0186798. Notice that probability is less than 0.1; thus with at least 90% confidence, the true mean is not 17 or greater and thus must be smaller. Assuming the hypothetical mean [17 or greater] is true, the likelihood of generating a sample mean of 15.4 is only 0.0187 and this is far less than the 10% permissible outside of 90% confidence. Indeed, any sample mean more than 1.299 standard errors below 17 would be too small to sustain the belief that the true mean is 17 or greater because qt(0.1, 49) is -1.299. Put in the original metric, any sample mean below 16.0285747 would require a rejection of the claim that the true mean is 17 or greater with 90% confidence."
  },
  {
    "objectID": "posts/week-1/index.html#the-confidence-interval",
    "href": "posts/week-1/index.html#the-confidence-interval",
    "title": "Week 1: R, Inference, and Regression: A Review",
    "section": "The Confidence Interval",
    "text": "The Confidence Interval\nThe confidence interval is always centered on the sample mean. Rearranging the equation above and solving for \\(\\mu\\) given the \\(t\\) above, we get\n\\[ \\mu = \\overline{x} - t(\\frac{s}{\\sqrt{n}}) = 15.4 - (-1.299*\\frac{5.29}{\\sqrt{50}}) = 16.37143 \\]\nWith 90% confidence, given this sample mean, the true value should be less than 16.37143."
  },
  {
    "objectID": "posts/week-1/index.html#the-native-t.test",
    "href": "posts/week-1/index.html#the-native-t.test",
    "title": "Week 1: R, Inference, and Regression: A Review",
    "section": "The native t.test",
    "text": "The native t.test\n\nt.test(cars$speed, conf.level = 0.9, alternative = \"less\", mu=17)\n\n\n    One Sample t-test\n\ndata:  cars$speed\nt = -2.1397, df = 49, p-value = 0.01869\nalternative hypothesis: true mean is less than 17\n90 percent confidence interval:\n     -Inf 16.37143\nsample estimates:\nmean of x \n     15.4"
  },
  {
    "objectID": "posts/week-1/index.html#simplifying",
    "href": "posts/week-1/index.html#simplifying",
    "title": "Week 1: R, Inference, and Regression: A Review",
    "section": "Simplifying?",
    "text": "Simplifying?\n\\[ t(\\frac{s}{\\sqrt{n}}) = \\overline{x} - \\mu \\] can lead to either:\n\\[  \\overline{x} - t(\\frac{s}{\\sqrt{n}}) = \\mu \\]\nor\n\\[ \\overline{x} = \\mu + t(\\frac{s}{\\sqrt{n}}) \\]\nSo a minus \\(t\\) will be below \\(\\mu\\) but above \\(\\overline{x}\\) and a positive \\(t\\) will be above \\(\\mu\\) but below \\(\\overline{x}\\).\n1. An hypothesis test given \\(\\mu\\) with an alternative that is less must then render an upper bound given \\(\\overline{x}\\).\n2. An hypothesis test given \\(\\mu\\) with an alternative that is greater must then render a lower bound given \\(\\overline{x}\\)."
  },
  {
    "objectID": "posts/week-1/index.html#a-graphical-representation",
    "href": "posts/week-1/index.html#a-graphical-representation",
    "title": "Week 1: R, Inference, and Regression: A Review",
    "section": "A graphical representation",
    "text": "A graphical representation\nGiven a sample size \\(n\\), some unknown constant \\(\\mu\\) and satisfaction of Lindeberg’s condition, the sampling distribution of the sample mean follows a \\(t\\) distribution with degrees of freedom \\(n-1\\). To render a graphical representation, let’s arbitrarily set n to 50, as in the above example. Here is a plot.\n\nplot(seq(-5,5, by=0.01), dt(seq(-5,5, by=0.01), df=49), xlab=expression(paste(\"x-bar -\",mu,\" (measured in std. errors of the mean)\", sep=\"\")), ylab=\"Density\", type=\"l\")\n\n\n\n\n\nInverting the scale transformation\nWe can now reverse the scale by the standard error of the mean. In the above example, it is 0.7478. Measured in miles per hour, we obtain:\n\nplot(seq(-5,5, by=0.01)*0.7478, dt(seq(-5,5, by=0.01), df=49), xlab=expression(paste(\"x-bar -\",mu,\" (measured in mph)\", sep=\"\")), ylab=\"Density\", type=\"l\")\n\n\n\n\nNow we will take the concrete example above.\n\n\nThe Hypothesis Test\nWe claim that the true mean is 17 or greater. Now we need center the distribution above as though the claim is true.\n\nplot(x=17+seq(-5,5, by=0.01)*0.7478, dt(seq(-5,5, by=0.01), df=49), xlab=expression(paste(mu,\" (measured in mph)\", sep=\"\")), ylab=\"Density\", type=\"l\")\nabline(v=17, col=\"red\")\npolygon(x = c(17+seq(0,5, by=0.01)*0.7478, 21), y = c(0, dt(seq(0,5, by=0.01), df=49)), col = \"red\")\n\n\n\n\nThe sample mean is estimated to be 15.4. How likely is that?\n\nplot(x=17+seq(-5,5, by=0.01)*0.7478, dt(seq(-5,5, by=0.01), df=49), xlab=expression(paste(\"x-bar -\",mu,\" (measured in mph)\", sep=\"\")), ylab=\"Density\", type=\"l\")\nabline(v=17, col=\"red\")\nabline(v=15.4, col=\"blue\")\npolygon(x = c(17+seq(0,5, by=0.01)*0.7478, 21), y = c(0, dt(seq(0,5, by=0.01), df=49)), col = \"red\")\npolygon(x = c(12, 17+seq(-5,-2.14, by=0.01)*0.7478), y = c(dt(seq(-5,-2.14, by=0.01), df=49), 0), col = \"blue\")\nabline(h=0, col=\"black\")\nabline(v=17 + qt(0.1, df=49)*0.7874, col=\"black\", lty=3)\n\n\n\n\nThe probability of seeing such a small sample mean if the true average is 17 is only 0.01869. The probability above the dotted black line is 0.9 with 0.1 below. WIth 90% confidence, anything below this would be sufficient evidence to reject the claim that the true average is 17 or above."
  },
  {
    "objectID": "posts/week-1/index.html#the-confidence-interval-1",
    "href": "posts/week-1/index.html#the-confidence-interval-1",
    "title": "Week 1: R, Inference, and Regression: A Review",
    "section": "The Confidence Interval",
    "text": "The Confidence Interval\nLet’s take the sample mean as the center and work out a confidence interval at 90%. It’s exactly the 16.37143 gives above.\n\nplot(x=15.4+seq(-5,5, by=0.01)*0.7478, dt(seq(-5,5, by=0.01), df=49), xlab=expression(paste(mu,\" | x-bar (measured in mph)\", sep=\"\")), ylab=\"Density\", type=\"l\")\nabline(v=15.4, col=\"blue\")\nabline(v=15.4 - qt(0.1, df=49)*0.7478, col=\"black\", lty=3)\npolygon(x = c(11, 15.4+seq(-5,1.3, by=0.01)*0.7478), y = c(dt(seq(-5,1.3, by=0.01), df=49), 0), col = \"blue\")\n\n\n\n\nAs an aside, 17 has exactly 0.01869 probability above it shown in orange.\n\nplot(x=15.4+seq(-5,5, by=0.01)*0.7478, dt(seq(-5,5, by=0.01), df=49), xlab=expression(paste(mu,\" | x-bar (measured in mph)\", sep=\"\")), ylab=\"Density\", type=\"l\")\nabline(v=15.4, col=\"blue\")\nabline(v=15.4 - qt(0.1, df=49)*0.7478, col=\"black\", lty=3)\npolygon(x = c(11, 15.4+seq(-5,1.3, by=0.01)*0.7478), y = c(dt(seq(-5,1.3, by=0.01), df=49), 0), col = \"blue\")\npolygon(x = c(15.4+seq(2.14,5, by=0.01)*0.7478, 17), y = c(dt(seq(2.14,5, by=0.01), df=49), 0), col = \"orange\")"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome to Models of Choice and Forecasting",
    "section": "",
    "text": "I hope you are ready! The syllabus can be found here.\nWe will work with two books.\nTo hit the ground running, you will probably need two key bits of software: R and RStudio. To wit,\nThe tentative reading plan is:"
  },
  {
    "objectID": "posts/welcome/index.html#people-analytics",
    "href": "posts/welcome/index.html#people-analytics",
    "title": "Welcome to Models of Choice and Forecasting",
    "section": "People Analytics",
    "text": "People Analytics\n\nWeeks 1 and 2: Review of Linear Models and Inferential Statistics [chapters 1-4]\nWeek 3: Binomial Logistic Regression [chapter 5]\nWeek 4: Ordered and Multinomial Logistic Regression [chapters 6 and 7]\nWeek 5: Hierarchical Data [chapter 8]\nWeek 6: Survival Analysis [chapter 9]\nWeek 7: Power Analysis: How much data do I need? and Review [chapter 10]"
  },
  {
    "objectID": "posts/welcome/index.html#forecasting",
    "href": "posts/welcome/index.html#forecasting",
    "title": "Welcome to Models of Choice and Forecasting",
    "section": "Forecasting",
    "text": "Forecasting\n\nWeeks 8 and 9: The Basics, Time as a Variable, and Decomposition [chapters 1-5]\nWeek 10: Judgemental Forecast and Regression\n[chapters 6 and 7]\nWeek 11: Exponential Smoothing and ARIMA\n[chapters 8 and 9]\nWeek 12: Dynamic Regression [chapter 10]\nWeek 13: Hierarchies, advanced forecasting and related issues\n[chapters 11-13]\nWeek 14: Presentations on a people analytics problem and a time series forecast Date TBA: We will have to reschedule this because it represents the cancelled class the first week."
  }
]