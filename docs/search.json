[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "TLDR: The course is what I think of as the natural successor for Data Analysis, Modeling, and Decision Making that extends basic regression to a broader universe of data types and the particular troubles in forecasting data observed over time.\nIt relies on two texts that exist in both paper and digital [with free access] forms. The first half of the course is built around The Handbook of Regression Modeling in People Analytics: With Examples in R, Python, and Julia by Keith McNulty [Global Director of Talent Sciences at McKinsey and Company] that covers models for discrete data types [binary, ordered, nominal choices, counts of events, and survival/duration analysis].\nThe second half of the course relies on the excellent Forecasting, Principles and Practice, 3rd Edition, by Rob J. Hyndman and George Athanasopoulos of Monash University in Australia that is entirely supported by R libraries for time series problems.\nThe lectures will focus on intuition and the mathematical logic but the goal is to put the tools into practice. To this end, the expectations are weekly homework exercises to insure that we can actually do what we are presented but there are two key summary deliverables: a project employing a detailed application of choice models near the middle and a project in time series forecasting due at the end of the term. Both are to be presented at the end of the term on date to be arranged to make up for the cancellation the first week."
  },
  {
    "objectID": "about.html#instructor",
    "href": "about.html#instructor",
    "title": "About",
    "section": "Instructor",
    "text": "Instructor\nRobert W. Walker is Associate Professor of Quantitative Methods in the Atkinson Graduate School of Management at Willamette University. He earned a Ph. D. in political science from the University of Rochester in 2005 and has previously held teaching positions at Dartmouth College, Rice University, Texas A&M University, and Washington University in Saint Louis. His current research develops and applies semi-Markov processes to time-series, cross-section data in international relations and international/comparative political economy. He teaches courses in quantitative methods/applied statistics and microeconomic strategy and previously taught four iterations in the U. S. National Science Foundation funded Empirical Implications of Theoretical Models sequence at Washington University in Saint Louis. His work with Curt Signorino and Muhammet Bas was awarded the Miller Prize for the best article in Political Analysis in 2009."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Models of Choice and Forecasting",
    "section": "",
    "text": "A Forecasting Example\n\n\n\nR\n\n\n\n\n\n\n\nRobert W. Walker\n\n\nDec 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 12: Models, Models, Models\n\n\n\nR\n\n\n\n\n\n\n\nRobert W. Walker\n\n\nNov 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 13: Models, Models, Models, part 2\n\n\n\nR\n\n\n\n\n\n\n\nRobert W. Walker\n\n\nNov 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 11: Models, Models, Models\n\n\n\nR\n\n\n\n\n\n\n\nRobert W. Walker\n\n\nNov 14, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 10: Finishing Features, the Toolbox and Evaluation\n\n\n\nR\n\n\n\n\n\n\n\nRobert W. Walker\n\n\nNov 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 9: Features and Decomposing Time\n\n\n\nR\n\n\n\n\n\n\n\nRobert W. Walker\n\n\nOct 31, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 8: Introducing Time\n\n\n\nR\n\n\n\n\n\n\n\nRobert W. Walker\n\n\nOct 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 7: Survival, Power, and Planning\n\n\n\nR\n\n\n\n\n\n\n\nRobert W. Walker\n\n\nOct 17, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 6: Measurement and Survival\n\n\n\nR\n\n\n\n\n\n\n\nRobert W. Walker\n\n\nOct 3, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 5: Hierarchical Data\n\n\n\nR\n\n\n\n\n\n\n\nRobert W. Walker\n\n\nSep 26, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 4: Ordered and Multinomial Logistic Regression\n\n\n\nR\n\n\n\n\n\n\n\nRobert W. Walker\n\n\nSep 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 3: Binomial Logistic Regression\n\n\n\nR\n\n\n\n\n\n\n\nRobert W. Walker\n\n\nSep 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Small Thread on Smart Prediction\n\n\n\nR\n\n\n\n\n\n\n\nRobert W. Walker\n\n\nSep 6, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 2: Inference and Regression: A Review\n\n\n\nR\n\n\n\n\n\n\n\nRobert W. Walker\n\n\nSep 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 1: R, Inference, and Regression: A Review\n\n\n\nR\n\n\n\n\n\n\n\nRobert W. Walker\n\n\nAug 29, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to Models of Choice and Forecasting\n\n\n\nnews\n\n\n\n\n\n\n\nRobert W. Walker\n\n\nAug 26, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/week-1/index.html",
    "href": "posts/week-1/index.html",
    "title": "Week 1: R, Inference, and Regression: A Review",
    "section": "",
    "text": "The slides are here. The first class video is available from youtube here\nOur first class meeting will focus on Chapters 1, 2, 3; I suspect we will leave Chapter 4 of Handbook of Regression Modeling in People Analytics for next time.\nUPDATE: We got through Chapters 1 and 2. 3 and 4 will come next meeting."
  },
  {
    "objectID": "posts/week-1/index.html#hypothesis-tests-and-confidence-intervals",
    "href": "posts/week-1/index.html#hypothesis-tests-and-confidence-intervals",
    "title": "Week 1: R, Inference, and Regression: A Review",
    "section": "Hypothesis Tests and Confidence Intervals",
    "text": "Hypothesis Tests and Confidence Intervals"
  },
  {
    "objectID": "posts/week-1/index.html#an-hypothesis-test",
    "href": "posts/week-1/index.html#an-hypothesis-test",
    "title": "Week 1: R, Inference, and Regression: A Review",
    "section": "An Hypothesis Test",
    "text": "An Hypothesis Test\nI will work with the speed variable. The hypothesis to advance is that 17 or greater is the true average speed. The alternative must then be that the average speed is less than 17. Knowing only the sample size, I can figure out what \\(t\\) must be to reject 17 or greater and conclude that the true average must be less with 90% probability. The sample mean would have to be at least qt(0.1, 49) standard errors below 17 to rule out a mean of 17 or greater. Now let’s see what we have. Let me skim the data for the relevant information.\n\nlibrary(skimr)\nskim(cars)\n\n\nData summary\n\n\nName\ncars\n\n\nNumber of rows\n50\n\n\nNumber of columns\n2\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nspeed\n0\n1\n15.40\n5.29\n4\n12\n15\n19\n25\n▂▅▇▇▃\n\n\ndist\n0\n1\n42.98\n25.77\n2\n26\n36\n56\n120\n▅▇▅▂▁\n\n\n\n\n\nDoing the math by hand, I get:\n\\[ t = \\frac{\\overline{x} - \\mu}{\\frac{s}{\\sqrt{n}}} = \\frac{15.4 - 17}{\\frac{5.29}{\\sqrt{50}}} = -2.14 \\]\nInterpreting the result, the sample mean is 2.14 standard errors below the hypothetical mean of 17. The probability of a sample mean of 15.4 [or smaller] given a true average of 17, this standard deviation and sample size is pt(-2.14, 49) = 0.0186798. Notice that probability is less than 0.1; thus with at least 90% confidence, the true mean is not 17 or greater and thus must be smaller. Assuming the hypothetical mean [17 or greater] is true, the likelihood of generating a sample mean of 15.4 is only 0.0187 and this is far less than the 10% permissible outside of 90% confidence. Indeed, any sample mean more than 1.299 standard errors below 17 would be too small to sustain the belief that the true mean is 17 or greater because qt(0.1, 49) is -1.299. Put in the original metric, any sample mean below 16.0285747 would require a rejection of the claim that the true mean is 17 or greater with 90% confidence."
  },
  {
    "objectID": "posts/week-1/index.html#the-confidence-interval",
    "href": "posts/week-1/index.html#the-confidence-interval",
    "title": "Week 1: R, Inference, and Regression: A Review",
    "section": "The Confidence Interval",
    "text": "The Confidence Interval\nThe confidence interval is always centered on the sample mean. Rearranging the equation above and solving for \\(\\mu\\) given the \\(t\\) above, we get\n\\[ \\mu = \\overline{x} - t(\\frac{s}{\\sqrt{n}}) = 15.4 - (-1.299*\\frac{5.29}{\\sqrt{50}}) = 16.37143 \\]\nWith 90% confidence, given this sample mean, the true value should be less than 16.37143."
  },
  {
    "objectID": "posts/week-1/index.html#the-native-t.test",
    "href": "posts/week-1/index.html#the-native-t.test",
    "title": "Week 1: R, Inference, and Regression: A Review",
    "section": "The native t.test",
    "text": "The native t.test\n\nt.test(cars$speed, conf.level = 0.9, alternative = \"less\", mu=17)\n\n\n    One Sample t-test\n\ndata:  cars$speed\nt = -2.1397, df = 49, p-value = 0.01869\nalternative hypothesis: true mean is less than 17\n90 percent confidence interval:\n     -Inf 16.37143\nsample estimates:\nmean of x \n     15.4"
  },
  {
    "objectID": "posts/week-1/index.html#simplifying",
    "href": "posts/week-1/index.html#simplifying",
    "title": "Week 1: R, Inference, and Regression: A Review",
    "section": "Simplifying?",
    "text": "Simplifying?\n\\[ t(\\frac{s}{\\sqrt{n}}) = \\overline{x} - \\mu \\] can lead to either:\n\\[  \\overline{x} - t(\\frac{s}{\\sqrt{n}}) = \\mu \\]\nor\n\\[ \\overline{x} = \\mu + t(\\frac{s}{\\sqrt{n}}) \\]\nSo a minus \\(t\\) will be below \\(\\mu\\) but above \\(\\overline{x}\\) and a positive \\(t\\) will be above \\(\\mu\\) but below \\(\\overline{x}\\).\n1. An hypothesis test given \\(\\mu\\) with an alternative that is less must then render an upper bound given \\(\\overline{x}\\).\n2. An hypothesis test given \\(\\mu\\) with an alternative that is greater must then render a lower bound given \\(\\overline{x}\\)."
  },
  {
    "objectID": "posts/week-1/index.html#a-graphical-representation",
    "href": "posts/week-1/index.html#a-graphical-representation",
    "title": "Week 1: R, Inference, and Regression: A Review",
    "section": "A graphical representation",
    "text": "A graphical representation\nGiven a sample size \\(n\\), some unknown constant \\(\\mu\\) and satisfaction of Lindeberg’s condition, the sampling distribution of the sample mean follows a \\(t\\) distribution with degrees of freedom \\(n-1\\). To render a graphical representation, let’s arbitrarily set n to 50, as in the above example. Here is a plot.\n\nplot(seq(-5,5, by=0.01), dt(seq(-5,5, by=0.01), df=49), xlab=expression(paste(\"x-bar -\",mu,\" (measured in std. errors of the mean)\", sep=\"\")), ylab=\"Density\", type=\"l\")\n\n\n\n\n\nInverting the scale transformation\nWe can now reverse the scale by the standard error of the mean. In the above example, it is 0.7478. Measured in miles per hour, we obtain:\n\nplot(seq(-5,5, by=0.01)*0.7478, dt(seq(-5,5, by=0.01), df=49), xlab=expression(paste(\"x-bar -\",mu,\" (measured in mph)\", sep=\"\")), ylab=\"Density\", type=\"l\")\n\n\n\n\nNow we will take the concrete example above.\n\n\nThe Hypothesis Test\nWe claim that the true mean is 17 or greater. Now we need center the distribution above as though the claim is true.\n\nplot(x=17+seq(-5,5, by=0.01)*0.7478, dt(seq(-5,5, by=0.01), df=49), xlab=expression(paste(mu,\" (measured in mph)\", sep=\"\")), ylab=\"Density\", type=\"l\")\nabline(v=17, col=\"red\")\npolygon(x = c(17+seq(0,5, by=0.01)*0.7478, 21), y = c(0, dt(seq(0,5, by=0.01), df=49)), col = \"red\")\n\n\n\n\nThe sample mean is estimated to be 15.4. How likely is that?\n\nplot(x=17+seq(-5,5, by=0.01)*0.7478, dt(seq(-5,5, by=0.01), df=49), xlab=expression(paste(\"x-bar -\",mu,\" (measured in mph)\", sep=\"\")), ylab=\"Density\", type=\"l\")\nabline(v=17, col=\"red\")\nabline(v=15.4, col=\"blue\")\npolygon(x = c(17+seq(0,5, by=0.01)*0.7478, 21), y = c(0, dt(seq(0,5, by=0.01), df=49)), col = \"red\")\npolygon(x = c(12, 17+seq(-5,-2.14, by=0.01)*0.7478), y = c(dt(seq(-5,-2.14, by=0.01), df=49), 0), col = \"blue\")\nabline(h=0, col=\"black\")\nabline(v=17 + qt(0.1, df=49)*0.7874, col=\"black\", lty=3)\n\n\n\n\nThe probability of seeing such a small sample mean if the true average is 17 is only 0.01869. The probability above the dotted black line is 0.9 with 0.1 below. WIth 90% confidence, anything below this would be sufficient evidence to reject the claim that the true average is 17 or above."
  },
  {
    "objectID": "posts/week-1/index.html#the-confidence-interval-1",
    "href": "posts/week-1/index.html#the-confidence-interval-1",
    "title": "Week 1: R, Inference, and Regression: A Review",
    "section": "The Confidence Interval",
    "text": "The Confidence Interval\nLet’s take the sample mean as the center and work out a confidence interval at 90%. It’s exactly the 16.37143 gives above.\n\nplot(x=15.4+seq(-5,5, by=0.01)*0.7478, dt(seq(-5,5, by=0.01), df=49), xlab=expression(paste(mu,\" | x-bar (measured in mph)\", sep=\"\")), ylab=\"Density\", type=\"l\")\nabline(v=15.4, col=\"blue\")\nabline(v=15.4 - qt(0.1, df=49)*0.7478, col=\"black\", lty=3)\npolygon(x = c(11, 15.4+seq(-5,1.3, by=0.01)*0.7478), y = c(dt(seq(-5,1.3, by=0.01), df=49), 0), col = \"blue\")\n\n\n\n\nAs an aside, 17 has exactly 0.01869 probability above it shown in orange.\n\nplot(x=15.4+seq(-5,5, by=0.01)*0.7478, dt(seq(-5,5, by=0.01), df=49), xlab=expression(paste(mu,\" | x-bar (measured in mph)\", sep=\"\")), ylab=\"Density\", type=\"l\")\nabline(v=15.4, col=\"blue\")\nabline(v=15.4 - qt(0.1, df=49)*0.7478, col=\"black\", lty=3)\npolygon(x = c(11, 15.4+seq(-5,1.3, by=0.01)*0.7478), y = c(dt(seq(-5,1.3, by=0.01), df=49), 0), col = \"blue\")\npolygon(x = c(15.4+seq(2.14,5, by=0.01)*0.7478, 17), y = c(dt(seq(2.14,5, by=0.01), df=49), 0), col = \"orange\")"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome to Models of Choice and Forecasting",
    "section": "",
    "text": "I hope you are ready! The syllabus can be found here.\nWe will work with two books.\nTo hit the ground running, you will probably need two key bits of software: R and RStudio. To wit,\nThe tentative reading plan is:"
  },
  {
    "objectID": "posts/welcome/index.html#people-analytics",
    "href": "posts/welcome/index.html#people-analytics",
    "title": "Welcome to Models of Choice and Forecasting",
    "section": "People Analytics",
    "text": "People Analytics\n\nWeeks 1 and 2: Review of Linear Models and Inferential Statistics [chapters 1-4]\nWeek 3: Binomial Logistic Regression [chapter 5]\nWeek 4: Ordered and Multinomial Logistic Regression [chapters 6 and 7]\nWeek 5: Hierarchical Data [chapter 8]\nWeek 6: Survival Analysis [chapter 9]\nWeek 7: Power Analysis: How much data do I need? and Review [chapter 10]"
  },
  {
    "objectID": "posts/welcome/index.html#forecasting",
    "href": "posts/welcome/index.html#forecasting",
    "title": "Welcome to Models of Choice and Forecasting",
    "section": "Forecasting",
    "text": "Forecasting\n\nWeeks 8 and 9: The Basics, Time as a Variable, and Decomposition [chapters 1-5]\nWeek 10: Judgemental Forecast and Regression\n[chapters 6 and 7]\nWeek 11: Exponential Smoothing and ARIMA\n[chapters 8 and 9]\nWeek 12: Dynamic Regression [chapter 10]\nWeek 13: Hierarchies, advanced forecasting and related issues\n[chapters 11-13]\nWeek 14: Presentations on a people analytics problem and a time series forecast Date TBA: We will have to reschedule this because it represents the cancelled class the first week."
  },
  {
    "objectID": "posts/week-2/index.html",
    "href": "posts/week-2/index.html",
    "title": "Week 2: Inference and Regression: A Review",
    "section": "",
    "text": "The slides are here.. The video is linked here.\nOur second class meeting will focus on Chapter 3; and Chapter 4 of Handbook of Regression Modeling in People Analytics for next time."
  },
  {
    "objectID": "posts/week-2/index.html#hypothesis-tests-and-confidence-intervals",
    "href": "posts/week-2/index.html#hypothesis-tests-and-confidence-intervals",
    "title": "Week 2: Inference and Regression: A Review",
    "section": "Hypothesis Tests and Confidence Intervals",
    "text": "Hypothesis Tests and Confidence Intervals\n\nSingle means with the cars data\nI will work with R’s internal dataset on cars: cars. There are two variables in the dataset, this is what they look like.\n\nplot(cars)"
  },
  {
    "objectID": "posts/week-2/index.html#an-hypothesis-test",
    "href": "posts/week-2/index.html#an-hypothesis-test",
    "title": "Week 2: Inference and Regression: A Review",
    "section": "An Hypothesis Test",
    "text": "An Hypothesis Test\nI will work with the speed variable. The hypothesis to advance is that 17 or greater is the true average speed. The alternative must then be that the average speed is less than 17. Knowing only the sample size, I can figure out what \\(t\\) must be to reject 17 or greater and conclude that the true average must be less with 90% probability. The sample mean would have to be at least qt(0.1, 49) standard errors below 17 to rule out a mean of 17 or greater. Now let’s see what we have. Let me skim the data for the relevant information.\n\nlibrary(skimr)\nskim(cars)\n\n\nData summary\n\n\nName\ncars\n\n\nNumber of rows\n50\n\n\nNumber of columns\n2\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nspeed\n0\n1\n15.40\n5.29\n4\n12\n15\n19\n25\n▂▅▇▇▃\n\n\ndist\n0\n1\n42.98\n25.77\n2\n26\n36\n56\n120\n▅▇▅▂▁\n\n\n\n\n\nDoing the math by hand, I get:\n\\[ t = \\frac{\\overline{x} - \\mu}{\\frac{s}{\\sqrt{n}}} = \\frac{15.4 - 17}{\\frac{5.29}{\\sqrt{50}}} = -2.14 \\]\nInterpreting the result, the sample mean is 2.14 standard errors below the hypothetical mean of 17. The probability of a sample mean of 15.4 [or smaller] given a true average of 17, this standard deviation and sample size is pt(-2.14, 49) = 0.0186798. Notice that probability is less than 0.1; thus with at least 90% confidence, the true mean is not 17 or greater and thus must be smaller. Assuming the hypothetical mean [17 or greater] is true, the likelihood of generating a sample mean of 15.4 is only 0.0187 and this is far less than the 10% permissible outside of 90% confidence. Indeed, any sample mean more than 1.299 standard errors below 17 would be too small to sustain the belief that the true mean is 17 or greater because qt(0.1, 49) is -1.299. Put in the original metric, any sample mean below 16.0285747 would require a rejection of the claim that the true mean is 17 or greater with 90% confidence."
  },
  {
    "objectID": "posts/week-2/index.html#the-confidence-interval",
    "href": "posts/week-2/index.html#the-confidence-interval",
    "title": "Week 2: Inference and Regression: A Review",
    "section": "The Confidence Interval",
    "text": "The Confidence Interval\nThe confidence interval is always centered on the sample mean. Rearranging the equation above and solving for \\(\\mu\\) given the \\(t\\) above, we get\n\\[ \\mu = \\overline{x} - t(\\frac{s}{\\sqrt{n}}) = 15.4 - (-1.299*\\frac{5.29}{\\sqrt{50}}) = 16.37143 \\]\nWith 90% confidence, given this sample mean, the true value should be less than 16.37143."
  },
  {
    "objectID": "posts/week-2/index.html#the-native-t.test",
    "href": "posts/week-2/index.html#the-native-t.test",
    "title": "Week 2: Inference and Regression: A Review",
    "section": "The native t.test",
    "text": "The native t.test\n\nt.test(cars$speed, conf.level = 0.9, alternative = \"less\", mu=17)\n\n\n    One Sample t-test\n\ndata:  cars$speed\nt = -2.1397, df = 49, p-value = 0.01869\nalternative hypothesis: true mean is less than 17\n90 percent confidence interval:\n     -Inf 16.37143\nsample estimates:\nmean of x \n     15.4"
  },
  {
    "objectID": "posts/week-2/index.html#simplifying",
    "href": "posts/week-2/index.html#simplifying",
    "title": "Week 2: Inference and Regression: A Review",
    "section": "Simplifying?",
    "text": "Simplifying?\n\\[ t(\\frac{s}{\\sqrt{n}}) = \\overline{x} - \\mu \\] can lead to either:\n\\[  \\overline{x} - t(\\frac{s}{\\sqrt{n}}) = \\mu \\]\nor\n\\[ \\overline{x} = \\mu + t(\\frac{s}{\\sqrt{n}}) \\]\nSo a minus \\(t\\) will be below \\(\\mu\\) but above \\(\\overline{x}\\) and a positive \\(t\\) will be above \\(\\mu\\) but below \\(\\overline{x}\\).\n1. An hypothesis test given \\(\\mu\\) with an alternative that is less must then render an upper bound given \\(\\overline{x}\\).\n2. An hypothesis test given \\(\\mu\\) with an alternative that is greater must then render a lower bound given \\(\\overline{x}\\)."
  },
  {
    "objectID": "posts/week-2/index.html#a-graphical-representation",
    "href": "posts/week-2/index.html#a-graphical-representation",
    "title": "Week 2: Inference and Regression: A Review",
    "section": "A graphical representation",
    "text": "A graphical representation\nGiven a sample size \\(n\\), some unknown constant \\(\\mu\\) and satisfaction of Lindeberg’s condition, the sampling distribution of the sample mean follows a \\(t\\) distribution with degrees of freedom \\(n-1\\). To render a graphical representation, let’s arbitrarily set n to 50, as in the above example. Here is a plot.\n\nplot(seq(-5,5, by=0.01), dt(seq(-5,5, by=0.01), df=49), xlab=expression(paste(\"x-bar -\",mu,\" (measured in std. errors of the mean)\", sep=\"\")), ylab=\"Density\", type=\"l\")\n\n\n\n\n\nInverting the scale transformation\nWe can now reverse the scale by the standard error of the mean. In the above example, it is 0.7478. Measured in miles per hour, we obtain:\n\nplot(seq(-5,5, by=0.01)*0.7478, dt(seq(-5,5, by=0.01), df=49), xlab=expression(paste(\"x-bar -\",mu,\" (measured in mph)\", sep=\"\")), ylab=\"Density\", type=\"l\")\n\n\n\n\nNow we will take the concrete example above.\n\n\nThe Hypothesis Test\nWe claim that the true mean is 17 or greater. Now we need center the distribution above as though the claim is true.\n\nplot(x=17+seq(-5,5, by=0.01)*0.7478, dt(seq(-5,5, by=0.01), df=49), xlab=expression(paste(mu,\" (measured in mph)\", sep=\"\")), ylab=\"Density\", type=\"l\")\nabline(v=17, col=\"red\")\npolygon(x = c(17+seq(0,5, by=0.01)*0.7478, 21), y = c(0, dt(seq(0,5, by=0.01), df=49)), col = \"red\")\n\n\n\n\nThe sample mean is estimated to be 15.4. How likely is that?\n\nplot(x=17+seq(-5,5, by=0.01)*0.7478, dt(seq(-5,5, by=0.01), df=49), xlab=expression(paste(\"x-bar -\",mu,\" (measured in mph)\", sep=\"\")), ylab=\"Density\", type=\"l\")\nabline(v=17, col=\"red\")\nabline(v=15.4, col=\"blue\")\npolygon(x = c(17+seq(0,5, by=0.01)*0.7478, 21), y = c(0, dt(seq(0,5, by=0.01), df=49)), col = \"red\")\npolygon(x = c(12, 17+seq(-5,-2.14, by=0.01)*0.7478), y = c(dt(seq(-5,-2.14, by=0.01), df=49), 0), col = \"blue\")\nabline(h=0, col=\"black\")\nabline(v=17 + qt(0.1, df=49)*0.7874, col=\"black\", lty=3)\n\n\n\n\nThe probability of seeing such a small sample mean if the true average is 17 is only 0.01869. The probability above the dotted black line is 0.9 with 0.1 below. WIth 90% confidence, anything below this would be sufficient evidence to reject the claim that the true average is 17 or above."
  },
  {
    "objectID": "posts/week-2/index.html#the-confidence-interval-1",
    "href": "posts/week-2/index.html#the-confidence-interval-1",
    "title": "Week 2: Inference and Regression: A Review",
    "section": "The Confidence Interval",
    "text": "The Confidence Interval\nLet’s take the sample mean as the center and work out a confidence interval at 90%. It’s exactly the 16.37143 gives above.\n\nplot(x=15.4+seq(-5,5, by=0.01)*0.7478, dt(seq(-5,5, by=0.01), df=49), xlab=expression(paste(mu,\" | x-bar (measured in mph)\", sep=\"\")), ylab=\"Density\", type=\"l\")\nabline(v=15.4, col=\"blue\")\nabline(v=15.4 - qt(0.1, df=49)*0.7478, col=\"black\", lty=3)\npolygon(x = c(11, 15.4+seq(-5,1.3, by=0.01)*0.7478), y = c(dt(seq(-5,1.3, by=0.01), df=49), 0), col = \"blue\")\n\n\n\n\nAs an aside, 17 has exactly 0.01869 probability above it shown in orange.\n\nplot(x=15.4+seq(-5,5, by=0.01)*0.7478, dt(seq(-5,5, by=0.01), df=49), xlab=expression(paste(mu,\" | x-bar (measured in mph)\", sep=\"\")), ylab=\"Density\", type=\"l\")\nabline(v=15.4, col=\"blue\")\nabline(v=15.4 - qt(0.1, df=49)*0.7478, col=\"black\", lty=3)\npolygon(x = c(11, 15.4+seq(-5,1.3, by=0.01)*0.7478), y = c(dt(seq(-5,1.3, by=0.01), df=49), 0), col = \"blue\")\npolygon(x = c(15.4+seq(2.14,5, by=0.01)*0.7478, 17), y = c(dt(seq(2.14,5, by=0.01), df=49), 0), col = \"orange\")"
  },
  {
    "objectID": "posts/week-2/index.html#a-visual-of-the-data",
    "href": "posts/week-2/index.html#a-visual-of-the-data",
    "title": "Week 2: Inference and Regression: A Review",
    "section": "A Visual of the Data",
    "text": "A Visual of the Data\n\nlibrary(GGally)\n\nLoading required package: ggplot2\n\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\n# display a pairplot of all four columns of data\nGGally::ggpairs(ugtests)\n\n\n\n\n\nmy.lm <- lm(Final ~ Yr1 + Yr2 + Yr3, data=ugtests)\nsummary(my.lm)\n\n\nCall:\nlm(formula = Final ~ Yr1 + Yr2 + Yr3, data = ugtests)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-92.638 -20.349   0.001  18.954  98.489 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 14.14599    5.48006   2.581  0.00999 ** \nYr1          0.07603    0.06538   1.163  0.24519    \nYr2          0.43129    0.03251  13.267  < 2e-16 ***\nYr3          0.86568    0.02914  29.710  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 30.43 on 971 degrees of freedom\nMultiple R-squared:  0.5303,    Adjusted R-squared:  0.5289 \nF-statistic: 365.5 on 3 and 971 DF,  p-value: < 2.2e-16\n\n\n\nconfint(my.lm)\n\n                  2.5 %     97.5 %\n(Intercept)  3.39187185 24.9001071\nYr1         -0.05227936  0.2043318\nYr2          0.36749170  0.4950791\nYr3          0.80850142  0.9228610\n\n\n\npredict(my.lm)\n\n        1         2         3         4         5         6         7         8 \n 82.77839 173.39734 159.84579 148.02242 115.77826 176.31713 168.52573 125.90895 \n        9        10        11        12        13        14        15        16 \n163.15331 133.00197 128.01391 168.83298 167.28471 178.01432 162.81842 183.81939 \n       17        18        19        20        21        22        23        24 \n143.96109 175.00126 148.29571 183.06708 157.92531 119.95460 165.18879 178.26106 \n       25        26        27        28        29        30        31        32 \n193.06715 166.52931 200.39004 182.94018 181.82752 104.19713 146.86195 158.59539 \n       33        34        35        36        37        38        39        40 \n137.30246 155.14171 147.59592 188.38693 149.46722 189.57199 165.36883 207.92058 \n       41        42        43        44        45        46        47        48 \n158.81869 145.13679 127.04145 184.89905 223.48248 190.68129 105.91152 188.47370 \n       49        50        51        52        53        54        55        56 \n169.91737 125.98673 179.91299 155.57364  94.78176 107.90209 142.01859 127.27405 \n       57        58        59        60        61        62        63        64 \n143.04734  90.43469 115.62032 186.36874 164.78997 182.04486 138.72937 173.80034 \n       65        66        67        68        69        70        71        72 \n162.81500 110.89761 157.53103 171.04054 164.29372 151.36275 203.73632 133.85978 \n       73        74        75        76        77        78        79        80 \n158.14239 145.03931 114.09836 136.39042 150.19917  99.56150 145.66277 116.39753 \n       81        82        83        84        85        86        87        88 \n124.72053 138.48918 167.84005 102.19417 187.51815 148.69592 199.82845 146.45894 \n       89        90        91        92        93        94        95        96 \n123.73077 134.25546 133.56188 171.39099 148.68036 160.00402 141.50056 143.96279 \n       97        98        99       100       101       102       103       104 \n126.22552 146.13759 160.82167 168.19393 139.15162 126.63646 142.87183 209.15707 \n      105       106       107       108       109       110       111       112 \n161.60200 169.65369 101.21716 132.64506 133.35245  98.58109 104.99919 122.47906 \n      113       114       115       116       117       118       119       120 \n132.50088 135.04371 169.26422 156.66084 201.79068 102.50589 222.36274 137.85639 \n      121       122       123       124       125       126       127       128 \n117.81825  92.61602 166.14124  90.81172 155.28613 221.52163 117.23941 125.32530 \n      129       130       131       132       133       134       135       136 \n154.28567 125.02463 149.23229 188.75433 167.65071 141.48812 148.73942 128.18946 \n      137       138       139       140       141       142       143       144 \n173.30410 113.50085 146.77068 144.70245 143.32800 172.42426 146.83111 161.80039 \n      145       146       147       148       149       150       151       152 \n155.74509 121.45958 120.19341 196.55029 107.52819 145.84903 129.87730  84.26017 \n      153       154       155       156       157       158       159       160 \n147.22201 172.38103 148.90671 191.37033  92.95881 113.93839 115.32834 217.95006 \n      161       162       163       164       165       166       167       168 \n109.04391 182.47442 173.66693 143.54644 213.24494 154.53040 157.99341 168.20918 \n      169       170       171       172       173       174       175       176 \n142.79439 111.22351 169.00364  95.59458 200.50023 150.77458 103.15936 136.32932 \n      177       178       179       180       181       182       183       184 \n152.35701 121.13371 116.24995 172.44290 105.74986 144.68854 142.16437 103.03841 \n      185       186       187       188       189       190       191       192 \n 84.20453 124.65866 175.16434 181.48777 183.17732 166.12881 109.68631 149.19871 \n      193       194       195       196       197       198       199       200 \n148.99240 123.27911 156.46977 111.37381 133.40676 105.70352 178.22203 162.98116 \n      201       202       203       204       205       206       207       208 \n 88.71579 142.38459 126.29081 195.54808 121.88463 175.31327 171.16320 128.80527 \n      209       210       211       212       213       214       215       216 \n157.38069 144.47601 145.67519 157.45049  90.83488 125.04501 117.56217 166.65784 \n      217       218       219       220       221       222       223       224 \n142.60365 151.28361 138.80714 152.33864 177.32217 135.09624 169.95920 161.33378 \n      225       226       227       228       229       230       231       232 \n150.99366 167.63206 102.71562 110.43832 159.52327 151.78922 158.52699 186.41680 \n      233       234       235       236       237       238       239       240 \n124.74060 118.60478 162.54510 198.55014 123.77119 209.05134 207.10887 110.39340 \n      241       242       243       244       245       246       247       248 \n139.38280 168.02915 166.40038 219.52660 144.11283 160.18067 182.99728 151.63440 \n      249       250       251       252       253       254       255       256 \n152.04702 175.37858 153.20573 193.75934 142.24807 204.76959 147.85201  91.68673 \n      257       258       259       260       261       262       263       264 \n178.87067 164.06869 167.01479 172.92703 123.77229 175.05272 117.60429 111.97751 \n      265       266       267       268       269       270       271       272 \n159.97603 166.63812 173.31198 207.68459 140.66875 135.57418 176.71423 170.00300 \n      273       274       275       276       277       278       279       280 \n170.04457 177.26620 133.02378 146.19662 166.87042 163.96474 156.38611 160.34511 \n      281       282       283       284       285       286       287       288 \n 71.51067 192.31939  89.27563 146.58132 139.44811 105.11565  79.53330 121.12299 \n      289       290       291       292       293       294       295       296 \n110.14355 178.41312 158.55532 181.01467 203.41524 207.36041 108.01030 173.06385 \n      297       298       299       300       301       302       303       304 \n146.09265 128.71230 116.85816 216.44539 169.53445 124.77337 164.95305 190.56794 \n      305       306       307       308       309       310       311       312 \n129.06273 142.52591  95.61497 171.09510 188.31374  93.51730 117.69757 121.30134 \n      313       314       315       316       317       318       319       320 \n178.57730 157.74528 170.68833 120.24799 116.37542 164.43192 175.61114 220.06158 \n      321       322       323       324       325       326       327       328 \n181.74555 160.98583  86.27842 195.97205 136.46895 102.86908 220.53640 125.83914 \n      329       330       331       332       333       334       335       336 \n186.27545 176.93132 178.40037 148.32790 181.84927 133.17072 167.12163 191.16061 \n      337       338       339       340       341       342       343       344 \n167.95000 211.35983 163.46813  96.61852 151.47610 139.82344 138.38826 176.47513 \n      345       346       347       348       349       350       351       352 \n106.40781 158.88878 179.08940 148.79682 143.06119  93.72813 134.88542 153.31118 \n      353       354       355       356       357       358       359       360 \n116.84713 186.85398 177.47107 142.76953 163.52582 167.98283 177.39367 213.53039 \n      361       362       363       364       365       366       367       368 \n161.02175 129.52203 179.97033 161.03589 136.94095 127.20454 159.24064 152.34627 \n      369       370       371       372       373       374       375       376 \n188.04240 190.15391 181.43039 132.62329 167.16688 157.13535 200.36373 123.08217 \n      377       378       379       380       381       382       383       384 \n 95.83962 182.55975 167.50830  76.89241 140.75411 116.13802 108.41218 171.20843 \n      385       386       387       388       389       390       391       392 \n108.95821 159.18156 148.49892 109.31069 139.47722 178.64745 165.07685 175.29912 \n      393       394       395       396       397       398       399       400 \n157.02854 160.68996 198.45546 120.21546 148.23662 146.56264 168.47318 172.63256 \n      401       402       403       404       405       406       407       408 \n199.19678 148.82619 111.56629 159.27455  77.41079 130.80692 104.47776 175.76773 \n      409       410       411       412       413       414       415       416 \n146.68981 136.07840 196.72408 116.78047 108.14538 140.87229 104.28901 220.51154 \n      417       418       419       420       421       422       423       424 \n124.22878 120.27876 150.89275 160.30190 125.30044 130.61268 165.22609 182.98173 \n      425       426       427       428       429       430       431       432 \n197.48268 127.15171 137.72440 138.36959 189.30688 131.46424 120.45398 126.79471 \n      433       434       435       436       437       438       439       440 \n159.92036 106.12099 145.96688 199.03398 200.80264  91.66356 207.26604 120.99236 \n      441       442       443       444       445       446       447       448 \n134.59685 186.49729 125.37228  91.33883 215.94567 113.12211 140.71538 183.96527 \n      449       450       451       452       453       454       455       456 \n109.96835 147.36813 152.39432 149.86401 106.21055 163.41078 168.27444 155.85423 \n      457       458       459       460       461       462       463       464 \n136.78099 204.50110 115.72090 166.82063 113.56614 123.34751 136.70975 143.39951 \n      465       466       467       468       469       470       471       472 \n135.34580  98.44458 164.99800 163.75184 158.33794 149.70401 146.75652 149.65797 \n      473       474       475       476       477       478       479       480 \n201.56434 160.68342 144.82817 147.41448 113.52576 191.18718 144.53477 145.80859 \n      481       482       483       484       485       486       487       488 \n145.64267 181.73417 141.61701 149.51494 132.31808 167.51278 128.26862 132.21064 \n      489       490       491       492       493       494       495       496 \n 94.26651 181.22267 139.62648 196.96431 131.21919  86.09835 159.41616 199.98189 \n      497       498       499       500       501       502       503       504 \n118.21873 127.61573 189.53638 160.71170 108.63548 137.19988 117.61193 115.58129 \n      505       506       507       508       509       510       511       512 \n181.74385 153.85581 111.85147 129.21202 127.75988 125.59242 156.39825 122.05988 \n      513       514       515       516       517       518       519       520 \n130.68281 168.16906 153.41066 145.22976  92.58597 162.04743 111.33337 108.24769 \n      521       522       523       524       525       526       527       528 \n167.66315 121.18656 149.18143 210.13095 183.02837 140.53427 146.56573 144.87310 \n      529       530       531       532       533       534       535       536 \n166.49509 122.59210 150.75937 125.61078  86.18543 102.22212 155.43198 148.15748 \n      537       538       539       540       541       542       543       544 \n149.35042 211.70608 166.89867 163.01990 165.98605 131.55750 139.05699 136.26600 \n      545       546       547       548       549       550       551       552 \n187.37512 209.16296 124.24118 217.15732 131.45206 200.44739 161.97900 211.75724 \n      553       554       555       556       557       558       559       560 \n196.60627 158.87603 146.42271 158.66215  94.57544 130.58982 219.83518 168.73653 \n      561       562       563       564       565       566       567       568 \n103.23676 138.61184 139.28506  90.38635 107.43178 145.56665 182.75359  84.66830 \n      569       570       571       572       573       574       575       576 \n 95.54172 110.40895 141.69131 149.70431 155.93306 147.90934  66.92172 147.40374 \n      577       578       579       580       581       582       583       584 \n164.29821 101.57863 140.66876 128.56197 180.70434 152.35734  98.03817 120.94437 \n      585       586       587       588       589       590       591       592 \n163.93671 103.51459 234.60747 183.02836 112.99979 126.88485 117.09249 197.43122 \n      593       594       595       596       597       598       599       600 \n189.30520 199.85646 204.25296 116.47294 133.09952 159.90792  94.78936 146.62481 \n      601       602       603       604       605       606       607       608 \n161.30100 206.37377 171.43030 102.88773 109.65036 109.00625 133.92054 119.10930 \n      609       610       611       612       613       614       615       616 \n133.56496 161.88262 215.55343 143.80112 137.94141 115.89952 171.28105 124.59056 \n      617       618       619       620       621       622       623       624 \n125.53644 117.37452 116.79146 120.18302 111.49305 145.67830 169.56072 142.96825 \n      625       626       627       628       629       630       631       632 \n199.42203 178.96055 179.89291 127.84212 168.28382 170.12228 174.47724 209.10902 \n      633       634       635       636       637       638       639       640 \n150.86026  98.97987 166.86275 142.15511 157.93122 133.83970 189.96146 181.17574 \n      641       642       643       644       645       646       647       648 \n117.20864 128.77141  88.50434 140.26888 113.20755 159.03742  93.20243 205.15286 \n      649       650       651       652       653       654       655       656 \n156.19785  93.57605 152.34148 167.76096 111.57876 156.65744 179.34831 116.69196 \n      657       658       659       660       661       662       663       664 \n137.27616 138.71104 106.39228 174.14174 136.91016 131.00698 157.96093 144.11312 \n      665       666       667       668       669       670       671       672 \n149.95415  97.68463  83.63646 140.30589 184.32985 101.12046 198.48657 141.59381 \n      673       674       675       676       677       678       679       680 \n122.32839 132.94777 189.39847 193.85573 140.69053 128.54644 123.63920 174.14799 \n      681       682       683       684       685       686       687       688 \n 76.07789 142.56740 142.26049 103.90995 153.78283 184.23997 169.68028 163.73493 \n      689       690       691       692       693       694       695       696 \n153.14078 214.91104 209.38485 187.78498 190.32461 203.45085 132.04024 147.59594 \n      697       698       699       700       701       702       703       704 \n209.54622 152.67528 149.49657 119.85993 158.63245 193.95325 157.32780  74.61815 \n      705       706       707       708       709       710       711       712 \n139.78303 148.86800 170.62168 177.30972 141.46807 165.29557  85.14906 195.68289 \n      713       714       715       716       717       718       719       720 \n 94.31627 174.03493  92.89663 124.01311 175.83164 101.39999 147.03294 142.49622 \n      721       722       723       724       725       726       727       728 \n102.47200 160.55174 153.39793 179.41667 161.80378 188.13850 170.92407 181.38065 \n      729       730       731       732       733       734       735       736 \n183.05153 109.29652 206.81269 107.39169 149.97108 115.57644 137.92279 112.17761 \n      737       738       739       740       741       742       743       744 \n161.60959 123.38311 102.45505 121.23915 148.07805 104.21889 101.91801 104.84263 \n      745       746       747       748       749       750       751       752 \n159.82396 145.34680 184.67126 149.98669 204.25467 127.65413 135.43458 188.38210 \n      753       754       755       756       757       758       759       760 \n111.04171 163.76741 106.66981 187.99267 170.43716 131.38677 196.23994 158.66072 \n      761       762       763       764       765       766       767       768 \n117.32960 116.56648  95.92667 204.91089 178.29951 170.92889 167.34383 117.10943 \n      769       770       771       772       773       774       775       776 \n166.87970 175.31637 151.84175 133.86458 147.68267 183.06057 139.74572 191.49750 \n      777       778       779       780       781       782       783       784 \n157.37761 206.26633 160.58114 188.55907 155.08604 158.00274 169.29532 130.42954 \n      785       786       787       788       789       790       791       792 \n117.71002 110.82296 115.95208 131.84435 160.53279 157.60878 137.71198 207.13405 \n      793       794       795       796       797       798       799       800 \n111.15026 134.09405 191.88358 143.70642 182.39076 123.30742 135.25593 159.66430 \n      801       802       803       804       805       806       807       808 \n137.93381 148.97340 126.11531  96.50831 122.65395 161.04010 182.92128 115.70395 \n      809       810       811       812       813       814       815       816 \n124.13266 130.35385 107.80088 109.31520 150.02567 165.23199 107.83846 208.98325 \n      817       818       819       820       821       822       823       824 \n191.25359 132.38617 132.99406 124.38528 190.38509 168.24510 144.47013 186.40262 \n      825       826       827       828       829       830       831       832 \n193.52193 199.05886 189.04803 165.37642 227.71931 181.57172 194.40629 114.47539 \n      833       834       835       836       837       838       839       840 \n126.94223 173.55531 173.64069  86.12804 158.79830 159.37572 114.80716 126.70284 \n      841       842       843       844       845       846       847       848 \n133.02065 131.38989  69.92034 133.43639 115.30965 174.83223 117.28155 215.16084 \n      849       850       851       852       853       854       855       856 \n115.09295 118.63449 149.71050  98.83716 137.78178 157.02373 181.20713 147.35541 \n      857       858       859       860       861       862       863       864 \n135.67820 162.41962 122.64183 166.25939 165.39337 173.02002 142.67348 118.10370 \n      865       866       867       868       869       870       871       872 \n121.27506 156.76180 102.17407 112.04873 161.54260 146.19527 151.90878  98.57488 \n      873       874       875       876       877       878       879       880 \n148.95197 141.54518 144.79879 135.99896 220.22154 166.94160 170.91508 134.34702 \n      881       882       883       884       885       886       887       888 \n140.27201 149.00314 181.41794 186.38880 112.23641 137.02320 150.14489 153.80293 \n      889       890       891       892       893       894       895       896 \n133.78237 177.00591  97.29345 177.82945 108.96474 143.77484 109.88781 137.99429 \n      897       898       899       900       901       902       903       904 \n195.43959 133.20663 165.10963  98.74387 102.76223 183.61170 249.30085 178.79915 \n      905       906       907       908       909       910       911       912 \n147.14736  82.47596 201.46996 151.11012 134.65252  92.85790 122.84505 139.48203 \n      913       914       915       916       917       918       919       920 \n179.77475 171.35085 173.99905 212.88938 125.60796 177.90413 165.16701 166.47500 \n      921       922       923       924       925       926       927       928 \n114.76052 135.98509 107.74073 168.20780 207.28439 128.85987 173.56322 125.16145 \n      929       930       931       932       933       934       935       936 \n 92.67335 162.16668 206.59530 187.35367 164.60405 159.80983 128.72787 169.05167 \n      937       938       939       940       941       942       943       944 \n129.30531 113.91941 116.44077 147.14597 116.33535 198.55777 135.72938 161.85915 \n      945       946       947       948       949       950       951       952 \n168.73342 165.43831 113.07268 173.07630  97.41609 163.47127 163.04448 178.74035 \n      953       954       955       956       957       958       959       960 \n161.54741 110.63222 185.37558 155.14032  81.43196 121.41604 168.27137 116.81465 \n      961       962       963       964       965       966       967       968 \n162.30799 128.27654 181.95155 106.08367 203.26144 124.03628 187.93702 158.60301 \n      969       970       971       972       973       974       975 \n 82.05091 167.86634 193.10652 144.21857 166.81020 154.91225 184.98919 \n\n\n\nplot(my.lm)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(car)\n\nLoading required package: carData\n\navPlots(my.lm)\n\n\n\n\n\nlibrary(gvlma)\ngvlma(my.lm)\n\n\nCall:\nlm(formula = Final ~ Yr1 + Yr2 + Yr3, data = ugtests)\n\nCoefficients:\n(Intercept)          Yr1          Yr2          Yr3  \n   14.14599      0.07603      0.43129      0.86568  \n\n\nASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS\nUSING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:\nLevel of Significance =  0.05 \n\nCall:\n gvlma(x = my.lm) \n\n                      Value   p-value                   Decision\nGlobal Stat        54.24704 4.672e-11 Assumptions NOT satisfied!\nSkewness            0.73224 3.922e-01    Assumptions acceptable.\nKurtosis            0.06324 8.015e-01    Assumptions acceptable.\nLink Function      53.43397 2.675e-13 Assumptions NOT satisfied!\nHeteroscedasticity  0.01759 8.945e-01    Assumptions acceptable.\n\n\n\nlibrary(visreg)\nvisreg(my.lm, xvar=\"Yr2\")"
  },
  {
    "objectID": "posts/week-3/index.html",
    "href": "posts/week-3/index.html",
    "title": "Week 3: Binomial Logistic Regression",
    "section": "",
    "text": "The slides are here.\nOur third class meeting will focus on Chapter 5 of Handbook of Regression Modeling in People Analytics."
  },
  {
    "objectID": "posts/week-3/index.html#overview-and-comments",
    "href": "posts/week-3/index.html#overview-and-comments",
    "title": "Week 3: Binomial Logistic Regression",
    "section": "Overview and Comments",
    "text": "Overview and Comments\nWhat we require is a regression type tool tuned to represent data drawn from a generic binomial distribution. There are actually a few such models that I will introduce you to. There are also some really interesting models that you can fit that build mixtures of the different approaches but we won’t go that far. I should also note that there is a whole class of models on binary classification using trees. If you remember regression trees, you can also build regression trees for binary problems. I will do a bit of this in the end. Before that, some initial observations:\n\nI am using stargazer to produce the tables; they are nice and easy to produce. They have raw html output so I can embed that directly using asis in the code chunks and typesetting to html.\n\nThis whole document makes use of fenced code chunks. You can copy and paste this into a new markdown or quarto to play along with the ticks built in.\n\nIf one wants to omit a chunk at the top, you would do it with the bracketed part adding option include=FALSE. I always suppress warnings and messages to read (surrounded by curly brackets) r setup, include=FALSE. If you use this option and load libraries, readers will find it hard to figure out how commands may have changed meaning by masking.\n\n\n```{r setup}\nknitr::opts_chunk$set(warning=FALSE, message=FALSE, tidy = TRUE)\nset.seed(9119)\n```"
  },
  {
    "objectID": "posts/week-3/index.html#a-probit-model",
    "href": "posts/week-3/index.html#a-probit-model",
    "title": "Week 3: Binomial Logistic Regression",
    "section": "A Probit Model",
    "text": "A Probit Model\nFirst, a little substitution and some notation. Let me label the normal probability up to \\(X_{i}\\beta\\) to be \\(\\Phi(X\\beta)\\) and the probability above \\(X\\beta\\) to be \\(1-\\Phi(X+{i}\\beta)\\). I could substitute this into the binomial and obtain the product for the entire sample – this is known as the likelihood.\n\\[\\prod^{n}_{i=1} \\Phi(X_{i}\\beta)^{1-y_{i}}(1-\\Phi(X_{i}\\beta))^{y_{i}}\\]\nTaking logs yields:\n\\[\\ln \\mathcal{L} =  \\sum^{n}_{i=1} (1-y_{i})\\ln \\Phi(X_{i}\\beta) + y_{i} \\ln (1-\\Phi(X_{i}\\beta))\\]\nSo the solution becomes\n\\[\\arg \\max_{\\beta} \\ln \\mathcal{L} =  \\arg \\max_{\\beta} \\sum^{n}_{i=1} (1-y_{i})\\ln \\Phi(X_{i}\\beta) + y_{i} \\ln (1-\\Phi(X_{i}\\beta))\\]\nIn English, we want to find the values of \\(\\beta\\) that maximize the log-likelihod of the entire sample.\n\nEstimation of a First GLM\nNow to another example with the same measure of Churn. The outcome of interest is Churn. The model specification will call glm, let me examine Churn as a function of InternetService, tenure, PhoneService, Contract and TotalCharges. There is one trick to deploying it, the outcome variable must be a factor type. To make the table nice, let me mutate the type to a factor and then we can model it.\n\n```{r, warning=FALSE, message=FALSE}\nChurn %<>%\n    mutate(ChurnF = as.factor(Churn))\nChurn %>%\n    select(Churn, ChurnF) %>%\n    mutate(as.numeric(ChurnF)) %>%\n    head()\n```\n\n  Churn ChurnF as.numeric(ChurnF)\n1    No     No                  1\n2    No     No                  1\n3   Yes    Yes                  2\n4    No     No                  1\n5   Yes    Yes                  2\n6   Yes    Yes                  2\n\n\nNow I want to estimate the model and have a look at the result. I will put this in a stargazer table.\n```{r, results='asis', warning=FALSE, message=FALSE}\nmy.probit <- glm(ChurnF ~ InternetService + tenure + PhoneService + Contract + TotalCharges,\n    family = binomial(link = \"probit\"), data = Churn)\nstargazer(my.probit, type = \"html\", style = \"apsr\")\n```\n\n\n\n\n\n\n\n\n\n\nChurnF\n\n\n\n\n\n\n\n\nInternetServiceFiber optic\n\n\n0.726***\n\n\n\n\n\n\n(0.053)\n\n\n\n\nInternetServiceNo\n\n\n-0.458***\n\n\n\n\n\n\n(0.070)\n\n\n\n\ntenure\n\n\n-0.028***\n\n\n\n\n\n\n(0.003)\n\n\n\n\nPhoneServiceYes\n\n\n-0.372***\n\n\n\n\n\n\n(0.073)\n\n\n\n\nContractOne year\n\n\n-0.489***\n\n\n\n\n\n\n(0.057)\n\n\n\n\nContractTwo year\n\n\n-0.866***\n\n\n\n\n\n\n(0.083)\n\n\n\n\nTotalCharges\n\n\n0.0001***\n\n\n\n\n\n\n(0.00003)\n\n\n\n\nConstant\n\n\n0.131*\n\n\n\n\n\n\n(0.069)\n\n\n\n\nN\n\n\n7,032\n\n\n\n\nLog Likelihood\n\n\n-3,019.439\n\n\n\n\nAIC\n\n\n6,054.878\n\n\n\n\n\n\n\n\np < .1; p < .05; p < .01\n\n\n\n\nWe can do astrology on the tables; read the stars. Fiber optic customers are more likely to Churn and those without internet service are less likely to Churn but both conditions are compared to a third category absorbed into the Constant. What is that category?\n\n```{r}\njanitor::tabyl(Churn$InternetService)\n```\n\n Churn$InternetService    n   percent\n                   DSL 2421 0.3437456\n           Fiber optic 3096 0.4395854\n                    No 1526 0.2166690\n\n\nDSL subscribers. It is the first in alphabetical order. That is the default option. That also means that the constant captures those on Month-to-month contracts and without phone service – the omitted category for each. So what do these coefficients show?\nThe slopes represent the effect of a one-unit change in \\(x\\) on the underlying distribution for the probabilities. Unless one has intuition for those distributions, they come across as nonsensical. In the table above, let me take the example of tenure. For each unit of tenure [another month having been a customer], the normal variable \\(Z \\sim N(0,1)\\) decreases by 0.028. But what that means depends on whether we are going from 0 to -0.028 or from -2 to -2.028. Remember the standard normal has about 95% of probability between -2 and 2 and has a modal/most common value at zero.\n\n```{r}\ndata.frame(Z = rnorm(10000)) %>%\n    ggplot(.) + aes(x = Z) + geom_density() + theme_minimal() + geom_vline(aes(xintercept = -2.028),\n    color = \"red\") + geom_vline(aes(xintercept = -2), color = \"red\") + geom_vline(aes(xintercept = -0.028),\n    color = \"blue\") + geom_vline(aes(xintercept = 0), color = \"blue\")\n```\n\n\n\n\nThe associated probabilities in that example would be either small or nearly trivial even over 1000s of customers.\n\n```{r}\npnorm(-2) - pnorm(-2.028)\npnorm(0) - pnorm(-0.028)\n```\n\n[1] 0.001470008\n[1] 0.01116892\n\n\nIt seems like most scholars I run across don’t actually know this; they tend to stick to stars (That’s why your book plays with odds and logistic regression but I want to start here because y’all have never seen the logistic distribution, except on my poster….) Before that, here’s another way of showing the actual estimated slopes even if their intuition is hard.\n\n```{r}\nlibrary(jtools)\nplot_summs(my.probit, inner_ci_level = 0.95)\n```\n\n\n\n\nI can make that plot better.\n\n\nA Scaled Coefficient Plot\nRemember scaling from last time and linear models? No rules against that. The two metric variables – tenure and TotalCharges – are now the change in Z for a one standard deviation change in the relevant variable. That’s 2267 dollars for TotalCharges and 24.6 months for tenure.\n\n```{r}\nChurn %>%\n    skim(TotalCharges, tenure)\n```\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n7043\n\n\nNumber of columns\n23\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nTotalCharges\n11\n1\n2283.30\n2266.77\n18.8\n401.45\n1397.47\n3794.74\n8684.8\n▇▂▂▂▁\n\n\ntenure\n0\n1\n32.37\n24.56\n0.0\n9.00\n29.00\n55.00\n72.0\n▇▃▃▃▆\n\n\n\n\n\n\n```{r}\nChurn %>%\n    mutate(tenure = scale(tenure), TotalCharges = scale(TotalCharges)) %>%\n    glm(ChurnF ~ InternetService + tenure + PhoneService + Contract + TotalCharges,\n        family = binomial(link = \"probit\"), data = .) %>%\n    plot_summs(., inner_ci_level = 0.95)\n```\n\n\n\n\nWhat the plot makes clear is that basically all of the predictors are deemed important in Churn decisions by conventional standards as all have a very low probability of no relationship/zero slope. But that’s as far as we can get with these unless our audience shares this intuition for probability distributions.\n\n\nThe Trouble with Non-linear Models\nI should be clear that the model does have lines; they are just lines inside of a nonlinear function – the F. The generalized linear part means that the interpretation of any one factor will depend on the values of the others. We will have to usually want to generate hypothetical data to understand what is really going on. After a presentation of the remaining models, I will return to my preferred method of interpretation."
  },
  {
    "objectID": "posts/week-3/index.html#logistic-regression",
    "href": "posts/week-3/index.html#logistic-regression",
    "title": "Week 3: Binomial Logistic Regression",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nThe logistic distribution is the focus of the textbook chapter. To respecify the model using that, the only change in syntax is the link, we need it to be link=\"logit\" which is the default.\nThe logistic function is given by:\n\\[\\Lambda = \\frac{e^{X\\beta}}{1+e^{X\\beta}}\\]\nBut the rest is the same; it takes the very general representation and provides a specific probability function for \\(F\\):\n\\[\\arg \\max_{\\beta} \\ln \\mathcal{L} =  \\arg \\max_{\\beta} \\sum^{n}_{i=1} (1-y_{i})\\ln \\Lambda(X_{i}\\beta) + y_{i} \\ln (1-\\Lambda(X_{i}\\beta))\\]\nOne of the advantages of using the logistic distribution is that you can analytically solve it with only categorical variables. The other is the interpretation of the estimates; the slope is an increment in the log-odds, e.g. \\(\\ln (\\frac{\\pi_{y=1}}{1-\\pi_{y=1}})\\).\n```{r, results='asis', warning=FALSE, message=FALSE}\nmy.logit <- glm(ChurnF ~ InternetService + tenure + PhoneService + Contract + TotalCharges,\n    family = binomial(link = \"logit\"), data = Churn)\nstargazer(my.logit, my.probit, type = \"html\", style = \"apsr\")\n```\n\n\n\n\n\n\n\n\n\n\nChurnF\n\n\n\n\n\n\nlogistic\n\n\nprobit\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nInternetServiceFiber optic\n\n\n1.172***\n\n\n0.726***\n\n\n\n\n\n\n(0.091)\n\n\n(0.053)\n\n\n\n\nInternetServiceNo\n\n\n-0.765***\n\n\n-0.458***\n\n\n\n\n\n\n(0.127)\n\n\n(0.070)\n\n\n\n\ntenure\n\n\n-0.063***\n\n\n-0.028***\n\n\n\n\n\n\n(0.006)\n\n\n(0.003)\n\n\n\n\nPhoneServiceYes\n\n\n-0.714***\n\n\n-0.372***\n\n\n\n\n\n\n(0.126)\n\n\n(0.073)\n\n\n\n\nContractOne year\n\n\n-0.874***\n\n\n-0.489***\n\n\n\n\n\n\n(0.103)\n\n\n(0.057)\n\n\n\n\nContractTwo year\n\n\n-1.781***\n\n\n-0.866***\n\n\n\n\n\n\n(0.172)\n\n\n(0.083)\n\n\n\n\nTotalCharges\n\n\n0.0004***\n\n\n0.0001***\n\n\n\n\n\n\n(0.0001)\n\n\n(0.00003)\n\n\n\n\nConstant\n\n\n0.373***\n\n\n0.131*\n\n\n\n\n\n\n(0.117)\n\n\n(0.069)\n\n\n\n\nN\n\n\n7,032\n\n\n7,032\n\n\n\n\nLog Likelihood\n\n\n-3,004.328\n\n\n-3,019.439\n\n\n\n\nAIC\n\n\n6,024.656\n\n\n6,054.878\n\n\n\n\n\n\n\n\np < .1; p < .05; p < .01\n\n\n\n\nIf we see these in a side by side comparison, it is obvious that the logistic version is bigger in absolute value across the board. So what do these mean in terms of actual odds of Churn or not?\n\n```{r}\nexp(my.logit$coefficients)\n```\n\n               (Intercept) InternetServiceFiber optic \n                 1.4521830                  3.2282235 \n         InternetServiceNo                     tenure \n                 0.4652907                  0.9393570 \n           PhoneServiceYes           ContractOne year \n                 0.4898382                  0.4173338 \n          ContractTwo year               TotalCharges \n                 0.1685345                  1.0003559 \n\n\nAll else equal,\n\nThe odds of Churning with Fiber optics, as opposed to DSL, increase by 223%.\nThe odds of Churning with No internet, as opposed to DSL, decrease by 53.5% .\nThe odds of Churning with No phone service, as opposed to Phone service, are 51% lower.\nThe odds of Churning decrease by 4% per unit tenure [month].\nThe odds of Churning increase by 0.04% per dollar of total charges.\nThe odds of Churning decrease under contracts. Compared to none, about 83% lower odds under a two-year contract and 58% lower odds under a one-year contract.\n\nIf you choose to work with odds, then the suggestion to exponentiate the confidence intervals for the odds-ratios is sound.\n\n```{r}\nexp(confint(my.logit))\n```\n\n                               2.5 %    97.5 %\n(Intercept)                1.1535576 1.8288933\nInternetServiceFiber optic 2.7035580 3.8611249\nInternetServiceNo          0.3619193 0.5951803\ntenure                     0.9284409 0.9500587\nPhoneServiceYes            0.3822153 0.6276984\nContractOne year           0.3400210 0.5101487\nContractTwo year           0.1190988 0.2338910\nTotalCharges               1.0002350 1.0004795\n\n\nThere are diagnostics that can be applied to these models. The various pseudo-\\(r^2\\) measures. This model fit is neither terrible nor good.\n\n```{r}\nlibrary(DescTools)\nDescTools::PseudoR2(my.logit, which = c(\"McFadden\", \"CoxSnell\", \"Nagelkerke\", \"Tjur\"))\n```\n\n  McFadden   CoxSnell Nagelkerke       Tjur \n 0.2621401  0.2618213  0.3817196  0.2798359 \n\n\nTaking advantage of the book’s example, I first need to clean up the data, there are a few missing values. Then let me estimate the regression and diagnose it.\n\n```{r}\nChurn.CC <- Churn %>%\n    select(ChurnF, InternetService, tenure, PhoneService, Contract, TotalCharges) %>%\n    filter(!is.na(TotalCharges))\nmy.logit.CC <- glm(ChurnF ~ InternetService + tenure + PhoneService + Contract +\n    TotalCharges, family = binomial(link = \"logit\"), data = Churn.CC)\nlibrary(LogisticDx)\n# get range of goodness-of-fit diagnostics\nmodel_diagnostics <- LogisticDx::gof(my.logit.CC, plotROC = TRUE)\n```\n\n\n\n\nOne very common plot for binary logistic regression is the ROC: the Receiver Operating Curve. It plots specificity against sensitivity. Specificity is the ability, in this case, to correctly identify non-Churners[few false positives is highly specific]; sensitivity is the ability of the test to correctly identify Churners [few false negatives is highly sensitive]. A useful mnemonic is that the presence of the letter f in specificity is a reminder that the False test results are False for the condition, while the t in sensitivity is True test results are True for the condition. Now, turning to the actual provided diagnostics, what all is in there? ?gof for example.\n\n```{r}\n# returns a list\nnames(model_diagnostics)\nmodel_diagnostics$gof\n```\n\n[1] \"ct\"    \"chiSq\" \"ctHL\"  \"gof\"   \"R2\"    \"auc\"  \n         test  stat       val df         pVal\n1:         HL chiSq 31.832686  8 9.979186e-05\n2:        mHL     F 16.436041  9 4.896337e-27\n3:       OsRo     Z  3.282456 NA 1.029070e-03\n4: SstPgeq0.5     Z  6.656484 NA 2.804560e-11\n5:   SstPl0.5     Z  5.983933 NA 2.178133e-09\n6:    SstBoth chiSq 80.116227  2 4.008504e-18\n7: SllPgeq0.5 chiSq 45.280618  1 1.707304e-11\n8:   SllPl0.5 chiSq 29.794958  1 4.802393e-08\n9:    SllBoth chiSq 54.741365  2 1.297369e-12\n\n\nThis is not a very good model. It fails all the tests. We need to add more predictors; I have those but let’s keep it simple for now. Let me look at two more."
  },
  {
    "objectID": "posts/week-3/index.html#other-binomial-glms",
    "href": "posts/week-3/index.html#other-binomial-glms",
    "title": "Week 3: Binomial Logistic Regression",
    "section": "Other Binomial GLMs",
    "text": "Other Binomial GLMs\nThe others\n```{r, results='asis', warning=FALSE, message=FALSE}\nmy.cauchit <- glm(ChurnF ~ InternetService + tenure + PhoneService + Contract + TotalCharges,\n    family = binomial(link = \"cauchit\"), data = Churn)\nmy.cloglogit <- glm(ChurnF ~ InternetService + tenure + PhoneService + Contract +\n    TotalCharges, family = binomial(link = \"cloglog\"), data = Churn)\nstargazer(my.cauchit, my.cloglogit, my.logit, my.probit, type = \"html\", style = \"apsr\")\n```\n\n\n\n\n\n\n\n\n\n\nChurnF\n\n\n\n\n\n\nglm: binomial\n\n\nglm: binomial\n\n\nlogistic\n\n\nprobit\n\n\n\n\n\n\nlink = cauchit\n\n\nlink = cloglog\n\n\n\n\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n(3)\n\n\n(4)\n\n\n\n\n\n\n\n\nInternetServiceFiber optic\n\n\n0.993***\n\n\n0.877***\n\n\n1.172***\n\n\n0.726***\n\n\n\n\n\n\n(0.094)\n\n\n(0.071)\n\n\n(0.091)\n\n\n(0.053)\n\n\n\n\nInternetServiceNo\n\n\n-0.790***\n\n\n-0.640***\n\n\n-0.765***\n\n\n-0.458***\n\n\n\n\n\n\n(0.174)\n\n\n(0.112)\n\n\n(0.127)\n\n\n(0.070)\n\n\n\n\ntenure\n\n\n-0.123***\n\n\n-0.059***\n\n\n-0.063***\n\n\n-0.028***\n\n\n\n\n\n\n(0.011)\n\n\n(0.005)\n\n\n(0.006)\n\n\n(0.003)\n\n\n\n\nPhoneServiceYes\n\n\n-0.705***\n\n\n-0.586***\n\n\n-0.714***\n\n\n-0.372***\n\n\n\n\n\n\n(0.136)\n\n\n(0.099)\n\n\n(0.126)\n\n\n(0.073)\n\n\n\n\nContractOne year\n\n\n-1.190***\n\n\n-0.767***\n\n\n-0.874***\n\n\n-0.489***\n\n\n\n\n\n\n(0.166)\n\n\n(0.092)\n\n\n(0.103)\n\n\n(0.057)\n\n\n\n\nContractTwo year\n\n\n-7.065***\n\n\n-1.696***\n\n\n-1.781***\n\n\n-0.866***\n\n\n\n\n\n\n(1.254)\n\n\n(0.161)\n\n\n(0.172)\n\n\n(0.083)\n\n\n\n\nTotalCharges\n\n\n0.001***\n\n\n0.0004***\n\n\n0.0004***\n\n\n0.0001***\n\n\n\n\n\n\n(0.0001)\n\n\n(0.0001)\n\n\n(0.0001)\n\n\n(0.00003)\n\n\n\n\nConstant\n\n\n0.590***\n\n\n-0.024\n\n\n0.373***\n\n\n0.131*\n\n\n\n\n\n\n(0.122)\n\n\n(0.088)\n\n\n(0.117)\n\n\n(0.069)\n\n\n\n\nN\n\n\n7,032\n\n\n7,032\n\n\n7,032\n\n\n7,032\n\n\n\n\nLog Likelihood\n\n\n-2,997.445\n\n\n-2,991.982\n\n\n-3,004.328\n\n\n-3,019.439\n\n\n\n\nAIC\n\n\n6,010.891\n\n\n5,999.964\n\n\n6,024.656\n\n\n6,054.878\n\n\n\n\n\n\n\n\np < .1; p < .05; p < .01\n\n\n\n\nThe best fit seems to be provided by the cloglog distribution which is asymmetric.\n\n```{r}\nlibrary(pROC)\npredicted <- predict(my.cloglogit, type = \"response\")\nauc(Churn.CC$ChurnF, predicted, plot = TRUE)\n```\n\nArea under the curve: 0.8386"
  },
  {
    "objectID": "posts/week-3/index.html#residuals",
    "href": "posts/week-3/index.html#residuals",
    "title": "Week 3: Binomial Logistic Regression",
    "section": "Residuals",
    "text": "Residuals\n\n```{r}\nd <- density(residuals(my.logit, \"pearson\"))\nplot(d, main = \"\")\n```\n\n\n\n\nThis is rather poor."
  },
  {
    "objectID": "posts/week-3/index.html#predicted-probability",
    "href": "posts/week-3/index.html#predicted-probability",
    "title": "Week 3: Binomial Logistic Regression",
    "section": "Predicted Probability",
    "text": "Predicted Probability\nI find that the most straightforward way to interpret them is with plots in the probability metric. Let me take the example of tenure.\nI will need to create data for interpretation. Let’s suppose we have a DSL user with phone service on a two year contract with average TotalCharges. The last thing I need to know is what values of tenure to show.\n\n```{r}\nlibrary(skimr)\nChurn %>%\n    filter(InternetService == \"DSL\", PhoneService == \"Yes\", Contract == \"Two year\") %>%\n    skim(tenure, TotalCharges)\n```\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n467\n\n\nNumber of columns\n23\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ntenure\n0\n1.00\n60.52\n14.79\n0.0\n55.00\n66.00\n71.00\n72.00\n▁▁▁▂▇\n\n\nTotalCharges\n3\n0.99\n4733.47\n1382.51\n130.5\n3867.24\n4913.88\n5879.86\n6859.05\n▁▂▅▇▇\n\n\n\n\n\nNow I can create the data and generate predictions in the probability metric of the response.\n\n```{r}\nTenure.Pred <- data.frame(InternetService = \"DSL\", PhoneService = \"Yes\", Contract = \"Two year\",\n    TotalCharges = 4733.5, tenure = seq(0, 72, by = 1))\nTenure.Pred$Prob.Churn <- predict(my.logit, newdata = Tenure.Pred, type = \"response\")\n```\n\nNow let me plot it.\n\n```{r}\nggplot(Tenure.Pred) + aes(x = tenure, y = Prob.Churn) + geom_line() + theme_minimal()\n```\n\n\n\n\nWe could get fancier, too.\n\n```{r}\nTenure.Pred.Three <- rbind(data.frame(InternetService = \"DSL\", PhoneService = \"Yes\",\n    Contract = \"Month-to-month\", TotalCharges = 4733.5, tenure = seq(0, 72, by = 1)),\n    data.frame(InternetService = \"DSL\", PhoneService = \"Yes\", Contract = \"One year\",\n        TotalCharges = 4733.5, tenure = seq(0, 72, by = 1)), data.frame(InternetService = \"DSL\",\n        PhoneService = \"Yes\", Contract = \"Two year\", TotalCharges = 4733.5, tenure = seq(0,\n            72, by = 1)))\nTenure.Pred.Three$Prob.Churn <- predict(my.logit, newdata = Tenure.Pred.Three, type = \"response\")\nggplot(Tenure.Pred.Three) + aes(x = tenure, y = Prob.Churn, color = Contract) + geom_line() +\n    theme_minimal() + labs(y = \"Pr(Churn)\")\n```"
  },
  {
    "objectID": "posts/week-3/index.html#quadratic-terms",
    "href": "posts/week-3/index.html#quadratic-terms",
    "title": "Week 3: Binomial Logistic Regression",
    "section": "Quadratic Terms?",
    "text": "Quadratic Terms?\nWhat would happen if I assume that the effect of tenure is not a line but instead has some curvature.\n```{r, results='asis', warning=FALSE, message=FALSE}\nmod.train.SQ <- train %>%\n    glm(ChurnF ~ InternetService + tenure + I(tenure^2) + PhoneService + Contract +\n        TotalCharges, family = binomial(link = \"probit\"), data = .)\nstargazer(mod.train, mod.train.SQ, type = \"html\", style = \"apsr\")\n```\n\n\n\n\n\n\n\n\n\n\nChurnF\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nInternetServiceFiber optic\n\n\n0.703***\n\n\n0.761***\n\n\n\n\n\n\n(0.061)\n\n\n(0.062)\n\n\n\n\nInternetServiceNo\n\n\n-0.520***\n\n\n-0.566***\n\n\n\n\n\n\n(0.082)\n\n\n(0.083)\n\n\n\n\ntenure\n\n\n-0.030***\n\n\n-0.043***\n\n\n\n\n\n\n(0.004)\n\n\n(0.004)\n\n\n\n\nI(tenure2)\n\n\n\n\n0.0003***\n\n\n\n\n\n\n\n\n(0.0001)\n\n\n\n\nPhoneServiceYes\n\n\n-0.296***\n\n\n-0.262***\n\n\n\n\n\n\n(0.085)\n\n\n(0.085)\n\n\n\n\nContractOne year\n\n\n-0.467***\n\n\n-0.465***\n\n\n\n\n\n\n(0.067)\n\n\n(0.067)\n\n\n\n\nContractTwo year\n\n\n-0.927***\n\n\n-1.067***\n\n\n\n\n\n\n(0.100)\n\n\n(0.105)\n\n\n\n\nTotalCharges\n\n\n0.0001***\n\n\n0.00005\n\n\n\n\n\n\n(0.00004)\n\n\n(0.00004)\n\n\n\n\nConstant\n\n\n0.086\n\n\n0.171**\n\n\n\n\n\n\n(0.081)\n\n\n(0.082)\n\n\n\n\nN\n\n\n5,275\n\n\n5,275\n\n\n\n\nLog Likelihood\n\n\n-2,235.094\n\n\n-2,220.230\n\n\n\n\nAIC\n\n\n4,486.189\n\n\n4,458.460\n\n\n\n\n\n\n\n\np < .1; p < .05; p < .01\n\n\n\n\nAs we can see from the table, the curvature appears to be different from zero though interpreting such a thing ceteris paribus is probably nonsense. Maybe better to see what happens in the metric of the predicted probability. Let me recycle the prediction data I used to draw this in the earlier section. What would the two predictions look like and how do they differ?\n\n```{r}\nTenure.Pred$Prob.Churn.2 <- predict(mod.train, newdata = Tenure.Pred, type = \"response\")\nTenure.Pred$Prob.Churn.Sq <- predict(mod.train.SQ, newdata = Tenure.Pred, type = \"response\")\nggplot(Tenure.Pred) + aes(x = tenure, y = Prob.Churn.Sq) + geom_line() + geom_line(aes(y = Prob.Churn.2),\n    color = \"purple\") + theme_minimal() + labs(y = \"Pr(Churn)\")\n```\n\n\n\n\nNow let’s predict for the test set, does it really do better?\n\n```{r}\ntest$Pred.Probs.Sq <- predict(mod.train, newdata = test, type = \"response\")\ntest %>%\n    mutate(Pred.Val.Sq = (Pred.Probs.Sq > 0.5)) %>%\n    janitor::tabyl(Churn, Pred.Val.Sq, show_na = FALSE) %>%\n    adorn_percentages(\"row\")\n```\n\n Churn     FALSE      TRUE\n    No 0.8961749 0.1038251\n   Yes 0.5042017 0.4957983\n\n\nNot usually. Such people are really unlikely to Churn no matter what; it only starts at about 0.25."
  },
  {
    "objectID": "posts/week-3/index.html#a-final-note-a-classification-tree",
    "href": "posts/week-3/index.html#a-final-note-a-classification-tree",
    "title": "Week 3: Binomial Logistic Regression",
    "section": "A final note: a classification tree",
    "text": "A final note: a classification tree\nFirst, I will start with a generic classification tree with everything set to the defaults. Then I will look at a report to refine it.\n\n```{r}\nlibrary(rpart)\nlibrary(rpart.plot)\nfit.BT <- rpart(ChurnF ~ InternetService + tenure + PhoneService + Contract + TotalCharges,\n    data = train, method = \"class\")\nrpart.plot(fit.BT)\n```\n\n\n\n\nHow well does it fit the test sample?\n\n```{r}\ntest$Churn.No <- predict(fit.BT, newdata = test)[, 1]\ntest$Churn.PredRT <- (test$Churn.No < 0.5)\ntest %>%\n    tabyl(Churn, Churn.PredRT)\n```\n\n Churn FALSE TRUE\n    No  1210   74\n   Yes   319  157\n\n\nNot very well. We can alter the tolerance for complexity using some diagnostics about the tree.\n\n```{r}\nprintcp(fit.BT)\n```\n\n\nClassification tree:\nrpart(formula = ChurnF ~ InternetService + tenure + PhoneService + \n    Contract + TotalCharges, data = train, method = \"class\")\n\nVariables actually used in tree construction:\n[1] Contract        InternetService tenure         \n\nRoot node error: 1393/5283 = 0.26368\n\nn= 5283 \n\n        CP nsplit rel error  xerror     xstd\n1 0.056712      0   1.00000 1.00000 0.022991\n2 0.010000      3   0.78177 0.78823 0.021172\n\n\nThe option cp controls a complexity parameter that keeps the tree from overfitting the tree. I want to show a fairly complex one so I will change from the default of 0.01 to 0.0025.\n\n```{r}\nfit.BT.2 <- rpart(ChurnF ~ InternetService + tenure + PhoneService + Contract + TotalCharges,\n    data = train, method = \"class\", cp = 0.0025)\nrpart.plot(fit.BT.2, extra = 106)\n```\n\n\n\n\n\n```{r}\ntest$Churn.No <- predict(fit.BT.2, newdata = test)[, 1]\ntest$Churn.PredRT2 <- (test$Churn.No < 0.5)\ntest %>%\n    tabyl(Churn, Churn.PredRT2)\n```\n\n Churn FALSE TRUE\n    No  1154  130\n   Yes   251  225\n\n\nIn this case, we have 1379 correct with the big tree and 1367 right with the smaller tree."
  },
  {
    "objectID": "posts/smart-Prediction/index.html",
    "href": "posts/smart-Prediction/index.html",
    "title": "A Small Thread on Smart Prediction",
    "section": "",
    "text": "A linear regression example. The data can be loaded from the web.\n\n# if needed, download ugtests data\nurl <- \"http://peopleanalytics-regression-book.org/data/ugtests.csv\"\nugtests <- read.csv(url)\nstr(ugtests)\n\n'data.frame':   975 obs. of  4 variables:\n $ Yr1  : int  27 70 27 26 46 86 40 60 49 80 ...\n $ Yr2  : int  50 104 36 75 77 122 100 92 98 127 ...\n $ Yr3  : int  52 126 148 115 75 119 125 78 119 67 ...\n $ Final: int  93 207 175 125 114 159 153 84 147 80 ...\n\n\nThere are 975 individuals graduating in the past three years from the biology department of a large academic institution. We have data on four examinations:\n\na first year exam ranging from 0 to 100 (Yr1)\na second year exam ranging from 0 to 200 (Yr2)\na third year exam ranging from 0 to 200 (Yr3)\na Final year exam ranging from 0 to 300 (Final)\n\n\nlibrary(skimr); library(kableExtra); library(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.8     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.1\n✔ readr   2.1.2     ✔ forcats 0.5.2\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter()     masks stats::filter()\n✖ dplyr::group_rows() masks kableExtra::group_rows()\n✖ dplyr::lag()        masks stats::lag()\n\nskim(ugtests) %>% dplyr::filter(skim_type==\"numeric\") %>% kable()\n\n\n\n \n  \n    skim_type \n    skim_variable \n    n_missing \n    complete_rate \n    numeric.mean \n    numeric.sd \n    numeric.p0 \n    numeric.p25 \n    numeric.p50 \n    numeric.p75 \n    numeric.p100 \n    numeric.hist \n  \n \n\n  \n    numeric \n    Yr1 \n    0 \n    1 \n    52.14564 \n    14.92408 \n    3 \n    42 \n    53 \n    62 \n    99 \n    ▁▃▇▅▁ \n  \n  \n    numeric \n    Yr2 \n    0 \n    1 \n    92.39897 \n    30.03847 \n    6 \n    73 \n    94 \n    112 \n    188 \n    ▁▅▇▃▁ \n  \n  \n    numeric \n    Yr3 \n    0 \n    1 \n    105.12103 \n    33.50705 \n    8 \n    81 \n    105 \n    130 \n    198 \n    ▁▅▇▅▁ \n  \n  \n    numeric \n    Final \n    0 \n    1 \n    148.96205 \n    44.33966 \n    8 \n    118 \n    147 \n    175 \n    295 \n    ▁▅▇▃▁ \n  \n\n\n\n\n\n\n\n\nlibrary(GGally)\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\n# display a pairplot of all four columns of data\nGGally::ggpairs(ugtests)\n\n\n\n\n\nmy.lm <- lm(Final ~ Yr1 + Yr2 + Yr3, data=ugtests)\nsummary(my.lm)\n\n\nCall:\nlm(formula = Final ~ Yr1 + Yr2 + Yr3, data = ugtests)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-92.638 -20.349   0.001  18.954  98.489 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 14.14599    5.48006   2.581  0.00999 ** \nYr1          0.07603    0.06538   1.163  0.24519    \nYr2          0.43129    0.03251  13.267  < 2e-16 ***\nYr3          0.86568    0.02914  29.710  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 30.43 on 971 degrees of freedom\nMultiple R-squared:  0.5303,    Adjusted R-squared:  0.5289 \nF-statistic: 365.5 on 3 and 971 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\nugtests %>% mutate(Fitted.Value = predict(my.lm)) %>% kable() %>% scroll_box(height=\"300px\")\n\n\n\n \n  \n    Yr1 \n    Yr2 \n    Yr3 \n    Final \n    Fitted.Value \n  \n \n\n  \n    27 \n    50 \n    52 \n    93 \n    82.77839 \n  \n  \n    70 \n    104 \n    126 \n    207 \n    173.39734 \n  \n  \n    27 \n    36 \n    148 \n    175 \n    159.84579 \n  \n  \n    26 \n    75 \n    115 \n    125 \n    148.02242 \n  \n  \n    46 \n    77 \n    75 \n    114 \n    115.77826 \n  \n  \n    86 \n    122 \n    119 \n    159 \n    176.31713 \n  \n  \n    40 \n    100 \n    125 \n    153 \n    168.52573 \n  \n  \n    60 \n    92 \n    78 \n    84 \n    125.90895 \n  \n  \n    49 \n    98 \n    119 \n    147 \n    163.15331 \n  \n  \n    80 \n    127 \n    67 \n    80 \n    133.00197 \n  \n  \n    43 \n    134 \n    61 \n    154 \n    128.01391 \n  \n  \n    26 \n    53 \n    150 \n    154 \n    168.83298 \n  \n  \n    64 \n    123 \n    110 \n    175 \n    167.28471 \n  \n  \n    62 \n    84 \n    142 \n    182 \n    178.01432 \n  \n  \n    61 \n    65 \n    134 \n    155 \n    162.81842 \n  \n  \n    60 \n    150 \n    116 \n    198 \n    183.81939 \n  \n  \n    58 \n    76 \n    107 \n    161 \n    143.96109 \n  \n  \n    28 \n    81 \n    143 \n    229 \n    175.00126 \n  \n  \n    64 \n    87 \n    106 \n    100 \n    148.29571 \n  \n  \n    55 \n    111 \n    135 \n    179 \n    183.06708 \n  \n  \n    77 \n    97 \n    111 \n    164 \n    157.92531 \n  \n  \n    50 \n    92 \n    72 \n    130 \n    119.95460 \n  \n  \n    47 \n    83 \n    129 \n    194 \n    165.18879 \n  \n  \n    65 \n    72 \n    148 \n    152 \n    178.26106 \n  \n  \n    57 \n    180 \n    112 \n    216 \n    193.06715 \n  \n  \n    41 \n    41 \n    152 \n    123 \n    166.52931 \n  \n  \n    49 \n    90 \n    166 \n    232 \n    200.39004 \n  \n  \n    93 \n    102 \n    136 \n    168 \n    182.94018 \n  \n  \n    32 \n    62 \n    160 \n    151 \n    181.82752 \n  \n  \n    53 \n    73 \n    63 \n    88 \n    104.19713 \n  \n  \n    28 \n    84 \n    109 \n    184 \n    146.86195 \n  \n  \n    75 \n    125 \n    98 \n    134 \n    158.59539 \n  \n  \n    44 \n    55 \n    111 \n    115 \n    137.30246 \n  \n  \n    41 \n    127 \n    96 \n    167 \n    155.14171 \n  \n  \n    55 \n    97 \n    101 \n    170 \n    147.59592 \n  \n  \n    68 \n    109 \n    141 \n    229 \n    188.38693 \n  \n  \n    73 \n    52 \n    124 \n    132 \n    149.46722 \n  \n  \n    32 \n    92 \n    154 \n    245 \n    189.57199 \n  \n  \n    49 \n    65 \n    138 \n    180 \n    165.36883 \n  \n  \n    63 \n    107 \n    165 \n    213 \n    207.92058 \n  \n  \n    43 \n    87 \n    120 \n    211 \n    158.81869 \n  \n  \n    40 \n    110 \n    93 \n    151 \n    145.13679 \n  \n  \n    46 \n    71 \n    91 \n    121 \n    127.04145 \n  \n  \n    28 \n    118 \n    136 \n    180 \n    184.89905 \n  \n  \n    46 \n    124 \n    176 \n    232 \n    223.48248 \n  \n  \n    41 \n    97 \n    152 \n    214 \n    190.68129 \n  \n  \n    76 \n    95 \n    52 \n    123 \n    105.91152 \n  \n  \n    52 \n    106 \n    144 \n    166 \n    188.47370 \n  \n  \n    47 \n    104 \n    124 \n    111 \n    169.91737 \n  \n  \n    54 \n    27 \n    111 \n    90 \n    125.98673 \n  \n  \n    36 \n    97 \n    140 \n    187 \n    179.91299 \n  \n  \n    73 \n    22 \n    146 \n    160 \n    155.57364 \n  \n  \n    60 \n    68 \n    54 \n    125 \n    94.78176 \n  \n  \n    67 \n    45 \n    80 \n    145 \n    107.90209 \n  \n  \n    50 \n    99 \n    94 \n    168 \n    142.01859 \n  \n  \n    32 \n    72 \n    92 \n    114 \n    127.27405 \n  \n  \n    75 \n    103 \n    91 \n    152 \n    143.04734 \n  \n  \n    60 \n    80 \n    43 \n    125 \n    90.43469 \n  \n  \n    54 \n    13 \n    106 \n    143 \n    115.62032 \n  \n  \n    69 \n    64 \n    161 \n    161 \n    186.36874 \n  \n  \n    54 \n    125 \n    107 \n    98 \n    164.78997 \n  \n  \n    52 \n    65 \n    157 \n    183 \n    182.04486 \n  \n  \n    36 \n    138 \n    72 \n    152 \n    138.72937 \n  \n  \n    36 \n    131 \n    116 \n    203 \n    173.80034 \n  \n  \n    39 \n    105 \n    116 \n    158 \n    162.81500 \n  \n  \n    50 \n    71 \n    72 \n    65 \n    110.89761 \n  \n  \n    55 \n    110 \n    106 \n    133 \n    157.53103 \n  \n  \n    33 \n    89 \n    134 \n    122 \n    171.04054 \n  \n  \n    52 \n    68 \n    135 \n    138 \n    164.29372 \n  \n  \n    59 \n    97 \n    105 \n    132 \n    151.36275 \n  \n  \n    42 \n    101 \n    165 \n    174 \n    203.73632 \n  \n  \n    39 \n    76 \n    97 \n    173 \n    133.85978 \n  \n  \n    45 \n    63 \n    131 \n    124 \n    158.14239 \n  \n  \n    55 \n    71 \n    111 \n    139 \n    145.03931 \n  \n  \n    69 \n    55 \n    82 \n    119 \n    114.09836 \n  \n  \n    72 \n    62 \n    104 \n    171 \n    136.39042 \n  \n  \n    61 \n    108 \n    98 \n    160 \n    150.19917 \n  \n  \n    55 \n    90 \n    49 \n    117 \n    99.56150 \n  \n  \n    35 \n    84 \n    107 \n    178 \n    145.66277 \n  \n  \n    59 \n    36 \n    95 \n    122 \n    116.39753 \n  \n  \n    44 \n    74 \n    87 \n    144 \n    124.72053 \n  \n  \n    43 \n    78 \n    101 \n    124 \n    138.48918 \n  \n  \n    66 \n    142 \n    101 \n    244 \n    167.84005 \n  \n  \n    38 \n    71 \n    63 \n    96 \n    102.19417 \n  \n  \n    62 \n    96 \n    147 \n    248 \n    187.51815 \n  \n  \n    46 \n    63 \n    120 \n    194 \n    148.69592 \n  \n  \n    59 \n    105 \n    157 \n    198 \n    199.82845 \n  \n  \n    68 \n    72 \n    111 \n    160 \n    146.45894 \n  \n  \n    54 \n    86 \n    79 \n    124 \n    123.73077 \n  \n  \n    44 \n    66 \n    102 \n    136 \n    134.25546 \n  \n  \n    41 \n    87 \n    91 \n    127 \n    133.56188 \n  \n  \n    15 \n    97 \n    132 \n    193 \n    171.39099 \n  \n  \n    52 \n    88 \n    107 \n    93 \n    148.68036 \n  \n  \n    53 \n    92 \n    118 \n    182 \n    160.00402 \n  \n  \n    54 \n    71 \n    107 \n    114 \n    141.50056 \n  \n  \n    75 \n    71 \n    108 \n    147 \n    143.96279 \n  \n  \n    52 \n    54 \n    98 \n    122 \n    126.22552 \n  \n  \n    41 \n    72 \n    113 \n    172 \n    146.13759 \n  \n  \n    52 \n    74 \n    128 \n    108 \n    160.82167 \n  \n  \n    64 \n    95 \n    125 \n    158 \n    168.19393 \n  \n  \n    64 \n    122 \n    78 \n    168 \n    139.15162 \n  \n  \n    29 \n    57 \n    99 \n    101 \n    126.63646 \n  \n  \n    50 \n    107 \n    91 \n    163 \n    142.87183 \n  \n  \n    62 \n    98 \n    171 \n    197 \n    209.15707 \n  \n  \n    45 \n    65 \n    134 \n    162 \n    161.60200 \n  \n  \n    49 \n    93 \n    129 \n    174 \n    169.65369 \n  \n  \n    70 \n    37 \n    76 \n    137 \n    101.21716 \n  \n  \n    40 \n    71 \n    98 \n    154 \n    132.64506 \n  \n  \n    50 \n    105 \n    81 \n    154 \n    133.35245 \n  \n  \n    53 \n    66 \n    60 \n    126 \n    98.58109 \n  \n  \n    70 \n    110 \n    44 \n    105 \n    104.99919 \n  \n  \n    65 \n    37 \n    101 \n    102 \n    122.47906 \n  \n  \n    79 \n    122 \n    69 \n    92 \n    132.50088 \n  \n  \n    60 \n    63 \n    103 \n    131 \n    135.04371 \n  \n  \n    50 \n    114 \n    118 \n    189 \n    169.26422 \n  \n  \n    66 \n    94 \n    112 \n    101 \n    156.66084 \n  \n  \n    46 \n    156 \n    135 \n    184 \n    201.79068 \n  \n  \n    31 \n    85 \n    57 \n    108 \n    102.50589 \n  \n  \n    82 \n    99 \n    184 \n    211 \n    222.36274 \n  \n  \n    63 \n    71 \n    102 \n    172 \n    137.85639 \n  \n  \n    33 \n    78 \n    78 \n    144 \n    117.81825 \n  \n  \n    48 \n    39 \n    67 \n    88 \n    92.61602 \n  \n  \n    31 \n    80 \n    133 \n    153 \n    166.14124 \n  \n  \n    59 \n    67 \n    50 \n    86 \n    90.81172 \n  \n  \n    60 \n    128 \n    94 \n    171 \n    155.28613 \n  \n  \n    60 \n    121 \n    174 \n    263 \n    221.52163 \n  \n  \n    26 \n    108 \n    63 \n    158 \n    117.23941 \n  \n  \n    36 \n    129 \n    61 \n    146 \n    125.32530 \n  \n  \n    63 \n    83 \n    115 \n    144 \n    154.28567 \n  \n  \n    48 \n    74 \n    87 \n    111 \n    125.02463 \n  \n  \n    26 \n    132 \n    88 \n    81 \n    149.23229 \n  \n  \n    39 \n    125 \n    136 \n    201 \n    188.75433 \n  \n  \n    46 \n    121 \n    113 \n    126 \n    167.65071 \n  \n  \n    54 \n    79 \n    103 \n    150 \n    141.48812 \n  \n  \n    70 \n    95 \n    102 \n    136 \n    148.73942 \n  \n  \n    50 \n    85 \n    85 \n    132 \n    128.18946 \n  \n  \n    34 \n    74 \n    144 \n    176 \n    173.30410 \n  \n  \n    62 \n    97 \n    61 \n    168 \n    113.50085 \n  \n  \n    49 \n    56 \n    121 \n    141 \n    146.77068 \n  \n  \n    10 \n    36 \n    132 \n    153 \n    144.70245 \n  \n  \n    50 \n    92 \n    99 \n    143 \n    143.32800 \n  \n  \n    23 \n    102 \n    130 \n    170 \n    172.42426 \n  \n  \n    74 \n    126 \n    84 \n    110 \n    146.83111 \n  \n  \n    31 \n    88 \n    124 \n    142 \n    161.80039 \n  \n  \n    38 \n    149 \n    86 \n    123 \n    155.74509 \n  \n  \n    70 \n    102 \n    67 \n    136 \n    121.45958 \n  \n  \n    42 \n    104 \n    67 \n    111 \n    120.19341 \n  \n  \n    50 \n    103 \n    155 \n    248 \n    196.55029 \n  \n  \n    56 \n    26 \n    90 \n    97 \n    107.52819 \n  \n  \n    43 \n    77 \n    110 \n    100 \n    145.84903 \n  \n  \n    38 \n    83 \n    89 \n    106 \n    129.87730 \n  \n  \n    70 \n    86 \n    32 \n    88 \n    84.26017 \n  \n  \n    50 \n    93 \n    103 \n    113 \n    147.22201 \n  \n  \n    45 \n    92 \n    133 \n    186 \n    172.38103 \n  \n  \n    50 \n    123 \n    90 \n    146 \n    148.90671 \n  \n  \n    73 \n    105 \n    146 \n    204 \n    191.37033 \n  \n  \n    59 \n    78 \n    47 \n    79 \n    92.95881 \n  \n  \n    44 \n    49 \n    87 \n    116 \n    113.93839 \n  \n  \n    34 \n    58 \n    85 \n    59 \n    115.32834 \n  \n  \n    48 \n    161 \n    151 \n    289 \n    217.95006 \n  \n  \n    53 \n    18 \n    96 \n    70 \n    109.04391 \n  \n  \n    47 \n    101 \n    140 \n    183 \n    182.47442 \n  \n  \n    46 \n    149 \n    106 \n    189 \n    173.66693 \n  \n  \n    25 \n    121 \n    87 \n    83 \n    143.54644 \n  \n  \n    71 \n    136 \n    156 \n    171 \n    213.24494 \n  \n  \n    27 \n    114 \n    103 \n    169 \n    154.53040 \n  \n  \n    61 \n    106 \n    108 \n    110 \n    157.99341 \n  \n  \n    36 \n    108 \n    121 \n    113 \n    168.20918 \n  \n  \n    66 \n    104 \n    91 \n    124 \n    142.79439 \n  \n  \n    42 \n    27 \n    95 \n    129 \n    111.22351 \n  \n  \n    58 \n    116 \n    116 \n    174 \n    169.00364 \n  \n  \n    48 \n    72 \n    54 \n    165 \n    95.59458 \n  \n  \n    80 \n    143 \n    137 \n    247 \n    200.50023 \n  \n  \n    52 \n    133 \n    87 \n    159 \n    150.77458 \n  \n  \n    50 \n    37 \n    80 \n    41 \n    103.15936 \n  \n  \n    27 \n    128 \n    75 \n    127 \n    136.32932 \n  \n  \n    44 \n    116 \n    98 \n    187 \n    152.35701 \n  \n  \n    60 \n    101 \n    68 \n    100 \n    121.13371 \n  \n  \n    64 \n    97 \n    64 \n    150 \n    116.24995 \n  \n  \n    35 \n    120 \n    120 \n    189 \n    172.44290 \n  \n  \n    34 \n    94 \n    56 \n    68 \n    105.74986 \n  \n  \n    57 \n    116 \n    88 \n    134 \n    144.68854 \n  \n  \n    76 \n    163 \n    60 \n    100 \n    142.16437 \n  \n  \n    48 \n    17 \n    90 \n    70 \n    103.03841 \n  \n  \n    74 \n    39 \n    55 \n    104 \n    84.20453 \n  \n  \n    54 \n    46 \n    100 \n    125 \n    124.65866 \n  \n  \n    47 \n    70 \n    147 \n    168 \n    175.16434 \n  \n  \n    57 \n    111 \n    133 \n    166 \n    181.48777 \n  \n  \n    56 \n    89 \n    146 \n    210 \n    183.17732 \n  \n  \n    25 \n    73 \n    137 \n    137 \n    166.12881 \n  \n  \n    73 \n    26 \n    91 \n    79 \n    109.68631 \n  \n  \n    64 \n    63 \n    119 \n    105 \n    149.19871 \n  \n  \n    67 \n    64 \n    118 \n    135 \n    148.99240 \n  \n  \n    43 \n    117 \n    64 \n    140 \n    123.27911 \n  \n  \n    47 \n    123 \n    99 \n    154 \n    156.46977 \n  \n  \n    57 \n    107 \n    54 \n    118 \n    111.37381 \n  \n  \n    21 \n    44 \n    114 \n    144 \n    133.40676 \n  \n  \n    50 \n    71 \n    66 \n    163 \n    105.70352 \n  \n  \n    54 \n    116 \n    127 \n    144 \n    178.22203 \n  \n  \n    76 \n    137 \n    97 \n    139 \n    162.98116 \n  \n  \n    48 \n    42 \n    61 \n    109 \n    88.71579 \n  \n  \n    32 \n    97 \n    97 \n    142 \n    142.38459 \n  \n  \n    70 \n    57 \n    95 \n    116 \n    126.29081 \n  \n  \n    60 \n    123 \n    143 \n    206 \n    195.54808 \n  \n  \n    76 \n    122 \n    57 \n    164 \n    121.88463 \n  \n  \n    55 \n    87 \n    138 \n    171 \n    175.31327 \n  \n  \n    46 \n    89 \n    133 \n    183 \n    171.16320 \n  \n  \n    59 \n    129 \n    63 \n    93 \n    128.80527 \n  \n  \n    64 \n    90 \n    115 \n    123 \n    157.38069 \n  \n  \n    54 \n    106 \n    93 \n    118 \n    144.47601 \n  \n  \n    47 \n    106 \n    95 \n    164 \n    145.67519 \n  \n  \n    71 \n    109 \n    105 \n    148 \n    157.45049 \n  \n  \n    54 \n    86 \n    41 \n    125 \n    90.83488 \n  \n  \n    59 \n    42 \n    102 \n    156 \n    125.04501 \n  \n  \n    36 \n    111 \n    61 \n    73 \n    117.56217 \n  \n  \n    50 \n    120 \n    112 \n    180 \n    166.65784 \n  \n  \n    57 \n    65 \n    111 \n    89 \n    142.60365 \n  \n  \n    58 \n    99 \n    104 \n    188 \n    151.28361 \n  \n  \n    30 \n    73 \n    105 \n    110 \n    138.80714 \n  \n  \n    78 \n    120 \n    93 \n    185 \n    152.33864 \n  \n  \n    36 \n    93 \n    139 \n    157 \n    177.32217 \n  \n  \n    56 \n    112 \n    79 \n    128 \n    135.09624 \n  \n  \n    42 \n    111 \n    121 \n    172 \n    169.95920 \n  \n  \n    76 \n    83 \n    122 \n    147 \n    161.33378 \n  \n  \n    65 \n    71 \n    117 \n    171 \n    150.99366 \n  \n  \n    40 \n    118 \n    115 \n    97 \n    167.63206 \n  \n  \n    56 \n    59 \n    68 \n    105 \n    102.71562 \n  \n  \n    56 \n    103 \n    55 \n    105 \n    110.43832 \n  \n  \n    81 \n    100 \n    111 \n    153 \n    159.52327 \n  \n  \n    42 \n    105 \n    103 \n    157 \n    151.78922 \n  \n  \n    57 \n    124 \n    100 \n    174 \n    158.52699 \n  \n  \n    58 \n    52 \n    168 \n    133 \n    186.41680 \n  \n  \n    27 \n    65 \n    93 \n    143 \n    124.74060 \n  \n  \n    38 \n    95 \n    70 \n    81 \n    118.60478 \n  \n  \n    41 \n    98 \n    119 \n    155 \n    162.54510 \n  \n  \n    65 \n    107 \n    154 \n    196 \n    198.55014 \n  \n  \n    60 \n    75 \n    84 \n    129 \n    123.77119 \n  \n  \n    50 \n    136 \n    153 \n    229 \n    209.05134 \n  \n  \n    24 \n    114 \n    164 \n    286 \n    207.10887 \n  \n  \n    55 \n    83 \n    65 \n    106 \n    110.39340 \n  \n  \n    73 \n    135 \n    71 \n    115 \n    139.38280 \n  \n  \n    28 \n    111 \n    120 \n    230 \n    168.02915 \n  \n  \n    52 \n    105 \n    119 \n    167 \n    166.40038 \n  \n  \n    56 \n    95 \n    185 \n    237 \n    219.52660 \n  \n  \n    38 \n    114 \n    90 \n    107 \n    144.11283 \n  \n  \n    21 \n    84 \n    125 \n    154 \n    160.18067 \n  \n  \n    48 \n    92 \n    145 \n    172 \n    182.99728 \n  \n  \n    50 \n    39 \n    135 \n    171 \n    151.63440 \n  \n  \n    50 \n    52 \n    129 \n    115 \n    152.04702 \n  \n  \n    61 \n    60 \n    151 \n    146 \n    175.37858 \n  \n  \n    61 \n    123 \n    94 \n    162 \n    153.20573 \n  \n  \n    65 \n    126 \n    139 \n    198 \n    193.75934 \n  \n  \n    42 \n    117 \n    86 \n    123 \n    142.24807 \n  \n  \n    50 \n    106 \n    163 \n    224 \n    204.76959 \n  \n  \n    46 \n    49 \n    126 \n    132 \n    147.85201 \n  \n  \n    59 \n    61 \n    54 \n    135 \n    91.68673 \n  \n  \n    62 \n    90 \n    140 \n    201 \n    178.87067 \n  \n  \n    85 \n    156 \n    88 \n    102 \n    164.06869 \n  \n  \n    72 \n    131 \n    105 \n    152 \n    167.01479 \n  \n  \n    53 \n    132 \n    113 \n    203 \n    172.92703 \n  \n  \n    21 \n    116 \n    67 \n    102 \n    123.77229 \n  \n  \n    45 \n    44 \n    160 \n    144 \n    175.05272 \n  \n  \n    59 \n    95 \n    67 \n    101 \n    117.60429 \n  \n  \n    76 \n    91 \n    61 \n    86 \n    111.97751 \n  \n  \n    53 \n    110 \n    109 \n    179 \n    159.97603 \n  \n  \n    71 \n    46 \n    147 \n    74 \n    166.63812 \n  \n  \n    69 \n    110 \n    123 \n    138 \n    173.31198 \n  \n  \n    37 \n    101 \n    170 \n    222 \n    207.68459 \n  \n  \n    50 \n    132 \n    76 \n    124 \n    140.66875 \n  \n  \n    56 \n    83 \n    94 \n    97 \n    135.57418 \n  \n  \n    68 \n    100 \n    132 \n    181 \n    176.71423 \n  \n  \n    94 \n    120 \n    112 \n    141 \n    170.00300 \n  \n  \n    37 \n    90 \n    132 \n    154 \n    170.04457 \n  \n  \n    24 \n    99 \n    137 \n    185 \n    177.26620 \n  \n  \n    62 \n    68 \n    98 \n    135 \n    133.02378 \n  \n  \n    77 \n    124 \n    84 \n    161 \n    146.19662 \n  \n  \n    29 \n    70 \n    139 \n    185 \n    166.87042 \n  \n  \n    48 \n    84 \n    127 \n    182 \n    163.96474 \n  \n  \n    63 \n    124 \n    97 \n    127 \n    156.38611 \n  \n  \n    47 \n    136 \n    97 \n    104 \n    160.34511 \n  \n  \n    50 \n    70 \n    27 \n    170 \n    71.51067 \n  \n  \n    17 \n    97 \n    156 \n    226 \n    192.31939 \n  \n  \n    45 \n    92 \n    37 \n    128 \n    89.27563 \n  \n  \n    41 \n    65 \n    117 \n    177 \n    146.58132 \n  \n  \n    79 \n    108 \n    84 \n    152 \n    139.44811 \n  \n  \n    71 \n    84 \n    57 \n    140 \n    105.11565 \n  \n  \n    75 \n    30 \n    54 \n    170 \n    79.53330 \n  \n  \n    65 \n    74 \n    81 \n    116 \n    121.12299 \n  \n  \n    52 \n    97 \n    58 \n    118 \n    110.14355 \n  \n  \n    67 \n    72 \n    148 \n    181 \n    178.41312 \n  \n  \n    67 \n    38 \n    142 \n    135 \n    158.55532 \n  \n  \n    56 \n    88 \n    144 \n    141 \n    181.01467 \n  \n  \n    61 \n    123 \n    152 \n    221 \n    203.41524 \n  \n  \n    50 \n    110 \n    164 \n    206 \n    207.36041 \n  \n  \n    35 \n    81 \n    65 \n    90 \n    108.01030 \n  \n  \n    71 \n    89 \n    133 \n    167 \n    173.06385 \n  \n  \n    52 \n    82 \n    107 \n    111 \n    146.09265 \n  \n  \n    63 \n    106 \n    74 \n    160 \n    128.71230 \n  \n  \n    72 \n    97 \n    64 \n    135 \n    116.85816 \n  \n  \n    56 \n    128 \n    165 \n    206 \n    216.44539 \n  \n  \n    64 \n    68 \n    140 \n    214 \n    169.53445 \n  \n  \n    62 \n    85 \n    80 \n    99 \n    124.77337 \n  \n  \n    73 \n    114 \n    111 \n    182 \n    164.95305 \n  \n  \n    40 \n    121 \n    140 \n    216 \n    190.56794 \n  \n  \n    57 \n    144 \n    56 \n    127 \n    129.06273 \n  \n  \n    45 \n    85 \n    102 \n    78 \n    142.52591 \n  \n  \n    53 \n    25 \n    77 \n    128 \n    95.61497 \n  \n  \n    56 \n    65 \n    144 \n    124 \n    171.09510 \n  \n  \n    27 \n    100 \n    149 \n    196 \n    188.31374 \n  \n  \n    43 \n    50 \n    63 \n    121 \n    93.51730 \n  \n  \n    77 \n    80 \n    73 \n    166 \n    117.69757 \n  \n  \n    50 \n    61 \n    89 \n    127 \n    121.30134 \n  \n  \n    47 \n    102 \n    135 \n    225 \n    178.57730 \n  \n  \n    63 \n    85 \n    118 \n    164 \n    157.74528 \n  \n  \n    64 \n    161 \n    95 \n    164 \n    170.68833 \n  \n  \n    59 \n    65 \n    85 \n    119 \n    120.24799 \n  \n  \n    49 \n    118 \n    55 \n    42 \n    116.37542 \n  \n  \n    71 \n    73 \n    131 \n    126 \n    164.43192 \n  \n  \n    65 \n    106 \n    128 \n    146 \n    175.61114 \n  \n  \n    41 \n    131 \n    169 \n    237 \n    220.06158 \n  \n  \n    71 \n    73 \n    151 \n    190 \n    181.74555 \n  \n  \n    44 \n    134 \n    99 \n    120 \n    160.98583 \n  \n  \n    39 \n    56 \n    52 \n    95 \n    86.27842 \n  \n  \n    99 \n    87 \n    158 \n    238 \n    195.97205 \n  \n  \n    17 \n    106 \n    87 \n    134 \n    136.46895 \n  \n  \n    41 \n    62 \n    68 \n    126 \n    102.86908 \n  \n  \n    47 \n    119 \n    175 \n    231 \n    220.53640 \n  \n  \n    65 \n    103 \n    72 \n    93 \n    125.83914 \n  \n  \n    57 \n    94 \n    147 \n    234 \n    186.27545 \n  \n  \n    36 \n    66 \n    152 \n    201 \n    176.93132 \n  \n  \n    39 \n    103 \n    135 \n    159 \n    178.40037 \n  \n  \n    31 \n    123 \n    91 \n    167 \n    148.32790 \n  \n  \n    44 \n    78 \n    151 \n    137 \n    181.84927 \n  \n  \n    19 \n    98 \n    87 \n    154 \n    133.17072 \n  \n  \n    39 \n    119 \n    114 \n    157 \n    167.12163 \n  \n  \n    42 \n    116 \n    143 \n    271 \n    191.16061 \n  \n  \n    33 \n    128 \n    111 \n    179 \n    167.95000 \n  \n  \n    46 \n    126 \n    161 \n    271 \n    211.35983 \n  \n  \n    48 \n    125 \n    106 \n    154 \n    163.46813 \n  \n  \n    56 \n    83 \n    49 \n    162 \n    96.61853 \n  \n  \n    60 \n    73 \n    117 \n    162 \n    151.47610 \n  \n  \n    61 \n    100 \n    90 \n    151 \n    139.82344 \n  \n  \n    48 \n    109 \n    85 \n    164 \n    138.38826 \n  \n  \n    42 \n    96 \n    136 \n    160 \n    176.47513 \n  \n  \n    54 \n    92 \n    56 \n    66 \n    106.40781 \n  \n  \n    84 \n    98 \n    111 \n    182 \n    158.88878 \n  \n  \n    77 \n    126 \n    121 \n    195 \n    179.08940 \n  \n  \n    53 \n    62 \n    120 \n    141 \n    148.79682 \n  \n  \n    58 \n    98 \n    95 \n    132 \n    143.06119 \n  \n  \n    29 \n    65 \n    57 \n    109 \n    93.72813 \n  \n  \n    64 \n    82 \n    93 \n    114 \n    134.88542 \n  \n  \n    33 \n    78 \n    119 \n    97 \n    153.31118 \n  \n  \n    55 \n    108 \n    60 \n    109 \n    116.84713 \n  \n  \n    42 \n    102 \n    145 \n    206 \n    186.85398 \n  \n  \n    56 \n    140 \n    114 \n    178 \n    177.47107 \n  \n  \n    54 \n    90 \n    99 \n    105 \n    142.76953 \n  \n  \n    65 \n    84 \n    125 \n    160 \n    163.52582 \n  \n  \n    38 \n    73 \n    138 \n    170 \n    167.98283 \n  \n  \n    54 \n    92 \n    138 \n    155 \n    177.39367 \n  \n  \n    69 \n    133 \n    158 \n    228 \n    213.53039 \n  \n  \n    55 \n    92 \n    119 \n    152 \n    161.02175 \n  \n  \n    45 \n    97 \n    81 \n    191 \n    129.52203 \n  \n  \n    49 \n    139 \n    118 \n    201 \n    179.97033 \n  \n  \n    72 \n    79 \n    124 \n    146 \n    161.03589 \n  \n  \n    57 \n    88 \n    93 \n    91 \n    136.94095 \n  \n  \n    59 \n    45 \n    103 \n    122 \n    127.20454 \n  \n  \n    49 \n    109 \n    109 \n    226 \n    159.24064 \n  \n  \n    61 \n    119 \n    95 \n    158 \n    152.34627 \n  \n  \n    58 \n    120 \n    136 \n    184 \n    188.04240 \n  \n  \n    57 \n    105 \n    146 \n    210 \n    190.15391 \n  \n  \n    62 \n    114 \n    131 \n    227 \n    181.43039 \n  \n  \n    40 \n    85 \n    91 \n    109 \n    132.62329 \n  \n  \n    56 \n    86 \n    129 \n    200 \n    167.16688 \n  \n  \n    50 \n    120 \n    101 \n    152 \n    157.13535 \n  \n  \n    78 \n    133 \n    142 \n    209 \n    200.36373 \n  \n  \n    22 \n    52 \n    98 \n    152 \n    123.08217 \n  \n  \n    34 \n    65 \n    59 \n    66 \n    95.83962 \n  \n  \n    60 \n    125 \n    127 \n    230 \n    182.55975 \n  \n  \n    66 \n    77 \n    133 \n    138 \n    167.50830 \n  \n  \n    47 \n    81 \n    28 \n    77 \n    76.89241 \n  \n  \n    45 \n    111 \n    87 \n    185 \n    140.75411 \n  \n  \n    40 \n    109 \n    60 \n    57 \n    116.13802 \n  \n  \n    52 \n    97 \n    56 \n    132 \n    108.41218 \n  \n  \n    69 \n    71 \n    140 \n    167 \n    171.20843 \n  \n  \n    48 \n    107 \n    52 \n    123 \n    108.95821 \n  \n  \n    37 \n    117 \n    106 \n    167 \n    159.18156 \n  \n  \n    55 \n    73 \n    114 \n    152 \n    148.49892 \n  \n  \n    57 \n    42 \n    84 \n    124 \n    109.31069 \n  \n  \n    28 \n    101 \n    92 \n    144 \n    139.47722 \n  \n  \n    52 \n    23 \n    174 \n    153 \n    178.64745 \n  \n  \n    29 \n    110 \n    117 \n    110 \n    165.07685 \n  \n  \n    44 \n    115 \n    125 \n    161 \n    175.29912 \n  \n  \n    65 \n    87 \n    116 \n    148 \n    157.02854 \n  \n  \n    79 \n    87 \n    119 \n    138 \n    160.68996 \n  \n  \n    58 \n    104 \n    156 \n    159 \n    198.45546 \n  \n  \n    82 \n    97 \n    67 \n    111 \n    120.21546 \n  \n  \n    58 \n    110 \n    95 \n    123 \n    148.23662 \n  \n  \n    47 \n    92 \n    103 \n    190 \n    146.56264 \n  \n  \n    56 \n    81 \n    133 \n    182 \n    168.47318 \n  \n  \n    77 \n    103 \n    125 \n    179 \n    172.63256 \n  \n  \n    28 \n    109 \n    157 \n    239 \n    199.19678 \n  \n  \n    54 \n    92 \n    105 \n    139 \n    148.82619 \n  \n  \n    59 \n    81 \n    67 \n    114 \n    111.56629 \n  \n  \n    21 \n    110 \n    111 \n    132 \n    159.27455 \n  \n  \n    47 \n    26 \n    56 \n    129 \n    77.41079 \n  \n  \n    43 \n    8 \n    127 \n    82 \n    130.80692 \n  \n  \n    40 \n    92 \n    55 \n    111 \n    104.47776 \n  \n  \n    44 \n    92 \n    137 \n    105 \n    175.76773 \n  \n  \n    49 \n    108 \n    95 \n    129 \n    146.68981 \n  \n  \n    45 \n    56 \n    109 \n    132 \n    136.07840 \n  \n  \n    70 \n    134 \n    138 \n    191 \n    196.72408 \n  \n  \n    36 \n    57 \n    87 \n    105 \n    116.78047 \n  \n  \n    60 \n    103 \n    52 \n    94 \n    108.14538 \n  \n  \n    57 \n    65 \n    109 \n    175 \n    140.87229 \n  \n  \n    82 \n    40 \n    77 \n    85 \n    104.28901 \n  \n  \n    35 \n    105 \n    183 \n    229 \n    220.51154 \n  \n  \n    37 \n    48 \n    100 \n    107 \n    124.22878 \n  \n  \n    49 \n    113 \n    62 \n    59 \n    120.27876 \n  \n  \n    64 \n    87 \n    109 \n    162 \n    150.89275 \n  \n  \n    57 \n    96 \n    116 \n    114 \n    160.30190 \n  \n  \n    24 \n    115 \n    69 \n    62 \n    125.30044 \n  \n  \n    48 \n    99 \n    81 \n    163 \n    130.61268 \n  \n  \n    59 \n    89 \n    125 \n    174 \n    165.22609 \n  \n  \n    48 \n    102 \n    140 \n    145 \n    182.98173 \n  \n  \n    39 \n    79 \n    169 \n    173 \n    197.48268 \n  \n  \n    35 \n    19 \n    118 \n    96 \n    127.15171 \n  \n  \n    50 \n    77 \n    100 \n    108 \n    137.72440 \n  \n  \n    54 \n    136 \n    71 \n    167 \n    138.36959 \n  \n  \n    63 \n    108 \n    143 \n    187 \n    189.30688 \n  \n  \n    19 \n    82 \n    93 \n    150 \n    131.46424 \n  \n  \n    40 \n    117 \n    61 \n    134 \n    120.45398 \n  \n  \n    43 \n    83 \n    85 \n    128 \n    126.79471 \n  \n  \n    69 \n    93 \n    116 \n    172 \n    159.92036 \n  \n  \n    49 \n    32 \n    86 \n    149 \n    106.12099 \n  \n  \n    39 \n    84 \n    107 \n    94 \n    145.96688 \n  \n  \n    49 \n    127 \n    146 \n    238 \n    199.03398 \n  \n  \n    61 \n    133 \n    144 \n    279 \n    200.80264 \n  \n  \n    70 \n    57 \n    55 \n    85 \n    91.66356 \n  \n  \n    71 \n    84 \n    175 \n    219 \n    207.26604 \n  \n  \n    59 \n    143 \n    47 \n    81 \n    120.99236 \n  \n  \n    72 \n    102 \n    82 \n    143 \n    134.59685 \n  \n  \n    72 \n    128 \n    129 \n    180 \n    186.49729 \n  \n  \n    52 \n    46 \n    101 \n    147 \n    125.37228 \n  \n  \n    3 \n    52 \n    63 \n    128 \n    91.33883 \n  \n  \n    56 \n    171 \n    143 \n    220 \n    215.94567 \n  \n  \n    52 \n    130 \n    45 \n    48 \n    113.12211 \n  \n  \n    62 \n    132 \n    75 \n    109 \n    140.71538 \n  \n  \n    38 \n    94 \n    146 \n    146 \n    183.96527 \n  \n  \n    49 \n    63 \n    75 \n    118 \n    109.96835 \n  \n  \n    86 \n    89 \n    102 \n    171 \n    147.36813 \n  \n  \n    50 \n    107 \n    102 \n    167 \n    152.39432 \n  \n  \n    33 \n    68 \n    120 \n    126 \n    149.86401 \n  \n  \n    17 \n    80 \n    65 \n    79 \n    106.21055 \n  \n  \n    41 \n    98 \n    120 \n    147 \n    163.41078 \n  \n  \n    66 \n    141 \n    102 \n    178 \n    168.27444 \n  \n  \n    84 \n    101 \n    106 \n    110 \n    155.85423 \n  \n  \n    32 \n    82 \n    98 \n    134 \n    136.78099 \n  \n  \n    35 \n    102 \n    166 \n    220 \n    204.50110 \n  \n  \n    45 \n    65 \n    81 \n    137 \n    115.72090 \n  \n  \n    41 \n    132 \n    107 \n    188 \n    166.82063 \n  \n  \n    80 \n    100 \n    58 \n    128 \n    113.56614 \n  \n  \n    55 \n    103 \n    70 \n    91 \n    123.34751 \n  \n  \n    60 \n    105 \n    84 \n    151 \n    136.70975 \n  \n  \n    62 \n    76 \n    106 \n    123 \n    143.39951 \n  \n  \n    25 \n    106 \n    85 \n    100 \n    135.34580 \n  \n  \n    57 \n    71 \n    57 \n    83 \n    98.44458 \n  \n  \n    62 \n    104 \n    117 \n    212 \n    164.99800 \n  \n  \n    53 \n    187 \n    75 \n    108 \n    163.75184 \n  \n  \n    71 \n    95 \n    113 \n    151 \n    158.33794 \n  \n  \n    26 \n    107 \n    101 \n    184 \n    149.70401 \n  \n  \n    44 \n    99 \n    100 \n    106 \n    146.75652 \n  \n  \n    76 \n    76 \n    112 \n    193 \n    149.65797 \n  \n  \n    42 \n    106 \n    160 \n    177 \n    201.56434 \n  \n  \n    57 \n    129 \n    100 \n    172 \n    160.68342 \n  \n  \n    47 \n    94 \n    100 \n    153 \n    144.82817 \n  \n  \n    58 \n    82 \n    108 \n    106 \n    147.41448 \n  \n  \n    50 \n    51 \n    85 \n    173 \n    113.52576 \n  \n  \n    65 \n    110 \n    144 \n    164 \n    191.18718 \n  \n  \n    44 \n    136 \n    79 \n    145 \n    144.53477 \n  \n  \n    43 \n    103 \n    97 \n    132 \n    145.80859 \n  \n  \n    64 \n    123 \n    85 \n    126 \n    145.64267 \n  \n  \n    50 \n    167 \n    106 \n    177 \n    181.73417 \n  \n  \n    61 \n    60 \n    112 \n    106 \n    141.61701 \n  \n  \n    52 \n    108 \n    98 \n    174 \n    149.51494 \n  \n  \n    75 \n    44 \n    108 \n    130 \n    132.31808 \n  \n  \n    67 \n    123 \n    110 \n    226 \n    167.51278 \n  \n  \n    39 \n    53 \n    102 \n    129 \n    128.26862 \n  \n  \n    52 \n    102 \n    81 \n    156 \n    132.21064 \n  \n  \n    48 \n    91 \n    43 \n    86 \n    94.26651 \n  \n  \n    76 \n    97 \n    138 \n    154 \n    181.22267 \n  \n  \n    52 \n    65 \n    108 \n    169 \n    139.62648 \n  \n  \n    39 \n    134 \n    141 \n    195 \n    196.96431 \n  \n  \n    39 \n    104 \n    80 \n    106 \n    131.21919 \n  \n  \n    49 \n    104 \n    27 \n    53 \n    86.09835 \n  \n  \n    68 \n    90 \n    117 \n    135 \n    159.41616 \n  \n  \n    56 \n    138 \n    141 \n    256 \n    199.98189 \n  \n  \n    61 \n    76 \n    77 \n    110 \n    118.21873 \n  \n  \n    88 \n    85 \n    81 \n    123 \n    127.61573 \n  \n  \n    43 \n    96 \n    151 \n    198 \n    189.53638 \n  \n  \n    91 \n    103 \n    110 \n    159 \n    160.71170 \n  \n  \n    26 \n    74 \n    70 \n    124 \n    108.63548 \n  \n  \n    8 \n    31 \n    126 \n    110 \n    137.19988 \n  \n  \n    36 \n    79 \n    77 \n    116 \n    117.61193 \n  \n  \n    43 \n    57 \n    85 \n    126 \n    115.58129 \n  \n  \n    54 \n    78 \n    150 \n    220 \n    181.74385 \n  \n  \n    40 \n    70 \n    123 \n    171 \n    153.85581 \n  \n  \n    17 \n    71 \n    76 \n    110 \n    111.85147 \n  \n  \n    63 \n    63 \n    96 \n    89 \n    129.21202 \n  \n  \n    67 \n    79 \n    86 \n    182 \n    127.75988 \n  \n  \n    50 \n    85 \n    82 \n    169 \n    125.59242 \n  \n  \n    35 \n    139 \n    92 \n    183 \n    156.39825 \n  \n  \n    55 \n    96 \n    72 \n    157 \n    122.05988 \n  \n  \n    65 \n    50 \n    104 \n    129 \n    130.68281 \n  \n  \n    58 \n    96 \n    125 \n    171 \n    168.16906 \n  \n  \n    63 \n    89 \n    111 \n    198 \n    153.41066 \n  \n  \n    30 \n    118 \n    90 \n    125 \n    145.22976 \n  \n  \n    33 \n    160 \n    8 \n    8 \n    92.58597 \n  \n  \n    62 \n    53 \n    139 \n    163 \n    162.04743 \n  \n  \n    57 \n    133 \n    41 \n    76 \n    111.33337 \n  \n  \n    50 \n    105 \n    52 \n    107 \n    108.24769 \n  \n  \n    46 \n    113 \n    117 \n    154 \n    167.66315 \n  \n  \n    72 \n    97 \n    69 \n    114 \n    121.18656 \n  \n  \n    65 \n    123 \n    89 \n    207 \n    149.18143 \n  \n  \n    42 \n    164 \n    141 \n    295 \n    210.13095 \n  \n  \n    54 \n    87 \n    147 \n    192 \n    183.02837 \n  \n  \n    81 \n    64 \n    107 \n    159 \n    140.53427 \n  \n  \n    59 \n    120 \n    88 \n    149 \n    146.56573 \n  \n  \n    42 \n    99 \n    98 \n    156 \n    144.87310 \n  \n  \n    47 \n    78 \n    133 \n    163 \n    166.49509 \n  \n  \n    44 \n    51 \n    96 \n    135 \n    122.59210 \n  \n  \n    56 \n    60 \n    123 \n    153 \n    150.75937 \n  \n  \n    22 \n    96 \n    79 \n    76 \n    125.61078 \n  \n  \n    55 \n    63 \n    47 \n    106 \n    86.18543 \n  \n  \n    56 \n    98 \n    48 \n    115 \n    102.22212 \n  \n  \n    50 \n    102 \n    108 \n    135 \n    155.43198 \n  \n  \n    57 \n    112 \n    94 \n    142 \n    148.15748 \n  \n  \n    62 \n    146 \n    78 \n    153 \n    149.35042 \n  \n  \n    61 \n    80 \n    183 \n    189 \n    211.70608 \n  \n  \n    81 \n    89 \n    125 \n    157 \n    166.89867 \n  \n  \n    59 \n    116 \n    109 \n    165 \n    163.01990 \n  \n  \n    41 \n    112 \n    116 \n    147 \n    165.98605 \n  \n  \n    49 \n    97 \n    83 \n    122 \n    131.55750 \n  \n  \n    33 \n    59 \n    112 \n    111 \n    139.05699 \n  \n  \n    72 \n    142 \n    64 \n    96 \n    136.26600 \n  \n  \n    38 \n    128 \n    133 \n    167 \n    187.37512 \n  \n  \n    52 \n    162 \n    140 \n    269 \n    209.16296 \n  \n  \n    61 \n    100 \n    72 \n    134 \n    124.24118 \n  \n  \n    37 \n    133 \n    165 \n    267 \n    217.15732 \n  \n  \n    71 \n    127 \n    66 \n    85 \n    131.45206 \n  \n  \n    56 \n    117 \n    152 \n    265 \n    200.44739 \n  \n  \n    62 \n    97 \n    117 \n    157 \n    161.97900 \n  \n  \n    56 \n    81 \n    183 \n    194 \n    211.75724 \n  \n  \n    50 \n    67 \n    173 \n    154 \n    196.60627 \n  \n  \n    56 \n    129 \n    98 \n    105 \n    158.87603 \n  \n  \n    29 \n    137 \n    82 \n    146 \n    146.42271 \n  \n  \n    40 \n    41 \n    143 \n    151 \n    158.66215 \n  \n  \n    63 \n    69 \n    53 \n    109 \n    94.57544 \n  \n  \n    81 \n    57 \n    99 \n    120 \n    130.58982 \n  \n  \n    67 \n    156 \n    154 \n    232 \n    219.83518 \n  \n  \n    38 \n    145 \n    103 \n    148 \n    168.73653 \n  \n  \n    52 \n    85 \n    56 \n    173 \n    103.23676 \n  \n  \n    56 \n    78 \n    100 \n    124 \n    138.61184 \n  \n  \n    42 \n    74 \n    104 \n    164 \n    139.28506 \n  \n  \n    31 \n    85 \n    43 \n    88 \n    90.38635 \n  \n  \n    50 \n    73 \n    67 \n    102 \n    107.43178 \n  \n  \n    63 \n    123 \n    85 \n    164 \n    145.56665 \n  \n  \n    75 \n    177 \n    100 \n    151 \n    182.75359 \n  \n  \n    69 \n    53 \n    49 \n    130 \n    84.66830 \n  \n  \n    42 \n    91 \n    45 \n    90 \n    95.54172 \n  \n  \n    55 \n    73 \n    70 \n    172 \n    110.40895 \n  \n  \n    57 \n    95 \n    95 \n    151 \n    141.69131 \n  \n  \n    54 \n    84 \n    110 \n    125 \n    149.70431 \n  \n  \n    57 \n    122 \n    98 \n    202 \n    155.93306 \n  \n  \n    65 \n    106 \n    96 \n    95 \n    147.90934 \n  \n  \n    40 \n    27 \n    44 \n    80 \n    66.92172 \n  \n  \n    75 \n    85 \n    105 \n    113 \n    147.40374 \n  \n  \n    53 \n    114 \n    112 \n    176 \n    164.29821 \n  \n  \n    75 \n    49 \n    70 \n    139 \n    101.57863 \n  \n  \n    44 \n    117 \n    84 \n    144 \n    140.66876 \n  \n  \n    66 \n    71 \n    91 \n    135 \n    128.56197 \n  \n  \n    52 \n    92 \n    142 \n    188 \n    180.70434 \n  \n  \n    54 \n    48 \n    131 \n    165 \n    152.35734 \n  \n  \n    57 \n    54 \n    65 \n    86 \n    98.03817 \n  \n  \n    40 \n    80 \n    80 \n    162 \n    120.94437 \n  \n  \n    66 \n    147 \n    94 \n    120 \n    163.93671 \n  \n  \n    61 \n    68 \n    64 \n    159 \n    103.51459 \n  \n  \n    27 \n    113 \n    196 \n    193 \n    234.60747 \n  \n  \n    60 \n    102 \n    139 \n    114 \n    183.02836 \n  \n  \n    49 \n    62 \n    79 \n    63 \n    112.99979 \n  \n  \n    73 \n    100 \n    74 \n    140 \n    126.88485 \n  \n  \n    57 \n    48 \n    90 \n    175 \n    117.09249 \n  \n  \n    22 \n    116 \n    152 \n    235 \n    197.43122 \n  \n  \n    34 \n    83 \n    158 \n    230 \n    189.30520 \n  \n  \n    53 \n    72 \n    174 \n    202 \n    199.85646 \n  \n  \n    43 \n    96 \n    168 \n    201 \n    204.25296 \n  \n  \n    10 \n    97 \n    69 \n    94 \n    116.47294 \n  \n  \n    29 \n    76 \n    97 \n    126 \n    133.09952 \n  \n  \n    69 \n    101 \n    112 \n    145 \n    159.90792 \n  \n  \n    55 \n    97 \n    40 \n    24 \n    94.78936 \n  \n  \n    71 \n    112 \n    91 \n    150 \n    146.62481 \n  \n  \n    47 \n    78 \n    127 \n    210 \n    161.30100 \n  \n  \n    54 \n    105 \n    165 \n    245 \n    206.37377 \n  \n  \n    66 \n    60 \n    146 \n    208 \n    171.43030 \n  \n  \n    47 \n    65 \n    66 \n    151 \n    102.88773 \n  \n  \n    74 \n    98 \n    55 \n    155 \n    109.65036 \n  \n  \n    43 \n    110 \n    51 \n    79 \n    109.00625 \n  \n  \n    74 \n    78 \n    93 \n    170 \n    133.92054 \n  \n  \n    61 \n    60 \n    86 \n    142 \n    119.10930 \n  \n  \n    59 \n    130 \n    68 \n    124 \n    133.56496 \n  \n  \n    38 \n    99 \n    118 \n    155 \n    161.88262 \n  \n  \n    67 \n    126 \n    164 \n    287 \n    215.55343 \n  \n  \n    39 \n    85 \n    104 \n    67 \n    143.80112 \n  \n  \n    54 \n    133 \n    72 \n    114 \n    137.94141 \n  \n  \n    70 \n    59 \n    82 \n    137 \n    115.89952 \n  \n  \n    42 \n    96 \n    130 \n    154 \n    171.28105 \n  \n  \n    64 \n    22 \n    111 \n    110 \n    124.59056 \n  \n  \n    44 \n    106 \n    72 \n    116 \n    125.53644 \n  \n  \n    33 \n    85 \n    74 \n    108 \n    117.37452 \n  \n  \n    71 \n    91 \n    67 \n    96 \n    116.79146 \n  \n  \n    63 \n    24 \n    105 \n    152 \n    120.18302 \n  \n  \n    42 \n    132 \n    43 \n    113 \n    111.49305 \n  \n  \n    47 \n    104 \n    96 \n    151 \n    145.67830 \n  \n  \n    59 \n    85 \n    132 \n    191 \n    169.56072 \n  \n  \n    50 \n    45 \n    122 \n    110 \n    142.96825 \n  \n  \n    71 \n    118 \n    149 \n    235 \n    199.42203 \n  \n  \n    46 \n    85 \n    144 \n    193 \n    178.96055 \n  \n  \n    53 \n    106 \n    134 \n    140 \n    179.89291 \n  \n  \n    68 \n    75 \n    88 \n    150 \n    127.84212 \n  \n  \n    42 \n    75 \n    137 \n    141 \n    168.28382 \n  \n  \n    61 \n    100 \n    125 \n    176 \n    170.12228 \n  \n  \n    96 \n    124 \n    115 \n    182 \n    174.47724 \n  \n  \n    73 \n    110 \n    164 \n    247 \n    209.10902 \n  \n  \n    69 \n    74 \n    115 \n    144 \n    150.86026 \n  \n  \n    64 \n    69 \n    58 \n    102 \n    98.97987 \n  \n  \n    64 \n    116 \n    113 \n    116 \n    166.86275 \n  \n  \n    40 \n    79 \n    105 \n    112 \n    142.15511 \n  \n  \n    55 \n    131 \n    96 \n    146 \n    157.93122 \n  \n  \n    62 \n    100 \n    83 \n    137 \n    133.83970 \n  \n  \n    37 \n    86 \n    157 \n    192 \n    189.96146 \n  \n  \n    36 \n    120 \n    130 \n    161 \n    181.17574 \n  \n  \n    36 \n    60 \n    86 \n    104 \n    117.20864 \n  \n  \n    57 \n    53 \n    101 \n    147 \n    128.77141 \n  \n  \n    18 \n    103 \n    33 \n    128 \n    88.50434 \n  \n  \n    72 \n    73 \n    103 \n    133 \n    140.26888 \n  \n  \n    11 \n    19 \n    104 \n    79 \n    113.20755 \n  \n  \n    58 \n    123 \n    101 \n    174 \n    159.03742 \n  \n  \n    68 \n    83 \n    44 \n    108 \n    93.20243 \n  \n  \n    43 \n    74 \n    180 \n    193 \n    205.15286 \n  \n  \n    10 \n    159 \n    84 \n    179 \n    156.19785 \n  \n  \n    39 \n    95 \n    41 \n    59 \n    93.57605 \n  \n  \n    32 \n    96 \n    109 \n    108 \n    152.34148 \n  \n  \n    41 \n    84 \n    132 \n    138 \n    167.76096 \n  \n  \n    47 \n    43 \n    87 \n    158 \n    111.57876 \n  \n  \n    32 \n    104 \n    110 \n    147 \n    156.65744 \n  \n  \n    34 \n    84 \n    146 \n    133 \n    179.34831 \n  \n  \n    53 \n    110 \n    59 \n    65 \n    116.69196 \n  \n  \n    61 \n    68 \n    103 \n    149 \n    137.27616 \n  \n  \n    46 \n    82 \n    99 \n    156 \n    138.71104 \n  \n  \n    42 \n    72 \n    67 \n    89 \n    106.39228 \n  \n  \n    58 \n    152 \n    104 \n    199 \n    174.14174 \n  \n  \n    79 \n    70 \n    100 \n    144 \n    136.91016 \n  \n  \n    58 \n    56 \n    102 \n    146 \n    131.00698 \n  \n  \n    60 \n    78 \n    122 \n    137 \n    157.96093 \n  \n  \n    72 \n    106 \n    91 \n    134 \n    144.11312 \n  \n  \n    63 \n    85 \n    109 \n    170 \n    149.95415 \n  \n  \n    69 \n    33 \n    74 \n    147 \n    97.68463 \n  \n  \n    32 \n    21 \n    67 \n    142 \n    83.63646 \n  \n  \n    50 \n    87 \n    98 \n    199 \n    140.30589 \n  \n  \n    43 \n    104 \n    141 \n    230 \n    184.32985 \n  \n  \n    30 \n    92 \n    52 \n    87 \n    101.12046 \n  \n  \n    58 \n    84 \n    166 \n    186 \n    198.48657 \n  \n  \n    84 \n    86 \n    97 \n    171 \n    141.59381 \n  \n  \n    58 \n    70 \n    85 \n    114 \n    122.32839 \n  \n  \n    55 \n    53 \n    106 \n    136 \n    132.94777 \n  \n  \n    58 \n    83 \n    156 \n    155 \n    189.39847 \n  \n  \n    83 \n    109 \n    146 \n    172 \n    193.85573 \n  \n  \n    44 \n    103 \n    91 \n    167 \n    140.69053 \n  \n  \n    54 \n    51 \n    102 \n    117 \n    128.54644 \n  \n  \n    47 \n    81 \n    82 \n    123 \n    123.63920 \n  \n  \n    40 \n    103 \n    130 \n    189 \n    174.14799 \n  \n  \n    42 \n    82 \n    27 \n    64 \n    76.07789 \n  \n  \n    30 \n    160 \n    66 \n    155 \n    142.56740 \n  \n  \n    54 \n    139 \n    74 \n    110 \n    142.26049 \n  \n  \n    50 \n    111 \n    44 \n    35 \n    103.90995 \n  \n  \n    69 \n    143 \n    84 \n    178 \n    153.78283 \n  \n  \n    59 \n    109 \n    137 \n    184 \n    184.23997 \n  \n  \n    66 \n    72 \n    138 \n    140 \n    169.68028 \n  \n  \n    40 \n    119 \n    110 \n    178 \n    163.73493 \n  \n  \n    53 \n    52 \n    130 \n    146 \n    153.14078 \n  \n  \n    41 \n    103 \n    177 \n    248 \n    214.91104 \n  \n  \n    37 \n    121 \n    162 \n    222 \n    209.38485 \n  \n  \n    42 \n    60 \n    167 \n    178 \n    187.78498 \n  \n  \n    65 \n    108 \n    144 \n    224 \n    190.32461 \n  \n  \n    50 \n    119 \n    155 \n    204 \n    203.45085 \n  \n  \n    72 \n    76 \n    92 \n    151 \n    132.04024 \n  \n  \n    43 \n    67 \n    117 \n    118 \n    147.59594 \n  \n  \n    45 \n    130 \n    157 \n    276 \n    209.54622 \n  \n  \n    53 \n    73 \n    119 \n    132 \n    152.67528 \n  \n  \n    80 \n    97 \n    101 \n    73 \n    149.49657 \n  \n  \n    43 \n    89 \n    74 \n    113 \n    119.85993 \n  \n  \n    29 \n    79 \n    125 \n    135 \n    158.63245 \n  \n  \n    44 \n    88 \n    160 \n    170 \n    193.95325 \n  \n  \n    70 \n    139 \n    90 \n    141 \n    157.32780 \n  \n  \n    45 \n    54 \n    39 \n    127 \n    74.61815 \n  \n  \n    49 \n    96 \n    93 \n    143 \n    139.78303 \n  \n  \n    55 \n    114 \n    94 \n    140 \n    148.86800 \n  \n  \n    39 \n    95 \n    130 \n    232 \n    170.62168 \n  \n  \n    36 \n    101 \n    135 \n    183 \n    177.30972 \n  \n  \n    59 \n    58 \n    113 \n    154 \n    141.46807 \n  \n  \n    44 \n    146 \n    98 \n    137 \n    165.29557 \n  \n  \n    41 \n    45 \n    56 \n    102 \n    85.14906 \n  \n  \n    39 \n    123 \n    145 \n    180 \n    195.68289 \n  \n  \n    54 \n    74 \n    51 \n    103 \n    94.31627 \n  \n  \n    73 \n    119 \n    119 \n    224 \n    174.03493 \n  \n  \n    47 \n    88 \n    43 \n    114 \n    92.89663 \n  \n  \n    52 \n    85 \n    80 \n    113 \n    124.01311 \n  \n  \n    61 \n    47 \n    158 \n    137 \n    175.83164 \n  \n  \n    56 \n    70 \n    61 \n    127 \n    101.39999 \n  \n  \n    70 \n    79 \n    108 \n    106 \n    147.03294 \n  \n  \n    28 \n    108 \n    92 \n    170 \n    142.49622 \n  \n  \n    47 \n    54 \n    71 \n    90 \n    102.47200 \n  \n  \n    66 \n    97 \n    115 \n    145 \n    160.55174 \n  \n  \n    29 \n    105 \n    106 \n    153 \n    153.39793 \n  \n  \n    70 \n    130 \n    120 \n    186 \n    179.41667 \n  \n  \n    71 \n    93 \n    118 \n    111 \n    161.80378 \n  \n  \n    42 \n    111 \n    142 \n    202 \n    188.13850 \n  \n  \n    38 \n    130 \n    113 \n    158 \n    170.92407 \n  \n  \n    50 \n    116 \n    131 \n    183 \n    181.38065 \n  \n  \n    49 \n    106 \n    138 \n    202 \n    183.05153 \n  \n  \n    58 \n    100 \n    55 \n    117 \n    109.29652 \n  \n  \n    37 \n    105 \n    167 \n    206 \n    206.81269 \n  \n  \n    54 \n    16 \n    95 \n    138 \n    107.39169 \n  \n  \n    64 \n    123 \n    90 \n    198 \n    149.97108 \n  \n  \n    44 \n    109 \n    59 \n    68 \n    115.57644 \n  \n  \n    30 \n    85 \n    98 \n    155 \n    137.92279 \n  \n  \n    67 \n    79 \n    68 \n    134 \n    112.17762 \n  \n  \n    52 \n    124 \n    104 \n    193 \n    161.60959 \n  \n  \n    50 \n    114 \n    65 \n    102 \n    123.38311 \n  \n  \n    58 \n    46 \n    74 \n    121 \n    102.45505 \n  \n  \n    38 \n    71 \n    85 \n    192 \n    121.23915 \n  \n  \n    28 \n    137 \n    84 \n    215 \n    148.07805 \n  \n  \n    59 \n    74 \n    62 \n    162 \n    104.21889 \n  \n  \n    58 \n    113 \n    40 \n    89 \n    101.91801 \n  \n  \n    79 \n    94 \n    51 \n    166 \n    104.84263 \n  \n  \n    57 \n    125 \n    101 \n    156 \n    159.82396 \n  \n  \n    99 \n    76 \n    105 \n    110 \n    145.34680 \n  \n  \n    59 \n    110 \n    137 \n    218 \n    184.67126 \n  \n  \n    34 \n    38 \n    135 \n    118 \n    149.98669 \n  \n  \n    54 \n    76 \n    177 \n    205 \n    204.25467 \n  \n  \n    61 \n    132 \n    60 \n    140 \n    127.65413 \n  \n  \n    48 \n    60 \n    106 \n    152 \n    135.43458 \n  \n  \n    63 \n    146 \n    123 \n    186 \n    188.38210 \n  \n  \n    53 \n    125 \n    45 \n    96 \n    111.04171 \n  \n  \n    41 \n    147 \n    96 \n    98 \n    163.76741 \n  \n  \n    23 \n    78 \n    66 \n    124 \n    106.66981 \n  \n  \n    40 \n    107 \n    144 \n    155 \n    187.99267 \n  \n  \n    30 \n    52 \n    152 \n    161 \n    170.43716 \n  \n  \n    53 \n    124 \n    69 \n    110 \n    131.38677 \n  \n  \n    58 \n    137 \n    137 \n    211 \n    196.23994 \n  \n  \n    69 \n    68 \n    127 \n    184 \n    158.66072 \n  \n  \n    32 \n    65 \n    84 \n    147 \n    117.32960 \n  \n  \n    74 \n    104 \n    60 \n    104 \n    116.56648 \n  \n  \n    58 \n    69 \n    55 \n    131 \n    95.92667 \n  \n  \n    75 \n    124 \n    152 \n    208 \n    204.91089 \n  \n  \n    14 \n    59 \n    159 \n    174 \n    178.29951 \n  \n  \n    49 \n    108 \n    123 \n    178 \n    170.92889 \n  \n  \n    52 \n    55 \n    145 \n    149 \n    167.34383 \n  \n  \n    52 \n    71 \n    79 \n    106 \n    117.10943 \n  \n  \n    53 \n    124 \n    110 \n    150 \n    166.87970 \n  \n  \n    61 \n    100 \n    131 \n    170 \n    175.31637 \n  \n  \n    38 \n    154 \n    79 \n    115 \n    151.84175 \n  \n  \n    62 \n    84 \n    91 \n    116 \n    133.86458 \n  \n  \n    45 \n    109 \n    96 \n    151 \n    147.68267 \n  \n  \n    21 \n    123 \n    132 \n    192 \n    183.06057 \n  \n  \n    37 \n    90 \n    97 \n    110 \n    139.74572 \n  \n  \n    75 \n    121 \n    138 \n    171 \n    191.49750 \n  \n  \n    52 \n    62 \n    130 \n    130 \n    157.37761 \n  \n  \n    31 \n    163 \n    138 \n    235 \n    206.26633 \n  \n  \n    55 \n    97 \n    116 \n    101 \n    160.58114 \n  \n  \n    47 \n    85 \n    155 \n    201 \n    188.55907 \n  \n  \n    57 \n    110 \n    103 \n    149 \n    155.08604 \n  \n  \n    61 \n    100 \n    111 \n    187 \n    158.00274 \n  \n  \n    50 \n    94 \n    128 \n    120 \n    169.29532 \n  \n  \n    40 \n    104 \n    79 \n    110 \n    130.42954 \n  \n  \n    71 \n    57 \n    85 \n    111 \n    117.71002 \n  \n  \n    50 \n    119 \n    48 \n    82 \n    110.82296 \n  \n  \n    54 \n    78 \n    74 \n    99 \n    115.95208 \n  \n  \n    36 \n    112 \n    77 \n    118 \n    131.84435 \n  \n  \n    32 \n    117 \n    108 \n    191 \n    160.53279 \n  \n  \n    61 \n    75 \n    123 \n    145 \n    157.60878 \n  \n  \n    38 \n    55 \n    112 \n    136 \n    137.71198 \n  \n  \n    58 \n    90 \n    173 \n    243 \n    207.13405 \n  \n  \n    31 \n    93 \n    63 \n    152 \n    111.15026 \n  \n  \n    60 \n    117 \n    75 \n    120 \n    134.09404 \n  \n  \n    34 \n    95 \n    155 \n    190 \n    191.88358 \n  \n  \n    44 \n    112 \n    90 \n    206 \n    143.70642 \n  \n  \n    63 \n    102 \n    138 \n    198 \n    182.39076 \n  \n  \n    59 \n    46 \n    98 \n    139 \n    123.30742 \n  \n  \n    35 \n    96 \n    89 \n    113 \n    135.25593 \n  \n  \n    60 \n    96 \n    115 \n    114 \n    159.66430 \n  \n  \n    53 \n    89 \n    94 \n    108 \n    137.93381 \n  \n  \n    57 \n    144 \n    79 \n    163 \n    148.97340 \n  \n  \n    39 \n    46 \n    103 \n    145 \n    126.11531 \n  \n  \n    43 \n    75 \n    54 \n    109 \n    96.50831 \n  \n  \n    46 \n    109 \n    67 \n    121 \n    122.65395 \n  \n  \n    33 \n    118 \n    108 \n    114 \n    161.04010 \n  \n  \n    35 \n    62 \n    161 \n    173 \n    182.92128 \n  \n  \n    56 \n    57 \n    84 \n    186 \n    115.70395 \n  \n  \n    65 \n    87 \n    78 \n    134 \n    124.13266 \n  \n  \n    49 \n    36 \n    112 \n    143 \n    130.35385 \n  \n  \n    38 \n    84 \n    63 \n    141 \n    107.80088 \n  \n  \n    46 \n    58 \n    77 \n    99 \n    109.31520 \n  \n  \n    75 \n    69 \n    116 \n    141 \n    150.02567 \n  \n  \n    43 \n    138 \n    102 \n    167 \n    165.23199 \n  \n  \n    90 \n    97 \n    52 \n    122 \n    107.83846 \n  \n  \n    60 \n    112 \n    164 \n    243 \n    208.98325 \n  \n  \n    32 \n    124 \n    140 \n    213 \n    191.25359 \n  \n  \n    71 \n    83 \n    89 \n    162 \n    132.38617 \n  \n  \n    57 \n    121 \n    72 \n    124 \n    132.99406 \n  \n  \n    58 \n    139 \n    53 \n    86 \n    124.38528 \n  \n  \n    66 \n    118 \n    139 \n    174 \n    190.38509 \n  \n  \n    53 \n    81 \n    133 \n    151 \n    168.24510 \n  \n  \n    64 \n    42 \n    124 \n    111 \n    144.47013 \n  \n  \n    59 \n    110 \n    139 \n    179 \n    186.40262 \n  \n  \n    62 \n    132 \n    136 \n    242 \n    193.52193 \n  \n  \n    49 \n    111 \n    154 \n    218 \n    199.05886 \n  \n  \n    70 \n    60 \n    166 \n    165 \n    189.04803 \n  \n  \n    56 \n    124 \n    108 \n    148 \n    165.37642 \n  \n  \n    45 \n    134 \n    176 \n    287 \n    227.71931 \n  \n  \n    69 \n    87 \n    144 \n    182 \n    181.57172 \n  \n  \n    56 \n    105 \n    151 \n    164 \n    194.40629 \n  \n  \n    68 \n    42 \n    89 \n    128 \n    114.47539 \n  \n  \n    68 \n    97 \n    76 \n    111 \n    126.94223 \n  \n  \n    44 \n    123 \n    119 \n    212 \n    173.55531 \n  \n  \n    33 \n    87 \n    138 \n    208 \n    173.64069 \n  \n  \n    66 \n    81 \n    37 \n    119 \n    86.12804 \n  \n  \n    44 \n    149 \n    89 \n    154 \n    158.79830 \n  \n  \n    68 \n    116 \n    104 \n    131 \n    159.37572 \n  \n  \n    56 \n    77 \n    73 \n    89 \n    114.80716 \n  \n  \n    8 \n    101 \n    79 \n    138 \n    126.70284 \n  \n  \n    74 \n    100 \n    81 \n    126 \n    133.02065 \n  \n  \n    47 \n    107 \n    78 \n    129 \n    131.38989 \n  \n  \n    29 \n    66 \n    29 \n    83 \n    69.92034 \n  \n  \n    68 \n    96 \n    84 \n    108 \n    133.43639 \n  \n  \n    46 \n    100 \n    63 \n    100 \n    115.30965 \n  \n  \n    49 \n    103 \n    130 \n    150 \n    174.83223 \n  \n  \n    43 \n    77 \n    77 \n    106 \n    117.28155 \n  \n  \n    68 \n    149 \n    152 \n    190 \n    215.16084 \n  \n  \n    64 \n    6 \n    108 \n    67 \n    115.09295 \n  \n  \n    49 \n    57 \n    88 \n    140 \n    118.63449 \n  \n  \n    72 \n    125 \n    88 \n    155 \n    149.71050 \n  \n  \n    56 \n    48 \n    69 \n    119 \n    98.83716 \n  \n  \n    39 \n    59 \n    110 \n    173 \n    137.78178 \n  \n  \n    48 \n    94 \n    114 \n    189 \n    157.02373 \n  \n  \n    70 \n    92 \n    141 \n    203 \n    181.20713 \n  \n  \n    46 \n    90 \n    105 \n    170 \n    147.35541 \n  \n  \n    57 \n    65 \n    103 \n    151 \n    135.67820 \n  \n  \n    62 \n    92 \n    120 \n    169 \n    162.41962 \n  \n  \n    62 \n    64 \n    88 \n    129 \n    122.64183 \n  \n  \n    55 \n    64 \n    139 \n    94 \n    166.25939 \n  \n  \n    45 \n    132 \n    105 \n    205 \n    165.39337 \n  \n  \n    37 \n    125 \n    118 \n    165 \n    173.02002 \n  \n  \n    46 \n    39 \n    125 \n    132 \n    142.67348 \n  \n  \n    31 \n    75 \n    80 \n    88 \n    118.10370 \n  \n  \n    61 \n    59 \n    89 \n    142 \n    121.27506 \n  \n  \n    43 \n    18 \n    152 \n    98 \n    156.76180 \n  \n  \n    67 \n    110 \n    41 \n    98 \n    102.17407 \n  \n  \n    54 \n    83 \n    67 \n    69 \n    112.04873 \n  \n  \n    17 \n    126 \n    106 \n    112 \n    161.54260 \n  \n  \n    64 \n    46 \n    124 \n    162 \n    146.19527 \n  \n  \n    49 \n    92 \n    109 \n    154 \n    151.90878 \n  \n  \n    47 \n    55 \n    66 \n    153 \n    98.57488 \n  \n  \n    61 \n    75 \n    113 \n    159 \n    148.95197 \n  \n  \n    27 \n    114 \n    88 \n    116 \n    141.54518 \n  \n  \n    52 \n    79 \n    107 \n    118 \n    144.79879 \n  \n  \n    22 \n    96 \n    91 \n    138 \n    135.99896 \n  \n  \n    66 \n    137 \n    164 \n    257 \n    220.22154 \n  \n  \n    31 \n    122 \n    113 \n    170 \n    166.94160 \n  \n  \n    42 \n    53 \n    151 \n    135 \n    170.91508 \n  \n  \n    57 \n    86 \n    91 \n    128 \n    134.34702 \n  \n  \n    60 \n    41 \n    120 \n    128 \n    140.27201 \n  \n  \n    50 \n    61 \n    121 \n    192 \n    149.00314 \n  \n  \n    68 \n    137 \n    119 \n    175 \n    181.41794 \n  \n  \n    58 \n    70 \n    159 \n    159 \n    186.38880 \n  \n  \n    39 \n    64 \n    78 \n    134 \n    112.23641 \n  \n  \n    58 \n    84 \n    95 \n    142 \n    137.02320 \n  \n  \n    72 \n    124 \n    89 \n    136 \n    150.14489 \n  \n  \n    40 \n    104 \n    106 \n    225 \n    153.80293 \n  \n  \n    43 \n    43 \n    113 \n    148 \n    133.78237 \n  \n  \n    66 \n    93 \n    136 \n    178 \n    177.00591 \n  \n  \n    53 \n    59 \n    62 \n    87 \n    97.29345 \n  \n  \n    49 \n    124 \n    123 \n    161 \n    177.82945 \n  \n  \n    70 \n    65 \n    71 \n    101 \n    108.96474 \n  \n  \n    50 \n    83 \n    104 \n    141 \n    143.77484 \n  \n  \n    59 \n    47 \n    82 \n    141 \n    109.88781 \n  \n  \n    54 \n    99 \n    89 \n    125 \n    137.99429 \n  \n  \n    52 \n    80 \n    165 \n    136 \n    195.43959 \n  \n  \n    42 \n    86 \n    91 \n    148 \n    133.20663 \n  \n  \n    58 \n    115 \n    112 \n    147 \n    165.10963 \n  \n  \n    44 \n    78 \n    55 \n    119 \n    98.74387 \n  \n  \n    80 \n    89 \n    51 \n    85 \n    102.76223 \n  \n  \n    62 \n    103 \n    139 \n    220 \n    183.61170 \n  \n  \n    50 \n    139 \n    198 \n    286 \n    249.30085 \n  \n  \n    50 \n    106 \n    133 \n    192 \n    178.79915 \n  \n  \n    50 \n    141 \n    79 \n    127 \n    147.14736 \n  \n  \n    52 \n    75 \n    37 \n    56 \n    82.47596 \n  \n  \n    63 \n    80 \n    171 \n    176 \n    201.46996 \n  \n  \n    66 \n    45 \n    130 \n    139 \n    151.11012 \n  \n  \n    50 \n    104 \n    83 \n    141 \n    134.65252 \n  \n  \n    58 \n    94 \n    39 \n    50 \n    92.85790 \n  \n  \n    53 \n    50 \n    96 \n    169 \n    122.84505 \n  \n  \n    45 \n    94 \n    94 \n    115 \n    139.48203 \n  \n  \n    35 \n    137 \n    120 \n    183 \n    179.77475 \n  \n  \n    43 \n    100 \n    128 \n    174 \n    171.35085 \n  \n  \n    38 \n    101 \n    131 \n    238 \n    173.99905 \n  \n  \n    44 \n    158 \n    147 \n    193 \n    212.88938 \n  \n  \n    56 \n    90 \n    79 \n    81 \n    125.60796 \n  \n  \n    37 \n    46 \n    163 \n    130 \n    177.90413 \n  \n  \n    47 \n    97 \n    122 \n    167 \n    165.16701 \n  \n  \n    70 \n    102 \n    119 \n    150 \n    166.47500 \n  \n  \n    50 \n    92 \n    66 \n    59 \n    114.76052 \n  \n  \n    45 \n    116 \n    79 \n    130 \n    135.98509 \n  \n  \n    53 \n    21 \n    93 \n    130 \n    107.74073 \n  \n  \n    41 \n    75 \n    137 \n    203 \n    168.20780 \n  \n  \n    49 \n    110 \n    164 \n    182 \n    207.28439 \n  \n  \n    64 \n    60 \n    97 \n    111 \n    128.85987 \n  \n  \n    67 \n    129 \n    114 \n    183 \n    173.56322 \n  \n  \n    72 \n    46 \n    99 \n    178 \n    125.16145 \n  \n  \n    67 \n    96 \n    37 \n    86 \n    92.67335 \n  \n  \n    47 \n    78 \n    128 \n    158 \n    162.16668 \n  \n  \n    47 \n    177 \n    130 \n    244 \n    206.59530 \n  \n  \n    54 \n    89 \n    151 \n    172 \n    187.35367 \n  \n  \n    56 \n    64 \n    137 \n    125 \n    164.60405 \n  \n  \n    34 \n    123 \n    104 \n    134 \n    159.80983 \n  \n  \n    57 \n    81 \n    87 \n    98 \n    128.72787 \n  \n  \n    59 \n    134 \n    107 \n    169 \n    169.05167 \n  \n  \n    75 \n    33 \n    110 \n    118 \n    129.30531 \n  \n  \n    28 \n    114 \n    56 \n    83 \n    113.91941 \n  \n  \n    31 \n    31 \n    100 \n    122 \n    116.44077 \n  \n  \n    55 \n    108 \n    95 \n    181 \n    147.14597 \n  \n  \n    41 \n    31 \n    99 \n    124 \n    116.33535 \n  \n  \n    48 \n    106 \n    156 \n    215 \n    198.55777 \n  \n  \n    40 \n    36 \n    119 \n    109 \n    135.72938 \n  \n  \n    21 \n    118 \n    110 \n    175 \n    161.85915 \n  \n  \n    38 \n    147 \n    102 \n    136 \n    168.73342 \n  \n  \n    34 \n    122 \n    111 \n    183 \n    165.43831 \n  \n  \n    62 \n    94 \n    62 \n    150 \n    113.07268 \n  \n  \n    71 \n    81 \n    137 \n    213 \n    173.07630 \n  \n  \n    78 \n    89 \n    45 \n    28 \n    97.41609 \n  \n  \n    36 \n    93 \n    123 \n    207 \n    163.47127 \n  \n  \n    31 \n    123 \n    108 \n    176 \n    163.04448 \n  \n  \n    84 \n    136 \n    115 \n    199 \n    178.74035 \n  \n  \n    34 \n    119 \n    108 \n    166 \n    161.54741 \n  \n  \n    41 \n    80 \n    68 \n    142 \n    110.63222 \n  \n  \n    45 \n    86 \n    151 \n    125 \n    185.37558 \n  \n  \n    46 \n    94 \n    112 \n    171 \n    155.14032 \n  \n  \n    55 \n    58 \n    44 \n    92 \n    81.43196 \n  \n  \n    64 \n    115 \n    61 \n    97 \n    121.41604 \n  \n  \n    48 \n    98 \n    125 \n    154 \n    168.27137 \n  \n  \n    54 \n    80 \n    74 \n    136 \n    116.81465 \n  \n  \n    66 \n    81 \n    125 \n    99 \n    162.30799 \n  \n  \n    56 \n    44 \n    105 \n    110 \n    128.27654 \n  \n  \n    46 \n    110 \n    135 \n    223 \n    181.95155 \n  \n  \n    49 \n    56 \n    74 \n    72 \n    106.08367 \n  \n  \n    66 \n    188 \n    119 \n    260 \n    203.26144 \n  \n  \n    41 \n    89 \n    79 \n    123 \n    124.03628 \n  \n  \n    50 \n    75 \n    159 \n    154 \n    187.93702 \n  \n  \n    64 \n    139 \n    92 \n    165 \n    158.60301 \n  \n  \n    46 \n    55 \n    47 \n    150 \n    82.05091 \n  \n  \n    49 \n    129 \n    109 \n    166 \n    167.86634 \n  \n  \n    78 \n    68 \n    166 \n    176 \n    193.10652 \n  \n  \n    44 \n    61 \n    116 \n    155 \n    144.21857 \n  \n  \n    80 \n    97 \n    121 \n    148 \n    166.81020 \n  \n  \n    37 \n    79 \n    120 \n    178 \n    154.91225 \n  \n  \n    58 \n    135 \n    125 \n    172 \n    184.98919 \n  \n\n\n\n\n\n\n\n\nSuppose we have a student with a 55 on Yr1, a 95 on Yr2 and a 110 on Yr3.\nFirst, what would their data look like?\n\nHypothetical.Student <- data.frame(Yr1=55, Yr2=95, Yr3=110)\nHypothetical.Student\n\n  Yr1 Yr2 Yr3\n1  55  95 110\n\n\nLet’s predict, first the single best guess – the fitted value.\nThe equation we sought estimates for was:\n\nlibrary(equatiomatic)\nequatiomatic::extract_eq(my.lm)\n\n\\[\n\\operatorname{Final} = \\alpha + \\beta_{1}(\\operatorname{Yr1}) + \\beta_{2}(\\operatorname{Yr2}) + \\beta_{3}(\\operatorname{Yr3}) + \\epsilon\n\\]\n\n\nAnd it was estimated to be\n\nequatiomatic::extract_eq(my.lm, use_coefs = TRUE, coef_digits = 4)\n\n\\[\n\\operatorname{\\widehat{Final}} = 14.146 + 0.076(\\operatorname{Yr1}) + 0.4313(\\operatorname{Yr2}) + 0.8657(\\operatorname{Yr3})\n\\]\n\n\nOur best guess for Hypothetical.Student must then be:\n\n14.145 + 0.076*55 + 0.4313*95 + 0.8657*110\n\n[1] 154.5255\n\n\nWhat does predict produce?\n\npredict(my.lm, newdata = Hypothetical.Student)\n\n       1 \n154.5245 \n\n\nThe same thing except that mine by hand was restricted to four digits.\nIn exact form, we could produce, using matrix multiplication,\n\nc(summary(my.lm)$coefficients[,1])\n\n(Intercept)         Yr1         Yr2         Yr3 \n14.14598945  0.07602621  0.43128539  0.86568123 \n\nc(1,Hypothetical.Student)\n\n[[1]]\n[1] 1\n\n$Yr1\n[1] 55\n\n$Yr2\n[1] 95\n\n$Yr3\n[1] 110\n\nc(1,55,95,110)%*%c(summary(my.lm)$coefficients[,1])\n\n         [,1]\n[1,] 154.5245\n\n\nBecause that is all predict does.\n\n\nTo produce confidence intervals, we can add the interval option. For an interval of the predicted average, we have:\n\npredict(my.lm, newdata = Hypothetical.Student, interval=\"confidence\")\n\n       fit     lwr     upr\n1 154.5245 152.551 156.498\n\n\n\n\n\nAn interval of the predicted range of data, we have\n\npredict(my.lm, newdata = Hypothetical.Student, interval=\"prediction\")\n\n       fit      lwr      upr\n1 154.5245 94.76778 214.2812\n\n\n\n\n\n\nOne great thing about R is smart prediction. So what does it do? Let’s try out the centering operations that I used.\n\nmy.lm <- lm(Final ~ scale(Yr1, scale=FALSE) + scale(Yr2, scale=FALSE) + scale(Yr3, scale=FALSE), data=ugtests)\nsummary(my.lm)\n\n\nCall:\nlm(formula = Final ~ scale(Yr1, scale = FALSE) + scale(Yr2, scale = FALSE) + \n    scale(Yr3, scale = FALSE), data = ugtests)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-92.638 -20.349   0.001  18.954  98.489 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(>|t|)    \n(Intercept)               148.96205    0.97467 152.833   <2e-16 ***\nscale(Yr1, scale = FALSE)   0.07603    0.06538   1.163    0.245    \nscale(Yr2, scale = FALSE)   0.43129    0.03251  13.267   <2e-16 ***\nscale(Yr3, scale = FALSE)   0.86568    0.02914  29.710   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 30.43 on 971 degrees of freedom\nMultiple R-squared:  0.5303,    Adjusted R-squared:  0.5289 \nF-statistic: 365.5 on 3 and 971 DF,  p-value: < 2.2e-16\n\n\n\nequatiomatic::extract_eq(my.lm, use_coefs = TRUE, coef_digits = 4)\n\n\\[\n\\operatorname{\\widehat{Final}} = 148.9621 + 0.076(\\operatorname{scale(Yr1,\\ scale\\ =\\ FALSE)}) + 0.4313(\\operatorname{scale(Yr2,\\ scale\\ =\\ FALSE)}) + 0.8657(\\operatorname{scale(Yr3,\\ scale\\ =\\ FALSE)})\n\\]\n\n\nOnly the intercept is impacted; the original intercept was the expected Final for a student that had all zeroes [possible but a very poor performance] and they’d have expected a 14.15 [the original intercept]. After the centering operation for each predictor, the average student [mean scores on Yr1, Yr2, and Yr3] could expect a score of 148.96205 – the intercept from the regression on centered data.\nNow let’s predict our Hypothetical.Student.\n\npredict(my.lm, newdata=Hypothetical.Student)\n\n       1 \n154.5245 \n\n\nThe result is the same.\n\npredict(my.lm, newdata=Hypothetical.Student, interval=\"confidence\")\n\n       fit     lwr     upr\n1 154.5245 152.551 156.498\n\n\n\npredict(my.lm, newdata=Hypothetical.Student, interval=\"prediction\")\n\n       fit      lwr      upr\n1 154.5245 94.76778 214.2812\n\n\nThis works because R knows that each variable was centered, the mean was subtracted because that is what scale does when the [unfortunately named] argument scale inside the function scale is set to FALSE => we are not creating z-scores [the default is \\(\\frac{x_{i} - \\overline{x}}{sd(x)}\\)] but are just taking \\(x_{i} - \\overline{x}\\) or centering the data.\n\n\n\nLet me try a regression where Yr1, Yr2, and Yr3 are allowed to have effects with curvature using each term and its square.\n\nmy.lm.Sq <- lm(Final ~ Yr1 + Yr1^2 + Yr2 + Yr2^2 + Yr3 + Yr3^2, data=ugtests)\nsummary(my.lm.Sq)\n\n\nCall:\nlm(formula = Final ~ Yr1 + Yr1^2 + Yr2 + Yr2^2 + Yr3 + Yr3^2, \n    data = ugtests)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-92.638 -20.349   0.001  18.954  98.489 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 14.14599    5.48006   2.581  0.00999 ** \nYr1          0.07603    0.06538   1.163  0.24519    \nYr2          0.43129    0.03251  13.267  < 2e-16 ***\nYr3          0.86568    0.02914  29.710  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 30.43 on 971 degrees of freedom\nMultiple R-squared:  0.5303,    Adjusted R-squared:  0.5289 \nF-statistic: 365.5 on 3 and 971 DF,  p-value: < 2.2e-16\n\n\nUnfortunately, that did not actually include the squared terms, we still only have three lines. We could use this:\n\nmy.lm.Sq <- lm(Final ~ Yr1 + Yr1*Yr1 + Yr2 + Yr2*Yr2 + Yr3 + Yr3*Yr3, data=ugtests)\nsummary(my.lm.Sq)\n\n\nCall:\nlm(formula = Final ~ Yr1 + Yr1 * Yr1 + Yr2 + Yr2 * Yr2 + Yr3 + \n    Yr3 * Yr3, data = ugtests)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-92.638 -20.349   0.001  18.954  98.489 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 14.14599    5.48006   2.581  0.00999 ** \nYr1          0.07603    0.06538   1.163  0.24519    \nYr2          0.43129    0.03251  13.267  < 2e-16 ***\nYr3          0.86568    0.02914  29.710  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 30.43 on 971 degrees of freedom\nMultiple R-squared:  0.5303,    Adjusted R-squared:  0.5289 \nF-statistic: 365.5 on 3 and 971 DF,  p-value: < 2.2e-16\n\n\nBut that also does not work. The key is to give \\(R\\) an object for the squared term that treats Yr1, Yr2, and Yr3 as base terms to be squared; the function for this is I.\n\nmy.lm.Sq <- lm(Final ~ Yr1 + I(Yr1^2) + Yr2 + I(Yr2^2) + Yr3 + I(Yr3^2), data=ugtests)\nsummary(my.lm.Sq)\n\n\nCall:\nlm(formula = Final ~ Yr1 + I(Yr1^2) + Yr2 + I(Yr2^2) + Yr3 + \n    I(Yr3^2), data = ugtests)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-94.292 -19.764  -0.006  18.961  93.503 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 24.4243935 13.4033492   1.822   0.0687 .  \nYr1          0.2023539  0.3209042   0.631   0.5285    \nI(Yr1^2)    -0.0011955  0.0030483  -0.392   0.6950    \nYr2          0.3434989  0.1446076   2.375   0.0177 *  \nI(Yr2^2)     0.0004756  0.0007687   0.619   0.5362    \nYr3          0.6617121  0.1549276   4.271 2.14e-05 ***\nI(Yr3^2)     0.0009624  0.0007183   1.340   0.1806    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 30.45 on 968 degrees of freedom\nMultiple R-squared:  0.5314,    Adjusted R-squared:  0.5285 \nF-statistic:   183 on 6 and 968 DF,  p-value: < 2.2e-16\n\n\nDoes the curvature improve fit? We can use an F test.\n\nanova(my.lm, my.lm.Sq)\n\nAnalysis of Variance Table\n\nModel 1: Final ~ scale(Yr1, scale = FALSE) + scale(Yr2, scale = FALSE) + \n    scale(Yr3, scale = FALSE)\nModel 2: Final ~ Yr1 + I(Yr1^2) + Yr2 + I(Yr2^2) + Yr3 + I(Yr3^2)\n  Res.Df    RSS Df Sum of Sq      F Pr(>F)\n1    971 899371                           \n2    968 897272  3    2098.6 0.7547 0.5197\n\n\nWe cannot tell the two models apart so the squared terms do not improve the model. That wasn’t really the goal, it was to illustrate Smart Prediction.\nWhat would we expect, given this new model, for our hypothetical student?\n\npredict(my.lm.Sq, newdata=Hypothetical.Student)\n\n       1 \n153.2958 \n\n\nI will forego the intervals but it just works because R knows to square each of Yr1, Yr2, and Yr3.\n\n\n\nSuppose that Yr1 and Yr2 have an interactive effect. We could use the I() construct but we do not have to. The regression would be:\n\nmy.lm.Int <- lm(Final ~ Yr1 + Yr2 + Yr1*Yr2 + Yr3, data=ugtests)\nsummary(my.lm.Int)\n\n\nCall:\nlm(formula = Final ~ Yr1 + Yr2 + Yr1 * Yr2 + Yr3, data = ugtests)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-94.745 -20.774  -0.013  19.112  98.618 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  2.961723  11.930732   0.248    0.804    \nYr1          0.290161   0.213180   1.361    0.174    \nYr2          0.550808   0.117829   4.675 3.36e-06 ***\nYr3          0.866264   0.029141  29.727  < 2e-16 ***\nYr1:Yr2     -0.002295   0.002175  -1.055    0.292    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 30.43 on 970 degrees of freedom\nMultiple R-squared:  0.5309,    Adjusted R-squared:  0.5289 \nF-statistic: 274.4 on 4 and 970 DF,  p-value: < 2.2e-16\n\nmy.lm.I <- lm(Final ~ Yr1 + Yr2 + I(Yr1*Yr2) + Yr3, data=ugtests)\nsummary(my.lm.I)\n\n\nCall:\nlm(formula = Final ~ Yr1 + Yr2 + I(Yr1 * Yr2) + Yr3, data = ugtests)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-94.745 -20.774  -0.013  19.112  98.618 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   2.961723  11.930732   0.248    0.804    \nYr1           0.290161   0.213180   1.361    0.174    \nYr2           0.550808   0.117829   4.675 3.36e-06 ***\nI(Yr1 * Yr2) -0.002295   0.002175  -1.055    0.292    \nYr3           0.866264   0.029141  29.727  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 30.43 on 970 degrees of freedom\nMultiple R-squared:  0.5309,    Adjusted R-squared:  0.5289 \nF-statistic: 274.4 on 4 and 970 DF,  p-value: < 2.2e-16\n\n\nIt is worth noting that this does not improve the fit of the model.\n\nanova(my.lm, my.lm.I)\n\nAnalysis of Variance Table\n\nModel 1: Final ~ scale(Yr1, scale = FALSE) + scale(Yr2, scale = FALSE) + \n    scale(Yr3, scale = FALSE)\nModel 2: Final ~ Yr1 + Yr2 + I(Yr1 * Yr2) + Yr3\n  Res.Df    RSS Df Sum of Sq      F Pr(>F)\n1    971 899371                           \n2    970 898340  1    1031.5 1.1137 0.2915\n\nanova(my.lm, my.lm.Int)\n\nAnalysis of Variance Table\n\nModel 1: Final ~ scale(Yr1, scale = FALSE) + scale(Yr2, scale = FALSE) + \n    scale(Yr3, scale = FALSE)\nModel 2: Final ~ Yr1 + Yr2 + Yr1 * Yr2 + Yr3\n  Res.Df    RSS Df Sum of Sq      F Pr(>F)\n1    971 899371                           \n2    970 898340  1    1031.5 1.1137 0.2915\n\n\nThe same holds for both:\n\npredict(my.lm.I, newdata=Hypothetical.Student)\n\n      1 \n154.544 \n\npredict(my.lm.Int, newdata=Hypothetical.Student)\n\n      1 \n154.544 \n\n\n\n\n\nLet me generate some fake data to showcase use cases for quadratic functions of x as determinants of y.\n\n\nI will generate \\(y\\) according to the following equation.\n\\[y = x + 2*x^2 + \\epsilon\\]\nwhere x is a sequence from -5 to 5 and \\(\\epsilon\\) is Normal(0,1).\n\nlibrary(magrittr)\n\n\nAttaching package: 'magrittr'\n\n\nThe following object is masked from 'package:purrr':\n\n    set_names\n\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n\nfake.df <- data.frame(x=seq(-5,5, by=0.05))\nfake.df$y <- fake.df$x + 2*fake.df$x^2 + rnorm(201)\n\n\n\n\nLet me plot x and y and include the estimated regression line [that does not include the squared term].\n\nfake.df %>% ggplot() + aes(x=x, y=y) + geom_point() + geom_smooth(method=\"lm\")\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nTo be transparent, here is the regression.\n\nsummary(lm(y~x, data=fake.df))\n\n\nCall:\nlm(formula = y ~ x, data = fake.df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.934 -13.695  -4.537  11.878  34.081 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  16.8327     1.0757  15.649  < 2e-16 ***\nx             0.9946     0.3708   2.682  0.00792 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.25 on 199 degrees of freedom\nMultiple R-squared:  0.0349,    Adjusted R-squared:  0.03005 \nF-statistic: 7.195 on 1 and 199 DF,  p-value: 0.007923\n\n\nNote just looking at the table would suggest we conclude that \\(y\\) is a linear function of \\(x\\) and this is, at best, partially true. It is actually a quadratic function of \\(x\\). The fit is not very good, though.\nWhat do the residuals look like?\n\nfake.df %<>% mutate(resid = lm(y~x, fake.df)$residuals)\nfake.df %>% ggplot() + aes(x=x, y=resid) + geom_point() + theme_minimal() + labs(y=\"Residuals from linear regression with only x\")\n\n\n\n\nThis is a characteristic pattern of a quadratic, a because the inflection point is at zero and \\(x\\) can be both positive or negative. Real world data is almost always a bit messier. Nevertheless, I digress. Let’s look at the regression estimates including a squared term.\n\nsummary(lm(y~x+I(x^2), fake.df))\n\n\nCall:\nlm(formula = y ~ x + I(x^2), data = fake.df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.48563 -0.65214 -0.01644  0.72777  2.29603 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.093122   0.110725  -0.841    0.401    \nx            0.994560   0.025444  39.089   <2e-16 ***\nI(x^2)       2.010986   0.009806 205.084   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.047 on 198 degrees of freedom\nMultiple R-squared:  0.9955,    Adjusted R-squared:  0.9954 \nF-statistic: 2.179e+04 on 2 and 198 DF,  p-value: < 2.2e-16\n\n\nNotice both the linear and the squared term are statistically different from zero and that the linear term has a much smaller standard error because it is far more precisely estimated. What do the residuals now look like?\n\nfake.df %<>% mutate(resid.sq = lm(y~x+I(x^2), fake.df)$residuals)\nfake.df %>% ggplot() + aes(x=x, y=resid.sq) + geom_point()\n\n\n\n\nThese are basically random with respect to \\(x\\).\nI should also point out that the residuals are well behaved now; they were not in the previous case.\n\nlibrary(gvlma)\ngvlma(lm(y~x, data=fake.df))\n\n\nCall:\nlm(formula = y ~ x, data = fake.df)\n\nCoefficients:\n(Intercept)            x  \n    16.8327       0.9946  \n\n\nASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS\nUSING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:\nLevel of Significance =  0.05 \n\nCall:\n gvlma(x = lm(y ~ x, data = fake.df)) \n\n                       Value   p-value                   Decision\nGlobal Stat        2.195e+02 0.0000000 Assumptions NOT satisfied!\nSkewness           1.300e+01 0.0003115 Assumptions NOT satisfied!\nKurtosis           6.447e+00 0.0111111 Assumptions NOT satisfied!\nLink Function      2.001e+02 0.0000000 Assumptions NOT satisfied!\nHeteroscedasticity 8.725e-07 0.9992547    Assumptions acceptable.\n\n\nVersus\n\ngvlma(lm(y~x+I(x^2), data=fake.df))\n\n\nCall:\nlm(formula = y ~ x + I(x^2), data = fake.df)\n\nCoefficients:\n(Intercept)            x       I(x^2)  \n   -0.09312      0.99456      2.01099  \n\n\nASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS\nUSING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:\nLevel of Significance =  0.05 \n\nCall:\n gvlma(x = lm(y ~ x + I(x^2), data = fake.df)) \n\n                     Value p-value                Decision\nGlobal Stat        2.63452  0.6207 Assumptions acceptable.\nSkewness           0.49801  0.4804 Assumptions acceptable.\nKurtosis           1.39940  0.2368 Assumptions acceptable.\nLink Function      0.70078  0.4025 Assumptions acceptable.\nHeteroscedasticity 0.03634  0.8488 Assumptions acceptable."
  },
  {
    "objectID": "posts/week-4/index.html",
    "href": "posts/week-4/index.html",
    "title": "Week 4: Ordered and Multinomial Logistic Regression",
    "section": "",
    "text": "The slides are here..\nOur fourth class meeting will focus on Chapter 6 and Chapter 7 of Handbook of Regression Modeling in People Analytics. The video will be on youtube."
  },
  {
    "objectID": "posts/week-4/index.html#the-skinny",
    "href": "posts/week-4/index.html#the-skinny",
    "title": "Week 4: Ordered and Multinomial Logistic Regression",
    "section": "The Skinny",
    "text": "The Skinny\nWith qualitative outcomes that are nominal (Chapter 6) or ordered (Chapter 7); linear regressions and/or binary GLMs are insufficiently flexible and/or rich to confront the problem. In the nominal case, how one chose to turn the categories to values is unknown and, in the ordered case, the idea that unit distance separates the categories is an arbitrary assumption that is unlikely to be true."
  },
  {
    "objectID": "posts/week-4/index.html#multinomial-models",
    "href": "posts/week-4/index.html#multinomial-models",
    "title": "Week 4: Ordered and Multinomial Logistic Regression",
    "section": "Multinomial Models",
    "text": "Multinomial Models"
  },
  {
    "objectID": "posts/week-4/index.html#a-bit-on-random-utility",
    "href": "posts/week-4/index.html#a-bit-on-random-utility",
    "title": "Week 4: Ordered and Multinomial Logistic Regression",
    "section": "A Bit on Random Utility",
    "text": "A Bit on Random Utility\nThere’s a great blog post that I found that details this. In a classic paper for which [among many others], Daniel L. McFadden was awarded the Nobel Prize in Economics, he develops a multinomial/conditional logistic regression model for the study of nominal outcomes. The core statistics demonstration is that, if random utility for options is described by a Gumbel/Type I extreme value distribution, then the difference in utility has a logistic distribution. From this observation, one can develop random utility models for unordered choices that follows from utility maximization. In short, we can use microeconomic foundations to motivate the models that follow."
  },
  {
    "objectID": "posts/week-4/index.html#a-bit-on-model-specification",
    "href": "posts/week-4/index.html#a-bit-on-model-specification",
    "title": "Week 4: Ordered and Multinomial Logistic Regression",
    "section": "A Bit On Model Specification",
    "text": "A Bit On Model Specification\nThere are two ways to think about such models. They can be motivated by choice-specific covariates or by chooser specific covariates [or a combination of both]. In general, if the covariates are chooser-specific, we call it a multinomial logit model while, if the covariates are choice specific, we call it conditional logit or conditional logistic regression. McFadden’s paper is built around transportation choices."
  },
  {
    "objectID": "posts/week-4/index.html#one-key-assumption-to-mcfaddens-approach",
    "href": "posts/week-4/index.html#one-key-assumption-to-mcfaddens-approach",
    "title": "Week 4: Ordered and Multinomial Logistic Regression",
    "section": "One Key Assumption to McFadden’s Approach",
    "text": "One Key Assumption to McFadden’s Approach\nThe independence of irrelevant alternatives contends that, when people choose among a set of alternatives, the odds of choosing option A over option B should not depend on the presence or absence of unchosen alternative C. Paul Allison, a famous emeritus sociologist at the University of Pennsylvania, has a nice blog post on this that is well worth the time.\nImagine the following scenario; you are out to dinner and you are given menus. One of your companions is excited to choose a steak from the menu. The server arrives and announces the specials of the day; your companion then decides that a pork chop option from the menu is preferable. Perhaps they were originally indifferent between the steak and the pork chop. Nevertheless, it would appear as though the presentation of irrelevant alternatives – the specials – had a nontrivial effect on your companion’s choices."
  },
  {
    "objectID": "posts/week-4/index.html#multinomial-unordered-outcomes",
    "href": "posts/week-4/index.html#multinomial-unordered-outcomes",
    "title": "Week 4: Ordered and Multinomial Logistic Regression",
    "section": "Multinomial [Unordered] Outcomes",
    "text": "Multinomial [Unordered] Outcomes\nAn example: exchange rate regimes and the vanishing middle.\nThere is a sizable literature on how countries structure markets for currency exchange. There are two polar approaches:\n\nFixed: the price is fixed and central banks offer the needed quantities to maintain a given price.\nFlexible: the quantities are fixed and the price adjusts.\n\nbut there is also a third: intermediate regimes; things like floating pegs, the ERM snake, and others that are mixtures of the two. The literature in international monetary economics highlights these as prone to instability."
  },
  {
    "objectID": "posts/week-4/index.html#the-data",
    "href": "posts/week-4/index.html#the-data",
    "title": "Week 4: Ordered and Multinomial Logistic Regression",
    "section": "The data",
    "text": "The data\n\n\n\nThe Data"
  },
  {
    "objectID": "posts/week-4/index.html#loading-the-data",
    "href": "posts/week-4/index.html#loading-the-data",
    "title": "Week 4: Ordered and Multinomial Logistic Regression",
    "section": "Loading the Data",
    "text": "Loading the Data\n\n```{r, message=FALSE, warning=FALSE}\nlibrary(foreign); library(tidyverse)\nEXRT.data <- read.dta(\"https://github.com/robertwwalker/xaringan/raw/main/CMF-Week-4/img/rr_dal_try1.dta\")\ntable(EXRT.data$regime2a)\nEXRT.data$regime2aF <- as.factor(EXRT.data$regime2a)\nEXRT.data$regime2aF <- relevel(EXRT.data$regime2aF, ref = \"1\")\n```\n\n\n   0    1    2 \n1016  240  206 \n\n\n\n0 is fixed.\n1 is intermediate\n2 is flexible/floating"
  },
  {
    "objectID": "posts/week-4/index.html#a-model",
    "href": "posts/week-4/index.html#a-model",
    "title": "Week 4: Ordered and Multinomial Logistic Regression",
    "section": "A Model",
    "text": "A Model\n\n```{r, results=\"hide\", message=FALSE, warning=FALSE}\nlibrary(nnet); library(stargazer)\nmulti_model <- multinom(\n  formula = regime2aF ~ probirch + dumirch +  fix_l +float_l, maxit=500, \n  data = EXRT.data\n)\n```"
  },
  {
    "objectID": "posts/week-4/index.html#result",
    "href": "posts/week-4/index.html#result",
    "title": "Week 4: Ordered and Multinomial Logistic Regression",
    "section": "Result",
    "text": "Result\n```{r, results=\"asis\", message=FALSE, warning=FALSE}\nstargazer(multi_model, type = \"html\")\n```\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\n0\n\n\n2\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nprobirch\n\n\n18.003***\n\n\n15.319**\n\n\n\n\n\n\n(5.632)\n\n\n(5.982)\n\n\n\n\n\n\n\n\n\n\n\n\ndumirch\n\n\n-23.831***\n\n\n-9.226\n\n\n\n\n\n\n(7.078)\n\n\n(7.483)\n\n\n\n\n\n\n\n\n\n\n\n\nfix_l\n\n\n6.076***\n\n\n3.347***\n\n\n\n\n\n\n(0.368)\n\n\n(0.431)\n\n\n\n\n\n\n\n\n\n\n\n\nfloat_l\n\n\n2.763***\n\n\n4.992***\n\n\n\n\n\n\n(0.443)\n\n\n(0.424)\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n-2.945***\n\n\n-3.277***\n\n\n\n\n\n\n(0.343)\n\n\n(0.385)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAkaike Inf. Crit.\n\n\n967.448\n\n\n967.448\n\n\n\n\n\n\n\n\nNote:\n\n\np<0.1; p<0.05; p<0.01"
  },
  {
    "objectID": "posts/week-4/index.html#a-bit-of-interpretation",
    "href": "posts/week-4/index.html#a-bit-of-interpretation",
    "title": "Week 4: Ordered and Multinomial Logistic Regression",
    "section": "A Bit of Interpretation",
    "text": "A Bit of Interpretation\n\n```{r, message=FALSE, warning=FALSE}\nEXRT.data <- EXRT.data %>% select(regime2aF,probirch, dumirch,fix_l,float_l)\nsummary(EXRT.data)\n```\n\n regime2aF    probirch          dumirch            fix_l       \n 1: 240    Min.   :0.00000   Min.   :0.00000   Min.   :0.0000  \n 0:1016    1st Qu.:0.01000   1st Qu.:0.00000   1st Qu.:0.0000  \n 2: 206    Median :0.04000   Median :0.00000   Median :1.0000  \n           Mean   :0.04034   Mean   :0.00754   Mean   :0.7175  \n           3rd Qu.:0.05000   3rd Qu.:0.01000   3rd Qu.:1.0000  \n           Max.   :0.34000   Max.   :0.16000   Max.   :1.0000  \n           NA's   :150       NA's   :150                       \n    float_l      \n Min.   :0.0000  \n 1st Qu.:0.0000  \n Median :0.0000  \n Mean   :0.1327  \n 3rd Qu.:0.0000  \n Max.   :1.0000"
  },
  {
    "objectID": "posts/week-4/index.html#a-plot",
    "href": "posts/week-4/index.html#a-plot",
    "title": "Week 4: Ordered and Multinomial Logistic Regression",
    "section": "A Plot",
    "text": "A Plot\n\n```{r}\npred.data.I <- data.frame(fix_l = 0, float_l = 0, probirch = seq(0,0.34, by=0.01), dumirch = 0)\nPreds.1 <- data.frame(predict(multi_model, newdata=pred.data.I, type=\"p\"))\nPreds.1$Irregular <- pred.data.I$probirch\nPreds.1.df <- data.frame(Preds.1)\nGraph.1 <- Preds.1.df %>% pivot_longer(., cols=c(X1,X2,X0)) %>%\nggplot(.) + aes(x=Irregular, y=value, color=name) + geom_point() + theme_minimal()\npred.data.I <- data.frame(fix_l = 0, float_l = 0, probirch = seq(0,0.34, by=0.01), dumirch = seq(0,0.34, by=0.01))\nPreds.1 <- data.frame(predict(multi_model, newdata=pred.data.I, type=\"p\"))\nPreds.1.df <- data.frame(Preds.1)\nPreds.1.df$Irregular <- pred.data.I$probirch\nGraph.2 <- Preds.1.df %>% pivot_longer(., cols=c(X1,X2,X0)) %>%\nggplot(.) + aes(x=Irregular, y=value, color=name) + geom_point() + theme_minimal()\nlibrary(patchwork)\n```"
  },
  {
    "objectID": "posts/week-4/index.html#a-picture",
    "href": "posts/week-4/index.html#a-picture",
    "title": "Week 4: Ordered and Multinomial Logistic Regression",
    "section": "A Picture",
    "text": "A Picture\n\n```{r}\nGraph.1 + Graph.2 + plot_annotation(\n  title = 'Intermediates')\n```"
  },
  {
    "objectID": "posts/week-4/index.html#a-plot-1",
    "href": "posts/week-4/index.html#a-plot-1",
    "title": "Week 4: Ordered and Multinomial Logistic Regression",
    "section": "A Plot",
    "text": "A Plot\n\n```{r}\npred.data.I <- data.frame(fix_l = 1, float_l = 0, probirch = seq(0,0.34, by=0.01), dumirch = 0)\nPreds.1 <- data.frame(predict(multi_model, newdata=pred.data.I, type=\"p\"))\nPreds.1$Irregular <- pred.data.I$probirch\nPreds.1.df <- data.frame(Preds.1)\nGraph.1 <- Preds.1.df %>% pivot_longer(., cols=c(X1,X2,X0)) %>%\nggplot(.) + aes(x=Irregular, y=value, color=name) + geom_point() + theme_minimal()\npred.data.I <- data.frame(fix_l = 1, float_l = 0, probirch = seq(0,0.34, by=0.01), dumirch = seq(0,0.34, by=0.01))\nPreds.1 <- data.frame(predict(multi_model, newdata=pred.data.I, type=\"p\"))\nPreds.1.df <- data.frame(Preds.1)\nPreds.1.df$Irregular <- pred.data.I$probirch\nGraph.2 <- Preds.1.df %>% pivot_longer(., cols=c(X1,X2,X0)) %>%\nggplot(.) + aes(x=Irregular, y=value, color=name) + geom_point() + theme_minimal()\nlibrary(patchwork)\n```"
  },
  {
    "objectID": "posts/week-4/index.html#a-picture-1",
    "href": "posts/week-4/index.html#a-picture-1",
    "title": "Week 4: Ordered and Multinomial Logistic Regression",
    "section": "A Picture",
    "text": "A Picture\n\n```{r}\nGraph.1 + Graph.2 + plot_annotation(\n  title = 'Fixes')\n```"
  },
  {
    "objectID": "posts/week-4/index.html#a-plot-2",
    "href": "posts/week-4/index.html#a-plot-2",
    "title": "Week 4: Ordered and Multinomial Logistic Regression",
    "section": "A Plot",
    "text": "A Plot\n\n```{r}\npred.data.I <- data.frame(fix_l = 0, float_l = 1, probirch = seq(0,0.34, by=0.01), dumirch = 0)\nPreds.1 <- data.frame(predict(multi_model, newdata=pred.data.I, type=\"p\"))\nPreds.1$Irregular <- pred.data.I$probirch\nPreds.1.df <- data.frame(Preds.1)\nGraph.1 <- Preds.1.df %>% pivot_longer(., cols=c(X1,X2,X0)) %>%\nggplot(.) + aes(x=Irregular, y=value, color=name) + geom_point() + theme_minimal()\npred.data.I <- data.frame(fix_l = 0, float_l = 1, probirch = seq(0,0.34, by=0.01), dumirch = seq(0,0.34, by=0.01))\nPreds.1 <- data.frame(predict(multi_model, newdata=pred.data.I, type=\"p\"))\nPreds.1.df <- data.frame(Preds.1)\nPreds.1.df$Irregular <- pred.data.I$probirch\nGraph.2 <- Preds.1.df %>% pivot_longer(., cols=c(X1,X2,X0)) %>%\nggplot(.) + aes(x=Irregular, y=value, color=name) + geom_point() + theme_minimal()\nlibrary(patchwork)\n```"
  },
  {
    "objectID": "posts/week-4/index.html#a-picture-2",
    "href": "posts/week-4/index.html#a-picture-2",
    "title": "Week 4: Ordered and Multinomial Logistic Regression",
    "section": "A Picture",
    "text": "A Picture\n\n```{r}\nGraph.1 + Graph.2 + plot_annotation(\n  title = 'Floats')\n```"
  },
  {
    "objectID": "posts/week-4/index.html#goodness-of-fit",
    "href": "posts/week-4/index.html#goodness-of-fit",
    "title": "Week 4: Ordered and Multinomial Logistic Regression",
    "section": "Goodness of Fit",
    "text": "Goodness of Fit\n\n```{r}\nDescTools::PseudoR2(multi_model, \n                    which = c(\"McFadden\", \"CoxSnell\", \"Nagelkerke\",\"AIC\"))\n```\n\nWarning in DescTools::PseudoR2(multi_model, which = c(\"McFadden\", \"CoxSnell\", :\nCould not find model or data element of multinom object for evaluating PseudoR2\nnull model. Will fit null model with new evaluation of 'EXRT.data'. Ensure\nobject has not changed since initial call, or try running multinom with 'model =\nTRUE'\n\n\n   McFadden    CoxSnell  Nagelkerke         AIC \n  0.5676473   0.6125299   0.7545319 967.4475199"
  },
  {
    "objectID": "posts/week-4/index.html#simpler",
    "href": "posts/week-4/index.html#simpler",
    "title": "Week 4: Ordered and Multinomial Logistic Regression",
    "section": "Simpler",
    "text": "Simpler\n\n```{r, message=FALSE, warning=FALSE}\nmulti_model.2 <- multinom(\n  formula = regime2aF ~ fix_l +float_l, maxit=500, \n  data = EXRT.data\n)\nmulti_model.3 <- multinom(\n  formula = regime2aF ~ probirch +  fix_l +float_l, maxit=500, \n  data = EXRT.data\n)\n```\n\n# weights:  12 (6 variable)\ninitial  value 1606.171166 \niter  10 value 532.176055\niter  20 value 531.979184\niter  20 value 531.979183\niter  20 value 531.979183\nfinal  value 531.979183 \nconverged\n# weights:  15 (8 variable)\ninitial  value 1441.379323 \niter  10 value 482.863574\niter  20 value 480.237337\niter  30 value 480.208852\nfinal  value 480.208838 \nconverged"
  },
  {
    "objectID": "posts/week-4/index.html#comparisons",
    "href": "posts/week-4/index.html#comparisons",
    "title": "Week 4: Ordered and Multinomial Logistic Regression",
    "section": "Comparisons",
    "text": "Comparisons\n\n```{r}\nDescTools::PseudoR2(multi_model.2, \n                    which = c(\"McFadden\", \"CoxSnell\", \"Nagelkerke\",\"AIC\"))\n```\n\nWarning in DescTools::PseudoR2(multi_model.2, which = c(\"McFadden\",\n\"CoxSnell\", : Could not find model or data element of multinom object for\nevaluating PseudoR2 null model. Will fit null model with new evaluation of\n'EXRT.data'. Ensure object has not changed since initial call, or try running\nmultinom with 'model = TRUE'\n\n```{r}\nDescTools::PseudoR2(multi_model.3, \n                    which = c(\"McFadden\", \"CoxSnell\", \"Nagelkerke\", \"AIC\"))\n```\n\nWarning in DescTools::PseudoR2(multi_model.3, which = c(\"McFadden\",\n\"CoxSnell\", : Could not find model or data element of multinom object for\nevaluating PseudoR2 null model. Will fit null model with new evaluation of\n'EXRT.data'. Ensure object has not changed since initial call, or try running\nmultinom with 'model = TRUE'\n\n\n    McFadden     CoxSnell   Nagelkerke          AIC \n   0.5592956    0.6029024    0.7459794 1075.9583656 \n   McFadden    CoxSnell  Nagelkerke         AIC \n  0.5617286   0.6086804   0.7497900 976.4176765"
  },
  {
    "objectID": "posts/week-4/index.html#the-best-model-is-the-one-presented",
    "href": "posts/week-4/index.html#the-best-model-is-the-one-presented",
    "title": "Week 4: Ordered and Multinomial Logistic Regression",
    "section": "The Best Model is the One Presented",
    "text": "The Best Model is the One Presented\nIt minimizes AIC."
  },
  {
    "objectID": "posts/week-4/index.html#ordered-models",
    "href": "posts/week-4/index.html#ordered-models",
    "title": "Week 4: Ordered and Multinomial Logistic Regression",
    "section": "Ordered Models",
    "text": "Ordered Models\nMy preferred method of thinking about ordered regression involves latent variables. So what is a latent variable? It is something that is unobservable, hence latent, and we only observe coarse realizations in the form of qualitative categories. Consider the example from Li in the Journal of Politics.\n\n\n\nLi Abstract"
  },
  {
    "objectID": "posts/week-4/index.html#the-outcome",
    "href": "posts/week-4/index.html#the-outcome",
    "title": "Week 4: Ordered and Multinomial Logistic Regression",
    "section": "The Outcome",
    "text": "The Outcome\nThe outcome is summed from six individual types of incentives. They are explained here.\n\n\n\nTax Incentives to FDI\n\n\nand\n\n\n\nTax Incentives Part 2"
  },
  {
    "objectID": "posts/week-4/index.html#inputs",
    "href": "posts/week-4/index.html#inputs",
    "title": "Week 4: Ordered and Multinomial Logistic Regression",
    "section": "Inputs",
    "text": "Inputs\nThere are two parts to the data description for the inputs.\n\n\n\nPart 1\n\n\nand \nand there is a further description of other variables that are deployed.\n\n\n\nControls: Part 1\n\n\nand\n\n\n\nControls: Part 2"
  },
  {
    "objectID": "posts/week-4/index.html#the-data-1",
    "href": "posts/week-4/index.html#the-data-1",
    "title": "Week 4: Ordered and Multinomial Logistic Regression",
    "section": "The Data",
    "text": "The Data\nThis should give us an idea of what is going on. The data come in Stata format; we can read these via the foreign or haven libraries in R.\n\n```{r}\nlibrary(MASS); library(foreign)\n```\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:patchwork':\n\n    area\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n```{r}\nLi.Data <- read.dta(\"https://github.com/robertwwalker/ChoiceAndForecasting/raw/main/posts/week-4/data/li-replication.dta\")\ntable(Li.Data$generosityg)\nLi.Data$generositygF <- as.factor(Li.Data$generosityg)\n```\n\n\n 0  1  2  3  4  5  6 \n13 10 19  7  2  1  1 \n\n\nIt is worthwhile to notice that the top of the scale is rather sparse.\nThere is also a concern about FDI and economies of scale. The following is a plot of the relationship between FDI and size of the economy in the sample.\n\n\n\nFD-Size\n\n\nWithout careful attention to normalization, China is a clear x-y outlier."
  },
  {
    "objectID": "posts/week-4/index.html#motivating-the-model",
    "href": "posts/week-4/index.html#motivating-the-model",
    "title": "Week 4: Ordered and Multinomial Logistic Regression",
    "section": "Motivating the Model",
    "text": "Motivating the Model\nSuppose there is some unobserved continuous variable, call it \\(y^{*}\\) that measures the willingness/utility to be derived from tax incentives to FDI. Unfortunately, this latent quantity is unobservable; we instead observe how many incentives are offered and posit that the number of incentives is a manifestation of increasing utility with unknown points of separation – cutpoints – that separate these latent utilities into a mutually exclusive and exhaustive partition. In a simplified example, consider this.\n\n```{r}\nplot(density(rlogis(1000)))\nabline(v=c(-3,-2,-1,0,2,4))\n```\n\n\n\n\nSo anything below -3 is zero incentives; anything between -3 and -2 is one incentive, … , and anything above 4 should be all six incentives. What we have is a regression problem but the outcome is unobserved and takes the form of a logistic random variable. Indeed, one could write the equation as:\n\\[y^{*} = X\\beta + \\epsilon\\] where \\(\\epsilon\\) is assumed to have a logistic distribution but this is otherwise just a linear regression. Indeed, the direct interpretation of the slopes is the effect of a one-unit change in X on that logistic random variable."
  },
  {
    "objectID": "posts/week-4/index.html#what-to-replicate",
    "href": "posts/week-4/index.html#what-to-replicate",
    "title": "Week 4: Ordered and Multinomial Logistic Regression",
    "section": "What to Replicate",
    "text": "What to Replicate\nThe table of estimates is presented in the paper; I will copy it here.\n\n\n\nResults Table\n\n\nI will choose two of a few models estimated in the paper. First, let us have a look at Model 1.\n\n```{r}\nli.mod1 <- polr(generositygF ~ law00log + transition, data=Li.Data)\nsummary(li.mod1)\n```\n\n\nRe-fitting to get Hessian\n\n\nCall:\npolr(formula = generositygF ~ law00log + transition, data = Li.Data)\n\nCoefficients:\n             Value Std. Error t value\nlaw00log   -0.6192     0.4756 -1.3019\ntransition -0.5161     0.7126 -0.7243\n\nIntercepts:\n    Value   Std. Error t value\n0|1 -1.3617  0.3768    -3.6144\n1|2 -0.4888  0.3323    -1.4707\n2|3  1.1785  0.3668     3.2133\n3|4  2.3771  0.5361     4.4339\n4|5  3.1160  0.7325     4.2541\n5|6  3.8252  1.0183     3.7565\n\nResidual Deviance: 164.3701 \nAIC: 180.3701 \n\n\nWe can read these by stars. There is nothing that is clearly different from zero as a slope or 1 as an odds-ratio. The authors deploy a common strategy for adjusting standard errors that, in this case, is necessary to find a relationship with statistical confidence. That’s a diversion. To the story. In general, the sign of the rule of law indicator is negative, so as rule of law increases, incentives decrease though we cannot rule out no effect. Transitions also have a negative sign; regime changes have no clear influence on incentives. There is additional information that is commonly given short-shrift. What do the cutpoints separating the categories imply? Let’s think this through recongizing that the estimates have an underlying t/normal distribution. 4|5 is within one standard error of both 3|4 and 5|6. The model cannot really tell these values apart. Things do improve in the lower part of the scale but we should note that this is where the vast majority of the data are actually observed.\n\nOdds Ratios\nNext, I will turn the estimates into odds-ratios by exponentiating the estimates.\n\n```{r}\nexp(li.mod1$coefficients)\n```\n\n  law00log transition \n 0.5384016  0.5968309 \n\n\nFor Kawika, this is one of the many cases that I am familiar with where robust is necessary to find something. Note neither effect can be differentiated from zero with much confidence at all. To further examine the claims, I will also replicate the right-most column."
  },
  {
    "objectID": "posts/week-4/index.html#column-4-estimates",
    "href": "posts/week-4/index.html#column-4-estimates",
    "title": "Week 4: Ordered and Multinomial Logistic Regression",
    "section": "Column 4 Estimates",
    "text": "Column 4 Estimates\n\n```{r}\nli.mod4 <- polr(generositygF ~ law00log + transition + fdiinf + democfdi + democ + autocfdi2 + autocfdir + reggengl + reggengl2 + gdppclog + gdplog, data=Li.Data)\nsummary(li.mod4)\n```\n\n\nRe-fitting to get Hessian\n\n\nCall:\npolr(formula = generositygF ~ law00log + transition + fdiinf + \n    democfdi + democ + autocfdi2 + autocfdir + reggengl + reggengl2 + \n    gdppclog + gdplog, data = Li.Data)\n\nCoefficients:\n              Value Std. Error t value\nlaw00log   -0.89148    0.66806 -1.3344\ntransition -0.57123    0.94945 -0.6016\nfdiinf      0.37605    0.18055  2.0828\ndemocfdi   -0.39969    0.18228 -2.1927\ndemoc      -1.23307    0.77661 -1.5878\nautocfdi2   1.24932    1.94198  0.6433\nautocfdir  -3.17796    1.95351 -1.6268\nreggengl    1.81476    0.44754  4.0550\nreggengl2  -0.05777    0.01444 -4.0007\ngdppclog    0.20891    0.43867  0.4762\ngdplog      0.15754    0.18138  0.8686\n\nIntercepts:\n    Value    Std. Error t value \n0|1  13.7773   0.1020   135.0542\n1|2  14.7314   0.3048    48.3349\n2|3  16.9740   0.5230    32.4561\n3|4  18.7911   0.8027    23.4107\n4|5  20.0387   1.1590    17.2900\n5|6  23.2947   4.7216     4.9336\n\nResidual Deviance: 134.2212 \nAIC: 168.2212 \n(2 observations deleted due to missingness)\n\n\nMeasured via odds-ratios, we can obtain those:\n\n```{r}\nexp(li.mod4$coefficients)\n```\n\n  law00log transition     fdiinf   democfdi      democ  autocfdi2  autocfdir \n0.41004648 0.56483013 1.45651991 0.67052543 0.29139805 3.48796970 0.04167039 \n  reggengl  reggengl2   gdppclog     gdplog \n6.13958891 0.94386864 1.23232943 1.17063051"
  },
  {
    "objectID": "posts/week-4/index.html#diagnostics-and-commentary",
    "href": "posts/week-4/index.html#diagnostics-and-commentary",
    "title": "Week 4: Ordered and Multinomial Logistic Regression",
    "section": "Diagnostics and Commentary",
    "text": "Diagnostics and Commentary\nGoodness of Fit:\n\n```{r}\nDescTools::PseudoR2(\n  li.mod1, \n  which = c(\"McFadden\", \"CoxSnell\", \"Nagelkerke\", \"AIC\")\n)\nDescTools::PseudoR2(\n  li.mod4, \n  which = c(\"McFadden\", \"CoxSnell\", \"Nagelkerke\", \"AIC\")\n)\n```\n\n    McFadden     CoxSnell   Nagelkerke          AIC \n  0.01104919   0.03405653   0.03560378 180.37009764 \n   McFadden    CoxSnell  Nagelkerke         AIC \n  0.1649728   0.4054507   0.4235700 168.2212440 \n\n\nThe last model is clearly better than the first by any of these measures. That said, there are a lot of additional predictors that add much complexity to the model and the difference in AIC is not very large.\nWhat about the others?\n\n```{r}\n# lipsitz test \n# generalhoslem::lipsitz.test(li.mod1)\n# generalhoslem::lipsitz.test(li.mod4)\n```\n\nThey fail to work because of sparseness.\n\nTesting Proportional-Odds\nThe text cites a test that owes to Brant on examining proportionality. It turns out that I know a bit about this; I published a purely theoretical stats paper showing that it is not at all clear what the alternative hypothesis embodied in this test actually means because the only model with a proper probability distribution for \\(y^{*}\\) is this proportional-odds model.\nI will follow the text with this caveat in mind:\n\n```{r}\nbrant::brant(li.mod1)\n```\n\n-------------------------------------------- \nTest for    X2  df  probability \n-------------------------------------------- \nOmnibus     13.51   10  0.2\nlaw00log    8.64    5   0.12\ntransition  4.28    5   0.51\n-------------------------------------------- \n\nH0: Parallel Regression Assumption holds\n\n\n\n\nHow Well Does the Model Predict?\n\n```{r}\nMat.Fit <- data.frame(fitted(li.mod4))\nlibrary(tidyverse)\nMat.Fit$pred.val <- rep(-999, 51)\nMat.Fit$pred.val[Mat.Fit$X0 > Mat.Fit$X1 & Mat.Fit$X0 > Mat.Fit$X2 & Mat.Fit$X0 > Mat.Fit$X3 & Mat.Fit$X0 > Mat.Fit$X4 & Mat.Fit$X0 > Mat.Fit$X5 & Mat.Fit$X0 > Mat.Fit$X6] <- 0\nMat.Fit$pred.val[Mat.Fit$X1 > Mat.Fit$X0 & Mat.Fit$X1 > Mat.Fit$X2 & Mat.Fit$X1 > Mat.Fit$X3 & Mat.Fit$X1 > Mat.Fit$X4 & Mat.Fit$X1 > Mat.Fit$X5 & Mat.Fit$X1 > Mat.Fit$X6] <- 1\nMat.Fit$pred.val[Mat.Fit$X2 > Mat.Fit$X0 & Mat.Fit$X2 > Mat.Fit$X1 & Mat.Fit$X2 > Mat.Fit$X3 & Mat.Fit$X2 > Mat.Fit$X4 & Mat.Fit$X2 > Mat.Fit$X5 & Mat.Fit$X2 > Mat.Fit$X6] <- 2\nMat.Fit$pred.val[Mat.Fit$X3 > Mat.Fit$X0 & Mat.Fit$X3 > Mat.Fit$X1 & Mat.Fit$X3 > Mat.Fit$X2 & Mat.Fit$X3 > Mat.Fit$X4 & Mat.Fit$X3 > Mat.Fit$X5 & Mat.Fit$X3 > Mat.Fit$X6] <- 3\nMat.Fit$pred.val[Mat.Fit$X5 > Mat.Fit$X0 & Mat.Fit$X5 > Mat.Fit$X1 & Mat.Fit$X5 > Mat.Fit$X2 & Mat.Fit$X5 > Mat.Fit$X3 & Mat.Fit$X5 > Mat.Fit$X4 & Mat.Fit$X5 > Mat.Fit$X6] <- 5\nMat.Fit$pred.val[Mat.Fit$X6 > Mat.Fit$X0 & Mat.Fit$X6 > Mat.Fit$X1 & Mat.Fit$X6 > Mat.Fit$X2 & Mat.Fit$X6 > Mat.Fit$X3 & Mat.Fit$X6 > Mat.Fit$X4 & Mat.Fit$X6 > Mat.Fit$X5] <- 6\nPred.Data <- Li.Data[c(1:28,30:41,43:53),]\ntable(Pred.Data$generosityg,Mat.Fit$pred.val)\n```\n\n   \n     0  2  3  6\n  0  6  7  0  0\n  1  5  4  0  0\n  2  5 12  1  0\n  3  1  2  4  0\n  4  0  1  1  0\n  5  0  1  0  0\n  6  0  0  0  1\n\n\nSo 6+12+4+1 or 23 of 51 are correctly predicted with a rather big and complicated model.\n\n\nOn AIC\nThe AIC [and BIC] are built around the idea of likelihood presented last time. The formal definition, which is correct on Wikipedia explains the following:\n\n\n\nAIC"
  },
  {
    "objectID": "posts/week-5/index.html",
    "href": "posts/week-5/index.html",
    "title": "Week 5: Hierarchical Data",
    "section": "",
    "text": "The slides are here..\nOur fifth class meeting will focus on Chapter 8 of Handbook of Regression Modeling in People Analytics."
  },
  {
    "objectID": "posts/week-5/index.html#the-skinny",
    "href": "posts/week-5/index.html#the-skinny",
    "title": "Week 5: Hierarchical Data",
    "section": "The Skinny",
    "text": "The Skinny\nHierarchical models represent an advance on more standard linear and generalized linear models with the recognition that data have hierarchical forms of organization with varying degrees of freedom for the predictors. These models can, generically, be combined with techniques that we have already learned to expand the range of our toolkit. Since we left last week off with ordered models, they will first occupy our attention."
  },
  {
    "objectID": "posts/week-5/index.html#ordered-models",
    "href": "posts/week-5/index.html#ordered-models",
    "title": "Week 5: Hierarchical Data",
    "section": "Ordered Models",
    "text": "Ordered Models\nMy preferred method of thinking about ordered regression involves latent variables. So what is a latent variable? It is something that is unobservable, hence latent, and we only observe coarse realizations in the form of qualitative categories. Consider the example from Li in the Journal of Politics.\n\n\n\nLi Abstract"
  },
  {
    "objectID": "posts/week-5/index.html#the-outcome",
    "href": "posts/week-5/index.html#the-outcome",
    "title": "Week 5: Hierarchical Data",
    "section": "The Outcome",
    "text": "The Outcome\nThe outcome is summed from six individual types of incentives. They are explained here.\n\n\n\nTax Incentives to FDI\n\n\nand\n\n\n\nTax Incentives Part 2"
  },
  {
    "objectID": "posts/week-5/index.html#inputs",
    "href": "posts/week-5/index.html#inputs",
    "title": "Week 5: Hierarchical Data",
    "section": "Inputs",
    "text": "Inputs\nThere are two parts to the data description for the inputs.\n\n\n\nPart 1\n\n\nand \nand there is a further description of other variables that are deployed.\n\n\n\nControls: Part 1\n\n\nand\n\n\n\nControls: Part 2"
  },
  {
    "objectID": "posts/week-5/index.html#the-data",
    "href": "posts/week-5/index.html#the-data",
    "title": "Week 5: Hierarchical Data",
    "section": "The Data",
    "text": "The Data\nThis should give us an idea of what is going on. The data come in Stata format; we can read these via the foreign or haven libraries in R.\n\n```{r}\nlibrary(MASS); library(foreign)\nLi.Data <- read.dta(\"./data/li-replication.dta\")\ntable(Li.Data$generosityg)\nLi.Data$generositygF <- as.factor(Li.Data$generosityg)\n```\n\n\n 0  1  2  3  4  5  6 \n13 10 19  7  2  1  1 \n\n\nIt is worthwhile to notice that the top of the scale is rather sparse.\nThere is also a concern about FDI and economies of scale. The following is a plot of the relationship between FDI and size of the economy in the sample.\n\n\n\nFD-Size\n\n\nWithout careful attention to normalization, China is a clear x-y outlier."
  },
  {
    "objectID": "posts/week-5/index.html#motivating-the-model",
    "href": "posts/week-5/index.html#motivating-the-model",
    "title": "Week 5: Hierarchical Data",
    "section": "Motivating the Model",
    "text": "Motivating the Model\nSuppose there is some unobserved continuous variable, call it \\(y^{*}\\) that measures the willingness/utility to be derived from tax incentives to FDI. Unfortunately, this latent quantity is unobservable; we instead observe how many incentives are offered and posit that the number of incentives is a manifestation of increasing utility with unknown points of separation – cutpoints – that separate these latent utilities into a mutually exclusive and exhaustive partition. In a simplified example, consider this.\n\n```{r}\nplot(density(rlogis(1000)))\nabline(v=c(-3,-2,-1,0,2,4))\n```\n\n\n\n\nSo anything below -3 is zero incentives; anything between -3 and -2 is one incentive, … , and anything above 4 should be all six incentives. What we have is a regression problem but the outcome is unobserved and takes the form of a logistic random variable. Indeed, one could write the equation as:\n\\[y^{*} = X\\beta + \\epsilon\\]\nwhere \\(\\epsilon\\) is assumed to have a logistic distribution but this is otherwise just a linear regression. Indeed, the direct interpretation of the slopes is the effect of a one-unit change in X on that logistic random variable."
  },
  {
    "objectID": "posts/week-5/index.html#what-to-replicate",
    "href": "posts/week-5/index.html#what-to-replicate",
    "title": "Week 5: Hierarchical Data",
    "section": "What to Replicate",
    "text": "What to Replicate\nThe table of estimates is presented in the paper; I will copy it here.\n\n\n\nResults Table\n\n\nI will choose two of a few models estimated in the paper. First, let us have a look at Model 1.\n\n```{r}\nli.mod1 <- polr(generositygF ~ law00log + transition, data=Li.Data)\nsummary(li.mod1)\n```\n\n\nRe-fitting to get Hessian\n\n\nCall:\npolr(formula = generositygF ~ law00log + transition, data = Li.Data)\n\nCoefficients:\n             Value Std. Error t value\nlaw00log   -0.6192     0.4756 -1.3019\ntransition -0.5161     0.7126 -0.7243\n\nIntercepts:\n    Value   Std. Error t value\n0|1 -1.3617  0.3768    -3.6144\n1|2 -0.4888  0.3323    -1.4707\n2|3  1.1785  0.3668     3.2133\n3|4  2.3771  0.5361     4.4339\n4|5  3.1160  0.7325     4.2541\n5|6  3.8252  1.0183     3.7565\n\nResidual Deviance: 164.3701 \nAIC: 180.3701 \n\n\nWe can read these by stars. There is nothing that is clearly different from zero as a slope or 1 as an odds-ratio. The authors deploy a common strategy for adjusting standard errors that, in this case, is necessary to find a relationship with statistical confidence. That’s a diversion. To the story. In general, the sign of the rule of law indicator is negative, so as rule of law increases, incentives decrease though we cannot rule out no effect. Transitions also have a negative sign; regime changes have no clear influence on incentives. There is additional information that is commonly given short-shrift. What do the cutpoints separating the categories imply? Let’s think this through recongizing that the estimates have an underlying t/normal distribution. 4|5 is within one standard error of both 3|4 and 5|6. The model cannot really tell these values apart. Things do improve in the lower part of the scale but we should note that this is where the vast majority of the data are actually observed.\n\nOdds Ratios\nNext, I will turn the estimates into odds-ratios by exponentiating the estimates.\n\n```{r}\nexp(li.mod1$coefficients)\n```\n\n  law00log transition \n 0.5384016  0.5968309 \n\n\nFor Kawika, this is one of the many cases that I am familiar with where robust is necessary to find something. Note neither effect can be differentiated from zero with much confidence at all. To further examine the claims, I will also replicate the right-most column."
  },
  {
    "objectID": "posts/week-5/index.html#column-4-estimates",
    "href": "posts/week-5/index.html#column-4-estimates",
    "title": "Week 5: Hierarchical Data",
    "section": "Column 4 Estimates",
    "text": "Column 4 Estimates\n\n```{r}\nli.mod4 <- polr(generositygF ~ law00log + transition + fdiinf + democfdi + democ + autocfdi2 + autocfdir + reggengl + reggengl2 + gdppclog + gdplog, data=Li.Data)\nsummary(li.mod4)\n```\n\n\nRe-fitting to get Hessian\n\n\nCall:\npolr(formula = generositygF ~ law00log + transition + fdiinf + \n    democfdi + democ + autocfdi2 + autocfdir + reggengl + reggengl2 + \n    gdppclog + gdplog, data = Li.Data)\n\nCoefficients:\n              Value Std. Error t value\nlaw00log   -0.89148    0.66806 -1.3344\ntransition -0.57123    0.94945 -0.6016\nfdiinf      0.37605    0.18055  2.0828\ndemocfdi   -0.39969    0.18228 -2.1927\ndemoc      -1.23307    0.77661 -1.5878\nautocfdi2   1.24932    1.94198  0.6433\nautocfdir  -3.17796    1.95351 -1.6268\nreggengl    1.81476    0.44754  4.0550\nreggengl2  -0.05777    0.01444 -4.0007\ngdppclog    0.20891    0.43867  0.4762\ngdplog      0.15754    0.18138  0.8686\n\nIntercepts:\n    Value    Std. Error t value \n0|1  13.7773   0.1020   135.0542\n1|2  14.7314   0.3048    48.3349\n2|3  16.9740   0.5230    32.4561\n3|4  18.7911   0.8027    23.4107\n4|5  20.0387   1.1590    17.2900\n5|6  23.2947   4.7216     4.9336\n\nResidual Deviance: 134.2212 \nAIC: 168.2212 \n(2 observations deleted due to missingness)\n\n\nMeasured via odds-ratios, we can obtain those:\n\n```{r}\nexp(li.mod4$coefficients)\n```\n\n  law00log transition     fdiinf   democfdi      democ  autocfdi2  autocfdir \n0.41004648 0.56483013 1.45651991 0.67052543 0.29139805 3.48796970 0.04167039 \n  reggengl  reggengl2   gdppclog     gdplog \n6.13958891 0.94386864 1.23232943 1.17063051"
  },
  {
    "objectID": "posts/week-5/index.html#diagnostics-and-commentary",
    "href": "posts/week-5/index.html#diagnostics-and-commentary",
    "title": "Week 5: Hierarchical Data",
    "section": "Diagnostics and Commentary",
    "text": "Diagnostics and Commentary\nGoodness of Fit:\n\n```{r}\nDescTools::PseudoR2(\n  li.mod1, \n  which = c(\"McFadden\", \"CoxSnell\", \"Nagelkerke\", \"AIC\")\n)\nDescTools::PseudoR2(\n  li.mod4, \n  which = c(\"McFadden\", \"CoxSnell\", \"Nagelkerke\", \"AIC\")\n)\n```\n\n    McFadden     CoxSnell   Nagelkerke          AIC \n  0.01104919   0.03405653   0.03560378 180.37009764 \n   McFadden    CoxSnell  Nagelkerke         AIC \n  0.1649728   0.4054507   0.4235700 168.2212440 \n\n\nThe last model is clearly better than the first by any of these measures. That said, there are a lot of additional predictors that add much complexity to the model and the difference in AIC is not very large.\nWhat about the others?\n\n```{r}\n# lipsitz test \n# generalhoslem::lipsitz.test(li.mod1)\n# generalhoslem::lipsitz.test(li.mod4)\n```\n\nThey fail to work because of sparseness.\n\nTesting Proportional-Odds\nThe text cites a test that owes to Brant on examining proportionality. It turns out that I know a bit about this; I published a purely theoretical stats paper showing that it is not at all clear what the alternative hypothesis embodied in this test actually means because the only model with a proper probability distribution for \\(y^{*}\\) is this proportional-odds model.\nI will follow the text with this caveat in mind:\n\n```{r}\nbrant::brant(li.mod1)\n```\n\n-------------------------------------------- \nTest for    X2  df  probability \n-------------------------------------------- \nOmnibus     13.51   10  0.2\nlaw00log    8.64    5   0.12\ntransition  4.28    5   0.51\n-------------------------------------------- \n\nH0: Parallel Regression Assumption holds\n\n\n\n\nHow Well Does the Model Predict?\n\n```{r}\nMat.Fit <- data.frame(fitted(li.mod4))\nlibrary(tidyverse)\n```\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.8     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.1\n✔ readr   2.1.2     ✔ forcats 0.5.2\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n✖ dplyr::select() masks MASS::select()\n\n```{r}\nMat.Fit$pred.val <- rep(-999, 51)\nMat.Fit$pred.val[Mat.Fit$X0 > Mat.Fit$X1 & Mat.Fit$X0 > Mat.Fit$X2 & Mat.Fit$X0 > Mat.Fit$X3 & Mat.Fit$X0 > Mat.Fit$X4 & Mat.Fit$X0 > Mat.Fit$X5 & Mat.Fit$X0 > Mat.Fit$X6] <- 0\nMat.Fit$pred.val[Mat.Fit$X1 > Mat.Fit$X0 & Mat.Fit$X1 > Mat.Fit$X2 & Mat.Fit$X1 > Mat.Fit$X3 & Mat.Fit$X1 > Mat.Fit$X4 & Mat.Fit$X1 > Mat.Fit$X5 & Mat.Fit$X1 > Mat.Fit$X6] <- 1\nMat.Fit$pred.val[Mat.Fit$X2 > Mat.Fit$X0 & Mat.Fit$X2 > Mat.Fit$X1 & Mat.Fit$X2 > Mat.Fit$X3 & Mat.Fit$X2 > Mat.Fit$X4 & Mat.Fit$X2 > Mat.Fit$X5 & Mat.Fit$X2 > Mat.Fit$X6] <- 2\nMat.Fit$pred.val[Mat.Fit$X3 > Mat.Fit$X0 & Mat.Fit$X3 > Mat.Fit$X1 & Mat.Fit$X3 > Mat.Fit$X2 & Mat.Fit$X3 > Mat.Fit$X4 & Mat.Fit$X3 > Mat.Fit$X5 & Mat.Fit$X3 > Mat.Fit$X6] <- 3\nMat.Fit$pred.val[Mat.Fit$X5 > Mat.Fit$X0 & Mat.Fit$X5 > Mat.Fit$X1 & Mat.Fit$X5 > Mat.Fit$X2 & Mat.Fit$X5 > Mat.Fit$X3 & Mat.Fit$X5 > Mat.Fit$X4 & Mat.Fit$X5 > Mat.Fit$X6] <- 5\nMat.Fit$pred.val[Mat.Fit$X6 > Mat.Fit$X0 & Mat.Fit$X6 > Mat.Fit$X1 & Mat.Fit$X6 > Mat.Fit$X2 & Mat.Fit$X6 > Mat.Fit$X3 & Mat.Fit$X6 > Mat.Fit$X4 & Mat.Fit$X6 > Mat.Fit$X5] <- 6\nPred.Data <- Li.Data[c(1:28,30:41,43:53),]\ntable(Pred.Data$generosityg,Mat.Fit$pred.val)\n```\n\n   \n     0  2  3  6\n  0  6  7  0  0\n  1  5  4  0  0\n  2  5 12  1  0\n  3  1  2  4  0\n  4  0  1  1  0\n  5  0  1  0  0\n  6  0  0  0  1\n\n\nSo 6+12+4+1 or 23 of 51 are correctly predicted with a rather big and complicated model.\n\n\nOn AIC\nThe AIC [and BIC] are built around the idea of likelihood presented last time. The formal definition, which is correct on Wikipedia explains the following:\n\n\n\nAIC"
  },
  {
    "objectID": "posts/week-5/index.html#hierarchical-models-and-sem",
    "href": "posts/week-5/index.html#hierarchical-models-and-sem",
    "title": "Week 5: Hierarchical Data",
    "section": "Hierarchical Models and SEM",
    "text": "Hierarchical Models and SEM\n\nHierarchical Models\nTo examine a hierarchical model, I am going to choose some interesting data on popularity. A description appears below; these data come from an Intro to Multilevel Analysis.\n\n\n\nPopularity Data\n\n\nThough the data are technically ordered, this feature is not exploited to build a hierarchical ordered regression model, though it could be done. Instead, the outcome of interest is an average of ordered scales.\n\n\nLoad the data\n\n```{r}\nlibrary(tidyverse)\nlibrary(haven)\npopular2data <- read_sav(file =\"https://github.com/MultiLevelAnalysis/Datasets-third-edition-Multilevel-book/blob/master/chapter%202/popularity/SPSS/popular2.sav?raw=true\")\npopular2data <- popular2data %>% dplyr::select(pupil, class, extrav, sex, texp, popular)\n```\n\n\n\nA Summary\n\n```{r}\nsummary(popular2data)\nhead(popular2data)\n```\n\n     pupil           class            extrav            sex        \n Min.   : 1.00   Min.   :  1.00   Min.   : 1.000   Min.   :0.0000  \n 1st Qu.: 6.00   1st Qu.: 25.00   1st Qu.: 4.000   1st Qu.:0.0000  \n Median :11.00   Median : 51.00   Median : 5.000   Median :1.0000  \n Mean   :10.65   Mean   : 50.37   Mean   : 5.215   Mean   :0.5055  \n 3rd Qu.:16.00   3rd Qu.: 76.00   3rd Qu.: 6.000   3rd Qu.:1.0000  \n Max.   :26.00   Max.   :100.00   Max.   :10.000   Max.   :1.0000  \n      texp          popular     \n Min.   : 2.00   Min.   :0.000  \n 1st Qu.: 8.00   1st Qu.:4.100  \n Median :15.00   Median :5.100  \n Mean   :14.26   Mean   :5.076  \n 3rd Qu.:20.00   3rd Qu.:6.000  \n Max.   :25.00   Max.   :9.500  \n# A tibble: 6 × 6\n  pupil class extrav sex        texp popular\n  <dbl> <dbl>  <dbl> <dbl+lbl> <dbl>   <dbl>\n1     1     1      5 1 [girl]     24     6.3\n2     2     1      7 0 [boy]      24     4.9\n3     3     1      4 1 [girl]     24     5.3\n4     4     1      3 1 [girl]     24     4.7\n5     5     1      5 1 [girl]     24     6  \n6     6     1      4 0 [boy]      24     4.7\n\n\n\n\nA plot of the relationship of interest\n\n```{r}\nggplot(data    = popular2data,\n       aes(x   = extrav,\n           y   = popular,\n           col = class))+ #to add the colours for different classes\n  geom_point(size     = 0.8,\n             alpha    = .8,\n             position = \"jitter\")+ #to add some random noise for plotting purposes\n  theme_minimal()+\n  theme(legend.position = \"none\")+\n  scale_color_gradientn(colours = rainbow(100))+\n  labs(title    = \"Popularity vs. Extraversion\",\n       subtitle = \"add colours for different classes\",\n       x = \"Extroversion\",\n       y = \"Average Popularity\")\n```\n\n\n\n\n\n\nWith the lines\n\n```{r}\nggplot(data      = popular2data,\n       aes(x     = extrav,\n           y     = popular,\n           col   = class,\n           group = class))+ #to add the colours for different classes\n  geom_point(size     = 1.2,\n             alpha    = .8,\n             position = \"jitter\")+ #to add some random noise for plotting purposes\n  theme_minimal()+\n  theme(legend.position = \"none\")+\n  scale_color_gradientn(colours = rainbow(100))+\n  geom_smooth(method = lm,\n              se     = FALSE,\n              size   = .5, \n              alpha  = .8)+ # to add regression line\n  labs(title    = \"Popularity vs. Extraversion\",\n       subtitle = \"add colours for different classes and regression lines\",\n       x = \"Extroversion\",\n       y = \"Average Popularity\")\n```\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\n\nA regression\n\n```{r}\nggplot(data = popular2data, \n       aes(x   = extrav,\n           y   = popular, \n           col = as.factor(sex)))+\n  geom_point(size     = 1, \n             alpha    = .7, \n             position = \"jitter\")+\n  geom_smooth(method   = lm,\n              se       = T, \n              size     = 1.5, \n              linetype = 1, \n              alpha    = .7)+\n  theme_minimal()+\n  labs(title    = \"Popularity and Extraversion for 2 Genders\", \n       subtitle = \"The linear relationship between the two is similar for both genders\")+\n  scale_color_manual(name   =\" Gender\",\n                     labels = c(\"Boys\", \"Girls\"),\n                     values = c(\"lightblue\", \"pink\"))\n```\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nA model with random intercepts\n\n```{r, warning=FALSE, message=FALSE}\nlibrary(lme4)\noptions(scipen=7)\nlibrary(lmerTest)\nmodel1 <- lmer(formula = popular ~ 1 + sex + extrav + (1|class), \n               data    = popular2data)\nsummary(model1)\n```\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: popular ~ 1 + sex + extrav + (1 | class)\n   Data: popular2data\n\nREML criterion at convergence: 4948.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.2091 -0.6575 -0.0044  0.6732  2.9755 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n class    (Intercept) 0.6272   0.7919  \n Residual             0.5921   0.7695  \nNumber of obs: 2000, groups:  class, 100\n\nFixed effects:\n              Estimate Std. Error         df t value Pr(>|t|)    \n(Intercept)    2.14096    0.11729  390.76822   18.25   <2e-16 ***\nsex            1.25300    0.03743 1926.69933   33.48   <2e-16 ***\nextrav         0.44161    0.01616 1956.77498   27.33   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n       (Intr) sex   \nsex    -0.100       \nextrav -0.705 -0.085\n\n\nThough in this case, we probably do not need them but p-values can be obtained from lmerTest. The standard lme4 summary does not have them.\nNow let’s add a second-level predictor. Teacher experience does not vary within a given classroom, only across the 100 classrooms. Let’s look at this model.\n\n```{r}\nmodel2 <- lmer(popular ~ 1 + sex + extrav + texp + (1 | class), data=popular2data)\nsummary(model2)\n```\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: popular ~ 1 + sex + extrav + texp + (1 | class)\n   Data: popular2data\n\nREML criterion at convergence: 4885\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.1745 -0.6491 -0.0075  0.6705  3.0078 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n class    (Intercept) 0.2954   0.5435  \n Residual             0.5920   0.7694  \nNumber of obs: 2000, groups:  class, 100\n\nFixed effects:\n               Estimate  Std. Error          df t value  Pr(>|t|)    \n(Intercept)    0.809766    0.169993  226.431473   4.764 0.0000034 ***\nsex            1.253800    0.037290 1948.303018  33.623   < 2e-16 ***\nextrav         0.454431    0.016165 1954.889209  28.112   < 2e-16 ***\ntexp           0.088407    0.008764  101.627424  10.087   < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n       (Intr) sex    extrav\nsex    -0.040              \nextrav -0.589 -0.090       \ntexp   -0.802 -0.036  0.139\n\n\nMore experienced teachers lead to higher reported average popularity.\n\n\nRandom slopes\n\n```{r, message=FALSE, warning=FALSE}\nmodel3 <- lmer(formula = popular ~ 1 + sex + extrav + texp + (1 + sex + extrav | class),\n               data    = popular2data, control=lmerControl(optCtrl=list(maxfun=100000) ))\nsummary(model3)\n```\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: popular ~ 1 + sex + extrav + texp + (1 + sex + extrav | class)\n   Data: popular2data\nControl: lmerControl(optCtrl = list(maxfun = 100000))\n\nREML criterion at convergence: 4833.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.1643 -0.6554 -0.0246  0.6711  2.9570 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr       \n class    (Intercept) 1.342020 1.15846             \n          sex         0.002404 0.04903  -0.39      \n          extrav      0.034742 0.18639  -0.88 -0.09\n Residual             0.551435 0.74259             \nNumber of obs: 2000, groups:  class, 100\n\nFixed effects:\n              Estimate Std. Error         df t value Pr(>|t|)    \n(Intercept)   0.758511   0.197316 181.050469   3.844 0.000167 ***\nsex           1.250810   0.036942 986.050567  33.859  < 2e-16 ***\nextrav        0.452854   0.024645  96.208501  18.375  < 2e-16 ***\ntexp          0.089520   0.008618 101.321705  10.388  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n       (Intr) sex    extrav\nsex    -0.062              \nextrav -0.718 -0.066       \ntexp   -0.684 -0.039  0.089\noptimizer (nloptwrap) convergence code: 0 (OK)\nModel failed to converge with max|grad| = 0.00597328 (tol = 0.002, component 1)\n\n\n\n\nExamining the Model\n\n```{r}\nranova(model3)\n```\n\nboundary (singular) fit: see help('isSingular')\n\n\nANOVA-like table for random-effects: Single term deletions\n\nModel:\npopular ~ sex + extrav + texp + (1 + sex + extrav | class)\n                                     npar  logLik    AIC    LRT Df\n<none>                                 11 -2416.6 4855.3          \nsex in (1 + sex + extrav | class)       8 -2417.4 4850.8  1.513  3\nextrav in (1 + sex + extrav | class)    8 -2441.9 4899.8 50.507  3\n                                           Pr(>Chisq)    \n<none>                                                   \nsex in (1 + sex + extrav | class)              0.6792    \nextrav in (1 + sex + extrav | class) 0.00000000006232 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe random effect associated with sex is not close to significance.\n\n\nA Crossed-Effects Model\n\n```{r}\nmodel5<-lmer(formula = popular ~ 1 + sex + extrav + texp+ extrav*texp + (1 + extrav | class), \n             data    = popular2data)\nsummary(model5)\n```\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: popular ~ 1 + sex + extrav + texp + extrav * texp + (1 + extrav |  \n    class)\n   Data: popular2data\n\nREML criterion at convergence: 4780.5\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-3.12872 -0.63857 -0.01129  0.67916  3.05006 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n class    (Intercept) 0.478639 0.69184       \n          extrav      0.005409 0.07355  -0.64\n Residual             0.552769 0.74348       \nNumber of obs: 2000, groups:  class, 100\n\nFixed effects:\n               Estimate  Std. Error          df t value Pr(>|t|)    \n(Intercept)   -1.209607    0.271901  109.345831  -4.449 2.09e-05 ***\nsex            1.240698    0.036233 1941.077365  34.243  < 2e-16 ***\nextrav         0.803578    0.040117   72.070164  20.031  < 2e-16 ***\ntexp           0.226197    0.016807   98.507109  13.458  < 2e-16 ***\nextrav:texp   -0.024728    0.002555   71.986847  -9.679 1.15e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) sex    extrav texp  \nsex          0.002                     \nextrav      -0.867 -0.065              \ntexp        -0.916 -0.047  0.801       \nextrav:texp  0.773  0.033 -0.901 -0.859\n\n\n\n\nA Picture\n\n```{r}\nggplot(data = popular2data,\n       aes(x = extrav, \n           y = popular, \n           col = as.factor(texp)))+\n  viridis::scale_color_viridis(discrete = TRUE)+\n  geom_point(size     = .7,\n             alpha    = .8, \n             position = \"jitter\")+\n  geom_smooth(method = lm,\n              se     = FALSE,\n              size   = 1,\n              alpha  = .4)+\n  theme_minimal()+\n  labs(title    = \"Interaction btw. Experience and Extraversion\", \n       subtitle = \"The relationship changes\", \n       col      = \"Years of\\nTeacher\\nExperience\")\n```\n\n`geom_smooth()` using formula 'y ~ x'"
  },
  {
    "objectID": "posts/week-5/index.html#structural-equations-models",
    "href": "posts/week-5/index.html#structural-equations-models",
    "title": "Week 5: Hierarchical Data",
    "section": "Structural Equations Models",
    "text": "Structural Equations Models\nA few weeks ago, Jack mentioned the use of principal components as a means for combining collinear variables. There is a more general language for describing models of this sort. The following example will play off of work I am currently finishing up with Elliot Maltz and a co-author.\nFirst, the data.\n\n```{r}\nlibrary(lavaan)\n```\n\nThis is lavaan 0.6-12\nlavaan is FREE software! Please report any bugs.\n\n```{r}\nload(url(\"https://github.com/robertwwalker/ChoiceAndForecasting/raw/main/posts/week-5/data/EMData.RData\"))\n```\n\nThere is a ton of data in here. Let me pay particular attention to specific parts we are interested in.\n\nAgentic\n\n```{r}\nnames(EMData)[[76]]\ntable(EMData.Anonymous$...76)\nnames(EMData)[[77]]\ntable(EMData.Anonymous$...77)\nnames(EMData)[[78]]\ntable(EMData.Anonymous$...78)\nnames(EMData)[[79]]\ntable(EMData.Anonymous$...79)\nAB <- cfa('Agentic =~ ...76 + ...77 + ...78 + ...79', data=EMData.Anonymous, ordered = TRUE)\n```\n\nWarning in lav_model_vcov(lavmodel = lavmodel, lavsamplestats = lavsamplestats, : lavaan WARNING:\n    The variance-covariance matrix of the estimated parameters (vcov)\n    does not appear to be positive definite! The smallest eigenvalue\n    (= 6.642114e-18) is close to zero. This may be a symptom that the\n    model is not identified.\n\n```{r}\nsummary(AB, fit.measures = TRUE, standardized = TRUE)\n```\n\n[1] \"28. You lack career guidance and support [People in the community expect you to be a leader]\"\n\n 1  2  3  4  5  6  7 \n22 18 22 21 12 15  5 \n[1] \"28. You lack career guidance and support [Your community encourages you to achieve individual success]\"\n\n 1  2  3  4  5  6  7 \n17 19 26 23 18 10  2 \n[1] \"28. You lack career guidance and support [You are expected to be assertive in your interactions with others]\"\n\n 1  2  3  4  5  6  7 \n13 27 27 31 11  4  2 \n[1] \"28. You lack career guidance and support [You are expected to have strong opinions]\"\n\n 1  2  3  4  5  6  7 \n11 26 19 31 23  3  2 \nlavaan 0.6-12 ended normally after 15 iterations\n\n  Estimator                                       DWLS\n  Optimization method                           NLMINB\n  Number of model parameters                        28\n\n  Number of observations                           115\n\nModel Test User Model:\n                                              Standard      Robust\n  Test Statistic                                 7.422      21.832\n  Degrees of freedom                                 2           2\n  P-value (Chi-square)                           0.024       0.000\n  Scaling correction factor                                  0.341\n  Shift parameter                                            0.087\n    simple second-order correction                                \n\nModel Test Baseline Model:\n\n  Test statistic                              2486.114    1657.812\n  Degrees of freedom                                 6           6\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.501\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.998       0.988\n  Tucker-Lewis Index (TLI)                       0.993       0.964\n                                                                  \n  Robust Comparative Fit Index (CFI)                            NA\n  Robust Tucker-Lewis Index (TLI)                               NA\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.154       0.295\n  90 Percent confidence interval - lower         0.047       0.191\n  90 Percent confidence interval - upper         0.280       0.412\n  P-value RMSEA <= 0.05                          0.053       0.000\n                                                                  \n  Robust RMSEA                                                  NA\n  90 Percent confidence interval - lower                        NA\n  90 Percent confidence interval - upper                        NA\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.033       0.033\n\nParameter Estimates:\n\n  Standard errors                           Robust.sem\n  Information                                 Expected\n  Information saturated (h1) model        Unstructured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  Agentic =~                                                            \n    ...76             1.000                               0.853    0.853\n    ...77             1.056    0.051   20.773    0.000    0.901    0.901\n    ...78             1.045    0.042   25.093    0.000    0.891    0.891\n    ...79             0.962    0.044   22.041    0.000    0.820    0.820\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   ....76             0.000                               0.000    0.000\n   ....77             0.000                               0.000    0.000\n   ....78             0.000                               0.000    0.000\n   ....79             0.000                               0.000    0.000\n    Agentic           0.000                               0.000    0.000\n\nThresholds:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n    ...76|t1         -0.873    0.135   -6.459    0.000   -0.873   -0.873\n    ...76|t2         -0.391    0.121   -3.241    0.001   -0.391   -0.391\n    ...76|t3          0.098    0.118    0.835    0.403    0.098    0.098\n    ...76|t4          0.588    0.125    4.702    0.000    0.588    0.588\n    ...76|t5          0.939    0.138    6.790    0.000    0.939    0.939\n    ...76|t6          1.712    0.207    8.262    0.000    1.712    1.712\n    ...77|t1         -1.046    0.144   -7.264    0.000   -1.046   -1.046\n    ...77|t2         -0.487    0.123   -3.975    0.000   -0.487   -0.487\n    ...77|t3          0.098    0.118    0.835    0.403    0.098    0.098\n    ...77|t4          0.641    0.127    5.062    0.000    0.641    0.641\n    ...77|t5          1.257    0.158    7.948    0.000    1.257    1.257\n    ...77|t6          2.111    0.285    7.411    0.000    2.111    2.111\n    ...78|t1         -1.211    0.155   -7.826    0.000   -1.211   -1.211\n    ...78|t2         -0.391    0.121   -3.241    0.001   -0.391   -0.391\n    ...78|t3          0.209    0.118    1.763    0.078    0.209    0.209\n    ...78|t4          1.046    0.144    7.264    0.000    1.046    1.046\n    ...78|t5          1.624    0.195    8.320    0.000    1.624    1.624\n    ...78|t6          2.111    0.285    7.411    0.000    2.111    2.111\n    ...79|t1         -1.307    0.162   -8.058    0.000   -1.307   -1.307\n    ...79|t2         -0.463    0.122   -3.792    0.000   -0.463   -0.463\n    ...79|t3         -0.033    0.117   -0.279    0.781   -0.033   -0.033\n    ...79|t4          0.695    0.128    5.418    0.000    0.695    0.695\n    ...79|t5          1.712    0.207    8.262    0.000    1.712    1.712\n    ...79|t6          2.111    0.285    7.411    0.000    2.111    2.111\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   ....76             0.273                               0.273    0.273\n   ....77             0.188                               0.188    0.188\n   ....78             0.207                               0.207    0.207\n   ....79             0.327                               0.327    0.327\n    Agentic           0.727    0.051   14.358    0.000    1.000    1.000\n\nScales y*:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n    ...76             1.000                               1.000    1.000\n    ...77             1.000                               1.000    1.000\n    ...78             1.000                               1.000    1.000\n    ...79             1.000                               1.000    1.000\n\n\n\n\nCommunal\n\n```{r}\nnames(EMData)[[80]]\ntable(EMData.Anonymous$...80)\nnames(EMData)[[81]]\ntable(EMData.Anonymous$...81)\nnames(EMData)[[84]]\ntable(EMData.Anonymous$...84)\nCB <- cfa('Communal =~ ...80 + ...81 + ...84', data=EMData.Anonymous, ordered = TRUE)\nsummary(CB, fit.measures = TRUE, standardized = TRUE)\n```\n\n[1] \"28. You lack career guidance and support [You are expected to be unselfish]\"\n\n 1  2  3  4  5  6  7 \n 5 13 24 25 25 19  4 \n[1] \"28. You lack career guidance and support [In your interactions with others you are expected to consider others opinions over your own]\"\n\n 1  2  3  4  5  6  7 \n 9 21 32 28 20  4  1 \n[1] \"28. You lack career guidance and support [Your community expects you to put others interests ahead of your own]\"\n\n 1  2  3  4  5  6  7 \n12 21 24 25 20 11  2 \nlavaan 0.6-12 ended normally after 13 iterations\n\n  Estimator                                       DWLS\n  Optimization method                           NLMINB\n  Number of model parameters                        21\n\n  Number of observations                           115\n\nModel Test User Model:\n                                              Standard      Robust\n  Test Statistic                                 0.000       0.000\n  Degrees of freedom                                 0           0\n\nModel Test Baseline Model:\n\n  Test statistic                               511.919     427.939\n  Degrees of freedom                                 3           3\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.198\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000       1.000\n  Tucker-Lewis Index (TLI)                       1.000       1.000\n                                                                  \n  Robust Comparative Fit Index (CFI)                            NA\n  Robust Tucker-Lewis Index (TLI)                               NA\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000       0.000\n  90 Percent confidence interval - lower         0.000       0.000\n  90 Percent confidence interval - upper         0.000       0.000\n  P-value RMSEA <= 0.05                             NA          NA\n                                                                  \n  Robust RMSEA                                                  NA\n  90 Percent confidence interval - lower                     0.000\n  90 Percent confidence interval - upper                     0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.000       0.000\n\nParameter Estimates:\n\n  Standard errors                           Robust.sem\n  Information                                 Expected\n  Information saturated (h1) model        Unstructured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  Communal =~                                                           \n    ...80             1.000                               0.823    0.823\n    ...81             0.908    0.078   11.572    0.000    0.747    0.747\n    ...84             0.983    0.083   11.823    0.000    0.808    0.808\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   ....80             0.000                               0.000    0.000\n   ....81             0.000                               0.000    0.000\n   ....84             0.000                               0.000    0.000\n    Communal          0.000                               0.000    0.000\n\nThresholds:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n    ...80|t1         -1.712    0.207   -8.262    0.000   -1.712   -1.712\n    ...80|t2         -1.009    0.142   -7.110    0.000   -1.009   -1.009\n    ...80|t3         -0.345    0.120   -2.872    0.004   -0.345   -0.345\n    ...80|t4          0.209    0.118    1.763    0.078    0.209    0.209\n    ...80|t5          0.842    0.134    6.289    0.000    0.842    0.842\n    ...80|t6          1.815    0.223    8.129    0.000    1.815    1.815\n    ...81|t1         -1.417    0.172   -8.235    0.000   -1.417   -1.417\n    ...81|t2         -0.641    0.127   -5.062    0.000   -0.641   -0.641\n    ...81|t3          0.098    0.118    0.835    0.403    0.098    0.098\n    ...81|t4          0.781    0.131    5.945    0.000    0.781    0.781\n    ...81|t5          1.712    0.207    8.262    0.000    1.712    1.712\n    ...81|t6          2.378    0.369    6.451    0.000    2.378    2.378\n    ...84|t1         -1.257    0.158   -7.948    0.000   -1.257   -1.257\n    ...84|t2         -0.562    0.124   -4.521    0.000   -0.562   -0.562\n    ...84|t3         -0.011    0.117   -0.093    0.926   -0.011   -0.011\n    ...84|t4          0.562    0.124    4.521    0.000    0.562    0.562\n    ...84|t5          1.211    0.155    7.826    0.000    1.211    1.211\n    ...84|t6          2.111    0.285    7.411    0.000    2.111    2.111\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   ....80             0.323                               0.323    0.323\n   ....81             0.443                               0.443    0.443\n   ....84             0.347                               0.347    0.347\n    Communal          0.677    0.072    9.442    0.000    1.000    1.000\n\nScales y*:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n    ...80             1.000                               1.000    1.000\n    ...81             1.000                               1.000    1.000\n    ...84             1.000                               1.000    1.000\n\n\n\n\nMentoring\n\n```{r}\nnames(EMData)[[13]]\ntable(EMData.Anonymous$...13)\nnames(EMData)[[14]]\ntable(EMData.Anonymous$...14)\nnames(EMData)[[15]]\ntable(EMData.Anonymous$...15)\nM <- cfa('Mentoring =~ ...13 + ...14 + ...15', data=EMData.Anonymous, ordered = TRUE)\nsummary(M, fit.measures = TRUE, standardized = TRUE)\n```\n\n[1] \"5. On a scale of 7 to 1, with 7 being very regularly and 1 being very rarely, how regularly do you:  [Encourage entrepreneurs to start businesses]\"\n\n 1  2  3  4  5  6  7 \n21 27 12 14 10 26  5 \n[1] \"5. On a scale of 7 to 1, with 7 being very regularly and 1 being very rarely, how regularly do you:  [Reassure other entrepreneurs when things are not going well]\"\n\n 1  2  3  4  5  6  7 \n22 21 10 19 33  4  6 \n[1] \"5. On a scale of 7 to 1, with 7 being very regularly and 1 being very rarely, how regularly do you:  [Help other entrepreneurs have confidence they can succeed]\"\n\n 1  2  3  4  5  6  7 \n21 29 27 11 21  5  1 \nlavaan 0.6-12 ended normally after 10 iterations\n\n  Estimator                                       DWLS\n  Optimization method                           NLMINB\n  Number of model parameters                        21\n\n  Number of observations                           115\n\nModel Test User Model:\n                                              Standard      Robust\n  Test Statistic                                 0.000       0.000\n  Degrees of freedom                                 0           0\n\nModel Test Baseline Model:\n\n  Test statistic                              1726.590    1624.559\n  Degrees of freedom                                 3           3\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.063\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000       1.000\n  Tucker-Lewis Index (TLI)                       1.000       1.000\n                                                                  \n  Robust Comparative Fit Index (CFI)                            NA\n  Robust Tucker-Lewis Index (TLI)                               NA\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000       0.000\n  90 Percent confidence interval - lower         0.000       0.000\n  90 Percent confidence interval - upper         0.000       0.000\n  P-value RMSEA <= 0.05                             NA          NA\n                                                                  \n  Robust RMSEA                                                  NA\n  90 Percent confidence interval - lower                     0.000\n  90 Percent confidence interval - upper                     0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.000       0.000\n\nParameter Estimates:\n\n  Standard errors                           Robust.sem\n  Information                                 Expected\n  Information saturated (h1) model        Unstructured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  Mentoring =~                                                          \n    ...13             1.000                               0.981    0.981\n    ...14             0.879    0.058   15.193    0.000    0.862    0.862\n    ...15             0.844    0.056   15.063    0.000    0.827    0.827\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   ....13             0.000                               0.000    0.000\n   ....14             0.000                               0.000    0.000\n   ....15             0.000                               0.000    0.000\n    Mentoring         0.000                               0.000    0.000\n\nThresholds:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n    ...13|t1         -0.905    0.137   -6.626    0.000   -0.905   -0.905\n    ...13|t2         -0.209    0.118   -1.763    0.078   -0.209   -0.209\n    ...13|t3          0.055    0.117    0.464    0.643    0.055    0.055\n    ...13|t4          0.368    0.120    3.057    0.002    0.368    0.368\n    ...13|t5          0.614    0.126    4.882    0.000    0.614    0.614\n    ...13|t6          1.712    0.207    8.262    0.000    1.712    1.712\n    ...14|t1         -0.873    0.135   -6.459    0.000   -0.873   -0.873\n    ...14|t2         -0.322    0.120   -2.688    0.007   -0.322   -0.322\n    ...14|t3         -0.098    0.118   -0.835    0.403   -0.098   -0.098\n    ...14|t4          0.322    0.120    2.688    0.007    0.322    0.322\n    ...14|t5          1.360    0.167    8.155    0.000    1.360    1.360\n    ...14|t6          1.624    0.195    8.320    0.000    1.624    1.624\n    ...15|t1         -0.905    0.137   -6.626    0.000   -0.905   -0.905\n    ...15|t2         -0.164    0.118   -1.392    0.164   -0.164   -0.164\n    ...15|t3          0.439    0.122    3.608    0.000    0.439    0.439\n    ...15|t4          0.723    0.129    5.595    0.000    0.723    0.723\n    ...15|t5          1.624    0.195    8.320    0.000    1.624    1.624\n    ...15|t6          2.378    0.369    6.451    0.000    2.378    2.378\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   ....13             0.039                               0.039    0.039\n   ....14             0.258                               0.258    0.258\n   ....15             0.315                               0.315    0.315\n    Mentoring         0.961    0.077   12.508    0.000    1.000    1.000\n\nScales y*:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n    ...13             1.000                               1.000    1.000\n    ...14             1.000                               1.000    1.000\n    ...15             1.000                               1.000    1.000\n\n\n\n\nSocial Influence\n\n```{r}\nnames(EMData)[[37]]\ntable(EMData.Anonymous$...37)\nnames(EMData)[[38]]\ntable(EMData.Anonymous$...38)\nnames(EMData)[[39]]\ntable(EMData.Anonymous$...39)\nSI <- cfa('Social.Influence =~ ...37 + ...38 + ...39', data=EMData.Anonymous, ordered = TRUE)\nsummary(SI, fit.measures = TRUE, standardized = TRUE)\n```\n\n[1] \"11. On a scale of 7 to 1, with 7 being strongly agree and 1 being strongly disagree, please indicate your level of agreement with the following items relating to your community:  [The respect you have in the community helps your business]\"\n\n 1  2  3  4  5  6 \n26 38 31  5  8  7 \n[1] \"11. On a scale of 7 to 1, with 7 being strongly agree and 1 being strongly disagree, please indicate your level of agreement with the following items relating to your community:  [Your understanding of the community helps your business]\"\n\n 1  2  3  4  5  6 \n27 40 28  6  7  7 \n[1] \"11. On a scale of 7 to 1, with 7 being strongly agree and 1 being strongly disagree, please indicate your level of agreement with the following items relating to your community:  [Your influence in the community helps your business]\"\n\n 1  2  3  4  5  6  7 \n30 38 30  4  7  5  1 \nlavaan 0.6-12 ended normally after 12 iterations\n\n  Estimator                                       DWLS\n  Optimization method                           NLMINB\n  Number of model parameters                        19\n\n  Number of observations                           115\n\nModel Test User Model:\n                                              Standard      Robust\n  Test Statistic                                 0.000       0.000\n  Degrees of freedom                                 0           0\n\nModel Test Baseline Model:\n\n  Test statistic                              3636.699    2922.199\n  Degrees of freedom                                 3           3\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.245\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000       1.000\n  Tucker-Lewis Index (TLI)                       1.000       1.000\n                                                                  \n  Robust Comparative Fit Index (CFI)                            NA\n  Robust Tucker-Lewis Index (TLI)                               NA\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000       0.000\n  90 Percent confidence interval - lower         0.000       0.000\n  90 Percent confidence interval - upper         0.000       0.000\n  P-value RMSEA <= 0.05                             NA          NA\n                                                                  \n  Robust RMSEA                                                  NA\n  90 Percent confidence interval - lower                     0.000\n  90 Percent confidence interval - upper                     0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.000       0.000\n\nParameter Estimates:\n\n  Standard errors                           Robust.sem\n  Information                                 Expected\n  Information saturated (h1) model        Unstructured\n\nLatent Variables:\n                      Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  Social.Influence =~                                                      \n    ...37                1.000                               0.928    0.928\n    ...38                1.055    0.031   33.595    0.000    0.979    0.979\n    ...39                0.942    0.029   32.441    0.000    0.874    0.874\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   ....37             0.000                               0.000    0.000\n   ....38             0.000                               0.000    0.000\n   ....39             0.000                               0.000    0.000\n    Social.Influnc    0.000                               0.000    0.000\n\nThresholds:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n    ...37|t1         -0.752    0.130   -5.771    0.000   -0.752   -0.752\n    ...37|t2          0.142    0.118    1.207    0.228    0.142    0.142\n    ...37|t3          0.939    0.138    6.790    0.000    0.939    0.939\n    ...37|t4          1.124    0.149    7.558    0.000    1.124    1.124\n    ...37|t5          1.548    0.186    8.325    0.000    1.548    1.548\n    ...38|t1         -0.723    0.129   -5.595    0.000   -0.723   -0.723\n    ...38|t2          0.209    0.118    1.763    0.078    0.209    0.209\n    ...38|t3          0.939    0.138    6.790    0.000    0.939    0.939\n    ...38|t4          1.166    0.152    7.696    0.000    1.166    1.166\n    ...38|t5          1.548    0.186    8.325    0.000    1.548    1.548\n    ...39|t1         -0.641    0.127   -5.062    0.000   -0.641   -0.641\n    ...39|t2          0.231    0.119    1.948    0.051    0.231    0.231\n    ...39|t3          1.046    0.144    7.264    0.000    1.046    1.046\n    ...39|t4          1.211    0.155    7.826    0.000    1.211    1.211\n    ...39|t5          1.624    0.195    8.320    0.000    1.624    1.624\n    ...39|t6          2.378    0.369    6.451    0.000    2.378    2.378\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   ....37             0.139                               0.139    0.139\n   ....38             0.041                               0.041    0.041\n   ....39             0.236                               0.236    0.236\n    Social.Influnc    0.861    0.034   25.088    0.000    1.000    1.000\n\nScales y*:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n    ...37             1.000                               1.000    1.000\n    ...38             1.000                               1.000    1.000\n    ...39             1.000                               1.000    1.000\n\n\n\n```{r}\nlibrary(lavaanPlot)\nlavaanPlot(model = SI, node_options = list(shape = \"box\", fontname = \"Helvetica\"), edge_options = list(color = \"grey\"), coefs = TRUE, covs = TRUE)\n```\n\n\n\n\n\n\n\nAn SEM\nNow let me combine those measurement models to produce a set of two structural equations. I wish to explain income and employment given these factors.\n\n```{r, warning=FALSE, message=FALSE}\nnames(EMData)[c(5,59)]\nStruct <- sem('Agentic =~ ...76 + ...77 + ...78 + ...79\n          Communal =~ ...80 + ...81 + ...84\n          Mentoring =~ ...13 + ...14 + ...15\n          Social.Influence =~ ...37 + ...38 + ...39\n          ...59 ~ Agentic + Communal + Mentoring + Social.Influence\n          ...5 ~ Agentic + Communal + Mentoring + Social.Influence', data=EMData.Anonymous, ordered = c(\"...13\",\"...14\", \"...15\", \"...80\",\"...81\", \"...84\", \"...76\",\"...77\", \"...78\", \"...79\",\"...37\", \"...38\", \"...39\"))\nsummary(Struct, fit.measures=TRUE, standardized=TRUE)\n```\n\n[1] \"3. What is the current income generated from the business? (USD PM)\"\n[2] \"20. How many people do you manage in your business?\"                \nlavaan 0.6-12 ended normally after 112 iterations\n\n  Estimator                                       DWLS\n  Optimization method                           NLMINB\n  Number of model parameters                       108\n\n  Number of observations                           115\n\nModel Test User Model:\n                                              Standard      Robust\n  Test Statistic                               176.253     255.843\n  Degrees of freedom                                77          77\n  P-value (Chi-square)                           0.000       0.000\n  Scaling correction factor                                  0.804\n  Shift parameter                                           36.682\n    simple second-order correction                                \n\nModel Test Baseline Model:\n\n  Test statistic                              9489.339    3694.019\n  Degrees of freedom                               105         105\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  2.615\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.989       0.950\n  Tucker-Lewis Index (TLI)                       0.986       0.932\n                                                                  \n  Robust Comparative Fit Index (CFI)                            NA\n  Robust Tucker-Lewis Index (TLI)                               NA\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.106       0.143\n  90 Percent confidence interval - lower         0.086       0.124\n  90 Percent confidence interval - upper         0.127       0.162\n  P-value RMSEA <= 0.05                          0.000       0.000\n                                                                  \n  Robust RMSEA                                                  NA\n  90 Percent confidence interval - lower                        NA\n  90 Percent confidence interval - upper                        NA\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.075       0.075\n\nParameter Estimates:\n\n  Standard errors                           Robust.sem\n  Information                                 Expected\n  Information saturated (h1) model        Unstructured\n\nLatent Variables:\n                      Estimate   Std.Err  z-value  P(>|z|)   Std.lv   Std.all\n  Agentic =~                                                                 \n    ...76                 1.000                                0.812    0.812\n    ...77                 1.110    0.051   21.573    0.000     0.901    0.901\n    ...78                 1.076    0.044   24.545    0.000     0.873    0.873\n    ...79                 1.097    0.047   23.283    0.000     0.890    0.890\n  Communal =~                                                                \n    ...80                 1.000                                0.715    0.715\n    ...81                 1.233    0.126    9.803    0.000     0.881    0.881\n    ...84                 1.103    0.091   12.190    0.000     0.788    0.788\n  Mentoring =~                                                               \n    ...13                 1.000                                0.961    0.961\n    ...14                 0.908    0.057   15.897    0.000     0.872    0.872\n    ...15                 0.876    0.054   16.228    0.000     0.842    0.842\n  Social.Influence =~                                                        \n    ...37                 1.000                                0.948    0.948\n    ...38                 1.004    0.030   33.882    0.000     0.952    0.952\n    ...39                 0.941    0.029   32.029    0.000     0.892    0.892\n\nRegressions:\n                   Estimate   Std.Err  z-value  P(>|z|)   Std.lv   Std.all\n  ...59 ~                                                                 \n    Agentic           -0.087    0.063   -1.386    0.166    -0.071   -0.052\n    Communal           0.119    0.108    1.100    0.271     0.085    0.062\n    Mentoring          0.166    0.075    2.213    0.027     0.159    0.116\n    Social.Influnc    -0.422    0.055   -7.620    0.000    -0.400   -0.292\n  ...5 ~                                                                  \n    Agentic           51.893   19.705    2.633    0.008    42.118    0.205\n    Communal          -9.427   20.599   -0.458    0.647    -6.736   -0.033\n    Mentoring          1.218   15.046    0.081    0.935     1.170    0.006\n    Social.Influnc    35.014   16.427    2.131    0.033    33.195    0.161\n\nCovariances:\n                   Estimate   Std.Err  z-value  P(>|z|)   Std.lv   Std.all\n  Agentic ~~                                                              \n    Communal           0.259    0.044    5.821    0.000     0.447    0.447\n    Mentoring          0.203    0.062    3.269    0.001     0.261    0.261\n    Social.Influnc     0.247    0.056    4.423    0.000     0.321    0.321\n  Communal ~~                                                             \n    Mentoring          0.134    0.057    2.329    0.020     0.195    0.195\n    Social.Influnc     0.308    0.062    4.963    0.000     0.454    0.454\n  Mentoring ~~                                                            \n    Social.Influnc     0.067    0.074    0.904    0.366     0.074    0.074\n ....59 ~~                                                                \n   ....5              78.509   19.599    4.006    0.000    78.509    0.304\n\nIntercepts:\n                   Estimate   Std.Err  z-value  P(>|z|)   Std.lv   Std.all\n   ....76              0.000                                0.000    0.000\n   ....77              0.000                                0.000    0.000\n   ....78              0.000                                0.000    0.000\n   ....79              0.000                                0.000    0.000\n   ....80              0.000                                0.000    0.000\n   ....81              0.000                                0.000    0.000\n   ....84              0.000                                0.000    0.000\n   ....13              0.000                                0.000    0.000\n   ....14              0.000                                0.000    0.000\n   ....15              0.000                                0.000    0.000\n   ....37              0.000                                0.000    0.000\n   ....38              0.000                                0.000    0.000\n   ....39              0.000                                0.000    0.000\n   ....59              0.435    0.305    1.427    0.154     0.435    0.317\n   ....5             380.609   23.778   16.007    0.000   380.609    1.849\n    Agentic            0.000                                0.000    0.000\n    Communal           0.000                                0.000    0.000\n    Mentoring          0.000                                0.000    0.000\n    Social.Influnc     0.000                                0.000    0.000\n\nThresholds:\n                   Estimate   Std.Err  z-value  P(>|z|)   Std.lv   Std.all\n    ...76|t1          -0.873    0.135   -6.459    0.000    -0.873   -0.873\n    ...76|t2          -0.391    0.121   -3.241    0.001    -0.391   -0.391\n    ...76|t3           0.098    0.118    0.835    0.403     0.098    0.098\n    ...76|t4           0.588    0.125    4.702    0.000     0.588    0.588\n    ...76|t5           0.939    0.138    6.790    0.000     0.939    0.939\n    ...76|t6           1.712    0.207    8.262    0.000     1.712    1.712\n    ...77|t1          -1.046    0.144   -7.264    0.000    -1.046   -1.046\n    ...77|t2          -0.487    0.123   -3.975    0.000    -0.487   -0.487\n    ...77|t3           0.098    0.118    0.835    0.403     0.098    0.098\n    ...77|t4           0.641    0.127    5.062    0.000     0.641    0.641\n    ...77|t5           1.257    0.158    7.948    0.000     1.257    1.257\n    ...77|t6           2.111    0.285    7.411    0.000     2.111    2.111\n    ...78|t1          -1.211    0.155   -7.826    0.000    -1.211   -1.211\n    ...78|t2          -0.391    0.121   -3.241    0.001    -0.391   -0.391\n    ...78|t3           0.209    0.118    1.763    0.078     0.209    0.209\n    ...78|t4           1.046    0.144    7.264    0.000     1.046    1.046\n    ...78|t5           1.624    0.195    8.320    0.000     1.624    1.624\n    ...78|t6           2.111    0.285    7.411    0.000     2.111    2.111\n    ...79|t1          -1.307    0.162   -8.058    0.000    -1.307   -1.307\n    ...79|t2          -0.463    0.122   -3.792    0.000    -0.463   -0.463\n    ...79|t3          -0.033    0.117   -0.279    0.781    -0.033   -0.033\n    ...79|t4           0.695    0.128    5.418    0.000     0.695    0.695\n    ...79|t5           1.712    0.207    8.262    0.000     1.712    1.712\n    ...79|t6           2.111    0.285    7.411    0.000     2.111    2.111\n    ...80|t1          -1.712    0.207   -8.262    0.000    -1.712   -1.712\n    ...80|t2          -1.009    0.142   -7.110    0.000    -1.009   -1.009\n    ...80|t3          -0.345    0.120   -2.872    0.004    -0.345   -0.345\n    ...80|t4           0.209    0.118    1.763    0.078     0.209    0.209\n    ...80|t5           0.842    0.134    6.289    0.000     0.842    0.842\n    ...80|t6           1.815    0.223    8.129    0.000     1.815    1.815\n    ...81|t1          -1.417    0.172   -8.235    0.000    -1.417   -1.417\n    ...81|t2          -0.641    0.127   -5.062    0.000    -0.641   -0.641\n    ...81|t3           0.098    0.118    0.835    0.403     0.098    0.098\n    ...81|t4           0.781    0.131    5.945    0.000     0.781    0.781\n    ...81|t5           1.712    0.207    8.262    0.000     1.712    1.712\n    ...81|t6           2.378    0.369    6.451    0.000     2.378    2.378\n    ...84|t1          -1.257    0.158   -7.948    0.000    -1.257   -1.257\n    ...84|t2          -0.562    0.124   -4.521    0.000    -0.562   -0.562\n    ...84|t3          -0.011    0.117   -0.093    0.926    -0.011   -0.011\n    ...84|t4           0.562    0.124    4.521    0.000     0.562    0.562\n    ...84|t5           1.211    0.155    7.826    0.000     1.211    1.211\n    ...84|t6           2.111    0.285    7.411    0.000     2.111    2.111\n    ...13|t1          -0.905    0.137   -6.626    0.000    -0.905   -0.905\n    ...13|t2          -0.209    0.118   -1.763    0.078    -0.209   -0.209\n    ...13|t3           0.055    0.117    0.464    0.643     0.055    0.055\n    ...13|t4           0.368    0.120    3.057    0.002     0.368    0.368\n    ...13|t5           0.614    0.126    4.882    0.000     0.614    0.614\n    ...13|t6           1.712    0.207    8.262    0.000     1.712    1.712\n    ...14|t1          -0.873    0.135   -6.459    0.000    -0.873   -0.873\n    ...14|t2          -0.322    0.120   -2.688    0.007    -0.322   -0.322\n    ...14|t3          -0.098    0.118   -0.835    0.403    -0.098   -0.098\n    ...14|t4           0.322    0.120    2.688    0.007     0.322    0.322\n    ...14|t5           1.360    0.167    8.155    0.000     1.360    1.360\n    ...14|t6           1.624    0.195    8.320    0.000     1.624    1.624\n    ...15|t1          -0.905    0.137   -6.626    0.000    -0.905   -0.905\n    ...15|t2          -0.164    0.118   -1.392    0.164    -0.164   -0.164\n    ...15|t3           0.439    0.122    3.608    0.000     0.439    0.439\n    ...15|t4           0.723    0.129    5.595    0.000     0.723    0.723\n    ...15|t5           1.624    0.195    8.320    0.000     1.624    1.624\n    ...15|t6           2.378    0.369    6.451    0.000     2.378    2.378\n    ...37|t1          -0.752    0.130   -5.771    0.000    -0.752   -0.752\n    ...37|t2           0.142    0.118    1.207    0.228     0.142    0.142\n    ...37|t3           0.939    0.138    6.790    0.000     0.939    0.939\n    ...37|t4           1.124    0.149    7.558    0.000     1.124    1.124\n    ...37|t5           1.548    0.186    8.325    0.000     1.548    1.548\n    ...38|t1          -0.723    0.129   -5.595    0.000    -0.723   -0.723\n    ...38|t2           0.209    0.118    1.763    0.078     0.209    0.209\n    ...38|t3           0.939    0.138    6.790    0.000     0.939    0.939\n    ...38|t4           1.166    0.152    7.696    0.000     1.166    1.166\n    ...38|t5           1.548    0.186    8.325    0.000     1.548    1.548\n    ...39|t1          -0.641    0.127   -5.062    0.000    -0.641   -0.641\n    ...39|t2           0.231    0.119    1.948    0.051     0.231    0.231\n    ...39|t3           1.046    0.144    7.264    0.000     1.046    1.046\n    ...39|t4           1.211    0.155    7.826    0.000     1.211    1.211\n    ...39|t5           1.624    0.195    8.320    0.000     1.624    1.624\n    ...39|t6           2.378    0.369    6.451    0.000     2.378    2.378\n\nVariances:\n                   Estimate   Std.Err  z-value  P(>|z|)   Std.lv   Std.all\n   ....76              0.341                                0.341    0.341\n   ....77              0.188                                0.188    0.188\n   ....78              0.238                                0.238    0.238\n   ....79              0.207                                0.207    0.207\n   ....80              0.489                                0.489    0.489\n   ....81              0.224                                0.224    0.224\n   ....84              0.378                                0.378    0.378\n   ....13              0.077                                0.077    0.077\n   ....14              0.239                                0.239    0.239\n   ....15              0.292                                0.292    0.292\n   ....37              0.101                                0.101    0.101\n   ....38              0.094                                0.094    0.094\n   ....39              0.204                                0.204    0.204\n   ....59              1.711    0.147   11.666    0.000     1.711    0.910\n   ....5           38984.352 4516.263    8.632    0.000 38984.352    0.920\n    Agentic            0.659    0.048   13.761    0.000     1.000    1.000\n    Communal           0.511    0.068    7.492    0.000     1.000    1.000\n    Mentoring          0.923    0.072   12.824    0.000     1.000    1.000\n    Social.Influnc     0.899    0.033   27.070    0.000     1.000    1.000\n\nScales y*:\n                   Estimate   Std.Err  z-value  P(>|z|)   Std.lv   Std.all\n    ...76              1.000                                1.000    1.000\n    ...77              1.000                                1.000    1.000\n    ...78              1.000                                1.000    1.000\n    ...79              1.000                                1.000    1.000\n    ...80              1.000                                1.000    1.000\n    ...81              1.000                                1.000    1.000\n    ...84              1.000                                1.000    1.000\n    ...13              1.000                                1.000    1.000\n    ...14              1.000                                1.000    1.000\n    ...15              1.000                                1.000    1.000\n    ...37              1.000                                1.000    1.000\n    ...38              1.000                                1.000    1.000\n    ...39              1.000                                1.000    1.000\n\n\n\n```{r}\nlavaanPlot(model=Struct, node_options = list(shape = \"box\", fontname = \"Helvetica\"), edge_options = list(color = \"grey\"), coefs = TRUE, covs = TRUE)\n```"
  },
  {
    "objectID": "posts/week-6/index.html",
    "href": "posts/week-6/index.html",
    "title": "Week 6: Measurement and Survival",
    "section": "",
    "text": "The slides are here..\nOur sixth class meeting will focus on Chapter 8 and Chapter 9 of Handbook of Regression Modeling in People Analytics."
  },
  {
    "objectID": "posts/week-6/index.html#the-skinny",
    "href": "posts/week-6/index.html#the-skinny",
    "title": "Week 6: Measurement and Survival",
    "section": "The Skinny",
    "text": "The Skinny\nHierarchical models represent an advance on more standard linear and generalized linear models with the recognition that data have hierarchical forms of organization with varying degrees of freedom for the predictors. These models can, generically, be combined with techniques that we have already learned to expand the range of our toolkit. Since we left last week off with ordered models, they will first occupy our attention."
  },
  {
    "objectID": "posts/week-6/index.html#hierarchical-models-and-sem",
    "href": "posts/week-6/index.html#hierarchical-models-and-sem",
    "title": "Week 6: Measurement and Survival",
    "section": "Hierarchical Models and SEM",
    "text": "Hierarchical Models and SEM\n\nHierarchical Models\nTo examine a hierarchical model, I am going to choose some interesting data on popularity. A description appears below; these data come from an Intro to Multilevel Analysis.\n\n\n\nPopularity Data\n\n\nThough the data are technically ordered, this feature is not exploited to build a hierarchical ordered regression model, though it could be done. Instead, the outcome of interest is an average of ordered scales.\n\n\nLoad the data\n\n```{r}\nlibrary(tidyverse)\n```\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.8     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.1\n✔ readr   2.1.2     ✔ forcats 0.5.2\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n```{r}\nlibrary(haven)\npopular2data <- read_sav(file =\"https://github.com/MultiLevelAnalysis/Datasets-third-edition-Multilevel-book/blob/master/chapter%202/popularity/SPSS/popular2.sav?raw=true\")\npopular2data <- popular2data %>% dplyr::select(pupil, class, extrav, sex, texp, popular)\n```\n\n\n\nA Summary\n\n```{r}\nsummary(popular2data)\nhead(popular2data)\n```\n\n     pupil           class            extrav            sex        \n Min.   : 1.00   Min.   :  1.00   Min.   : 1.000   Min.   :0.0000  \n 1st Qu.: 6.00   1st Qu.: 25.00   1st Qu.: 4.000   1st Qu.:0.0000  \n Median :11.00   Median : 51.00   Median : 5.000   Median :1.0000  \n Mean   :10.65   Mean   : 50.37   Mean   : 5.215   Mean   :0.5055  \n 3rd Qu.:16.00   3rd Qu.: 76.00   3rd Qu.: 6.000   3rd Qu.:1.0000  \n Max.   :26.00   Max.   :100.00   Max.   :10.000   Max.   :1.0000  \n      texp          popular     \n Min.   : 2.00   Min.   :0.000  \n 1st Qu.: 8.00   1st Qu.:4.100  \n Median :15.00   Median :5.100  \n Mean   :14.26   Mean   :5.076  \n 3rd Qu.:20.00   3rd Qu.:6.000  \n Max.   :25.00   Max.   :9.500  \n# A tibble: 6 × 6\n  pupil class extrav sex        texp popular\n  <dbl> <dbl>  <dbl> <dbl+lbl> <dbl>   <dbl>\n1     1     1      5 1 [girl]     24     6.3\n2     2     1      7 0 [boy]      24     4.9\n3     3     1      4 1 [girl]     24     5.3\n4     4     1      3 1 [girl]     24     4.7\n5     5     1      5 1 [girl]     24     6  \n6     6     1      4 0 [boy]      24     4.7\n\n\n\n\nA plot of the relationship of interest\n\n```{r}\nggplot(data    = popular2data,\n       aes(x   = extrav,\n           y   = popular,\n           col = class))+ #to add the colours for different classes\n  geom_point(size     = 0.8,\n             alpha    = .8,\n             position = \"jitter\")+ #to add some random noise for plotting purposes\n  theme_minimal()+\n  theme(legend.position = \"none\")+\n  scale_color_gradientn(colours = rainbow(100))+\n  labs(title    = \"Popularity vs. Extraversion\",\n       subtitle = \"add colours for different classes\",\n       x = \"Extroversion\",\n       y = \"Average Popularity\")\n```\n\n\n\n\n\n\nWith the lines\n\n```{r}\nggplot(data      = popular2data,\n       aes(x     = extrav,\n           y     = popular,\n           col   = class,\n           group = class))+ #to add the colours for different classes\n  geom_point(size     = 1.2,\n             alpha    = .8,\n             position = \"jitter\")+ #to add some random noise for plotting purposes\n  theme_minimal()+\n  theme(legend.position = \"none\")+\n  scale_color_gradientn(colours = rainbow(100))+\n  geom_smooth(method = lm,\n              se     = FALSE,\n              size   = .5, \n              alpha  = .8)+ # to add regression line\n  labs(title    = \"Popularity vs. Extraversion\",\n       subtitle = \"add colours for different classes and regression lines\",\n       x = \"Extroversion\",\n       y = \"Average Popularity\")\n```\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\n\nA regression\n\n```{r}\nggplot(data = popular2data, \n       aes(x   = extrav,\n           y   = popular, \n           col = as.factor(sex)))+\n  geom_point(size     = 1, \n             alpha    = .7, \n             position = \"jitter\")+\n  geom_smooth(method   = lm,\n              se       = T, \n              size     = 1.5, \n              linetype = 1, \n              alpha    = .7)+\n  theme_minimal()+\n  labs(title    = \"Popularity and Extraversion for 2 Genders\", \n       subtitle = \"The linear relationship between the two is similar for both genders\")+\n  scale_color_manual(name   =\" Gender\",\n                     labels = c(\"Boys\", \"Girls\"),\n                     values = c(\"lightblue\", \"pink\"))\n```\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nA model with random intercepts\n\n```{r, warning=FALSE, message=FALSE}\nlibrary(lme4)\noptions(scipen=7)\nlibrary(lmerTest)\nmodel1 <- lmer(formula = popular ~ 1 + sex + extrav + (1|class), \n               data    = popular2data)\nsummary(model1)\n```\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: popular ~ 1 + sex + extrav + (1 | class)\n   Data: popular2data\n\nREML criterion at convergence: 4948.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.2091 -0.6575 -0.0044  0.6732  2.9755 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n class    (Intercept) 0.6272   0.7919  \n Residual             0.5921   0.7695  \nNumber of obs: 2000, groups:  class, 100\n\nFixed effects:\n              Estimate Std. Error         df t value Pr(>|t|)    \n(Intercept)    2.14096    0.11729  390.76822   18.25   <2e-16 ***\nsex            1.25300    0.03743 1926.69933   33.48   <2e-16 ***\nextrav         0.44161    0.01616 1956.77498   27.33   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n       (Intr) sex   \nsex    -0.100       \nextrav -0.705 -0.085\n\n\nThough in this case, we probably do not need them but p-values can be obtained from lmerTest. The standard lme4 summary does not have them.\nNow let’s add a second-level predictor. Teacher experience does not vary within a given classroom, only across the 100 classrooms. Let’s look at this model.\n\n```{r}\nmodel2 <- lmer(popular ~ 1 + sex + extrav + texp + (1 | class), data=popular2data)\nsummary(model2)\n```\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: popular ~ 1 + sex + extrav + texp + (1 | class)\n   Data: popular2data\n\nREML criterion at convergence: 4885\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.1745 -0.6491 -0.0075  0.6705  3.0078 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n class    (Intercept) 0.2954   0.5435  \n Residual             0.5920   0.7694  \nNumber of obs: 2000, groups:  class, 100\n\nFixed effects:\n               Estimate  Std. Error          df t value  Pr(>|t|)    \n(Intercept)    0.809766    0.169993  226.431473   4.764 0.0000034 ***\nsex            1.253800    0.037290 1948.303018  33.623   < 2e-16 ***\nextrav         0.454431    0.016165 1954.889209  28.112   < 2e-16 ***\ntexp           0.088407    0.008764  101.627424  10.087   < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n       (Intr) sex    extrav\nsex    -0.040              \nextrav -0.589 -0.090       \ntexp   -0.802 -0.036  0.139\n\n\nMore experienced teachers lead to higher reported average popularity.\n\n\nRandom slopes\n\n```{r, message=FALSE, warning=FALSE}\nmodel3 <- lmer(formula = popular ~ 1 + sex + extrav + texp + (1 + sex + extrav | class),\n               data    = popular2data, control=lmerControl(optCtrl=list(maxfun=100000) ))\nsummary(model3)\n```\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: popular ~ 1 + sex + extrav + texp + (1 + sex + extrav | class)\n   Data: popular2data\nControl: lmerControl(optCtrl = list(maxfun = 100000))\n\nREML criterion at convergence: 4833.3\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.1643 -0.6554 -0.0246  0.6711  2.9570 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr       \n class    (Intercept) 1.342020 1.15846             \n          sex         0.002404 0.04903  -0.39      \n          extrav      0.034742 0.18639  -0.88 -0.09\n Residual             0.551435 0.74259             \nNumber of obs: 2000, groups:  class, 100\n\nFixed effects:\n              Estimate Std. Error         df t value Pr(>|t|)    \n(Intercept)   0.758511   0.197316 181.050469   3.844 0.000167 ***\nsex           1.250810   0.036942 986.050567  33.859  < 2e-16 ***\nextrav        0.452854   0.024645  96.208501  18.375  < 2e-16 ***\ntexp          0.089520   0.008618 101.321705  10.388  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n       (Intr) sex    extrav\nsex    -0.062              \nextrav -0.718 -0.066       \ntexp   -0.684 -0.039  0.089\noptimizer (nloptwrap) convergence code: 0 (OK)\nModel failed to converge with max|grad| = 0.00597328 (tol = 0.002, component 1)\n\n\n\n\nExamining the Model\n\n```{r}\nranova(model3)\n```\n\nboundary (singular) fit: see help('isSingular')\n\n\nANOVA-like table for random-effects: Single term deletions\n\nModel:\npopular ~ sex + extrav + texp + (1 + sex + extrav | class)\n                                     npar  logLik    AIC    LRT Df\n<none>                                 11 -2416.6 4855.3          \nsex in (1 + sex + extrav | class)       8 -2417.4 4850.8  1.513  3\nextrav in (1 + sex + extrav | class)    8 -2441.9 4899.8 50.507  3\n                                           Pr(>Chisq)    \n<none>                                                   \nsex in (1 + sex + extrav | class)              0.6792    \nextrav in (1 + sex + extrav | class) 0.00000000006232 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe random effect associated with sex is not close to significance.\n\n\nA Crossed-Effects Model\n\n```{r}\nmodel5<-lmer(formula = popular ~ 1 + sex + extrav + texp+ extrav*texp + (1 + extrav | class), \n             data    = popular2data)\nsummary(model5)\n```\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: popular ~ 1 + sex + extrav + texp + extrav * texp + (1 + extrav |  \n    class)\n   Data: popular2data\n\nREML criterion at convergence: 4780.5\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-3.12872 -0.63857 -0.01129  0.67916  3.05006 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n class    (Intercept) 0.478639 0.69184       \n          extrav      0.005409 0.07355  -0.64\n Residual             0.552769 0.74348       \nNumber of obs: 2000, groups:  class, 100\n\nFixed effects:\n               Estimate  Std. Error          df t value Pr(>|t|)    \n(Intercept)   -1.209607    0.271901  109.345831  -4.449 2.09e-05 ***\nsex            1.240698    0.036233 1941.077365  34.243  < 2e-16 ***\nextrav         0.803578    0.040117   72.070164  20.031  < 2e-16 ***\ntexp           0.226197    0.016807   98.507109  13.458  < 2e-16 ***\nextrav:texp   -0.024728    0.002555   71.986847  -9.679 1.15e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) sex    extrav texp  \nsex          0.002                     \nextrav      -0.867 -0.065              \ntexp        -0.916 -0.047  0.801       \nextrav:texp  0.773  0.033 -0.901 -0.859\n\n\n\n\nA Picture\n\n```{r}\nggplot(data = popular2data,\n       aes(x = extrav, \n           y = popular, \n           col = as.factor(texp)))+\n  viridis::scale_color_viridis(discrete = TRUE)+\n  geom_point(size     = .7,\n             alpha    = .8, \n             position = \"jitter\")+\n  geom_smooth(method = lm,\n              se     = FALSE,\n              size   = 1,\n              alpha  = .4)+\n  theme_minimal()+\n  labs(title    = \"Interaction btw. Experience and Extraversion\", \n       subtitle = \"The relationship changes\", \n       col      = \"Years of\\nTeacher\\nExperience\")\n```\n\n`geom_smooth()` using formula 'y ~ x'"
  },
  {
    "objectID": "posts/week-6/index.html#structural-equations-models",
    "href": "posts/week-6/index.html#structural-equations-models",
    "title": "Week 6: Measurement and Survival",
    "section": "Structural Equations Models",
    "text": "Structural Equations Models\nA few weeks ago, Jack mentioned the use of principal components as a means for combining collinear variables. There is a more general language for describing models of this sort. The following example will play off of work I am currently finishing up with Elliot Maltz and a co-author.\nFirst, the data.\n\n```{r}\nlibrary(lavaan)\n```\n\nThis is lavaan 0.6-12\nlavaan is FREE software! Please report any bugs.\n\n```{r}\nload(url(\"https://github.com/robertwwalker/ChoiceAndForecasting/raw/main/posts/week-5/data/EMData.RData\"))\n```\n\nThere is a ton of data in here. Let me pay particular attention to specific parts we are interested in.\n\nAgentic\n\n```{r}\nnames(EMData)[[76]]\ntable(EMData.Anonymous$...76)\nnames(EMData)[[77]]\ntable(EMData.Anonymous$...77)\nnames(EMData)[[78]]\ntable(EMData.Anonymous$...78)\nnames(EMData)[[79]]\ntable(EMData.Anonymous$...79)\nAB <- cfa('Agentic =~ ...76 + ...77 + ...78 + ...79', data=EMData.Anonymous, ordered = TRUE)\n```\n\nWarning in lav_model_vcov(lavmodel = lavmodel, lavsamplestats = lavsamplestats, : lavaan WARNING:\n    The variance-covariance matrix of the estimated parameters (vcov)\n    does not appear to be positive definite! The smallest eigenvalue\n    (= 6.642114e-18) is close to zero. This may be a symptom that the\n    model is not identified.\n\n```{r}\nsummary(AB, fit.measures = TRUE, standardized = TRUE)\n```\n\n[1] \"28. You lack career guidance and support [People in the community expect you to be a leader]\"\n\n 1  2  3  4  5  6  7 \n22 18 22 21 12 15  5 \n[1] \"28. You lack career guidance and support [Your community encourages you to achieve individual success]\"\n\n 1  2  3  4  5  6  7 \n17 19 26 23 18 10  2 \n[1] \"28. You lack career guidance and support [You are expected to be assertive in your interactions with others]\"\n\n 1  2  3  4  5  6  7 \n13 27 27 31 11  4  2 \n[1] \"28. You lack career guidance and support [You are expected to have strong opinions]\"\n\n 1  2  3  4  5  6  7 \n11 26 19 31 23  3  2 \nlavaan 0.6-12 ended normally after 15 iterations\n\n  Estimator                                       DWLS\n  Optimization method                           NLMINB\n  Number of model parameters                        28\n\n  Number of observations                           115\n\nModel Test User Model:\n                                              Standard      Robust\n  Test Statistic                                 7.422      21.832\n  Degrees of freedom                                 2           2\n  P-value (Chi-square)                           0.024       0.000\n  Scaling correction factor                                  0.341\n  Shift parameter                                            0.087\n    simple second-order correction                                \n\nModel Test Baseline Model:\n\n  Test statistic                              2486.114    1657.812\n  Degrees of freedom                                 6           6\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.501\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.998       0.988\n  Tucker-Lewis Index (TLI)                       0.993       0.964\n                                                                  \n  Robust Comparative Fit Index (CFI)                            NA\n  Robust Tucker-Lewis Index (TLI)                               NA\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.154       0.295\n  90 Percent confidence interval - lower         0.047       0.191\n  90 Percent confidence interval - upper         0.280       0.412\n  P-value RMSEA <= 0.05                          0.053       0.000\n                                                                  \n  Robust RMSEA                                                  NA\n  90 Percent confidence interval - lower                        NA\n  90 Percent confidence interval - upper                        NA\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.033       0.033\n\nParameter Estimates:\n\n  Standard errors                           Robust.sem\n  Information                                 Expected\n  Information saturated (h1) model        Unstructured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  Agentic =~                                                            \n    ...76             1.000                               0.853    0.853\n    ...77             1.056    0.051   20.773    0.000    0.901    0.901\n    ...78             1.045    0.042   25.093    0.000    0.891    0.891\n    ...79             0.962    0.044   22.041    0.000    0.820    0.820\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   ....76             0.000                               0.000    0.000\n   ....77             0.000                               0.000    0.000\n   ....78             0.000                               0.000    0.000\n   ....79             0.000                               0.000    0.000\n    Agentic           0.000                               0.000    0.000\n\nThresholds:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n    ...76|t1         -0.873    0.135   -6.459    0.000   -0.873   -0.873\n    ...76|t2         -0.391    0.121   -3.241    0.001   -0.391   -0.391\n    ...76|t3          0.098    0.118    0.835    0.403    0.098    0.098\n    ...76|t4          0.588    0.125    4.702    0.000    0.588    0.588\n    ...76|t5          0.939    0.138    6.790    0.000    0.939    0.939\n    ...76|t6          1.712    0.207    8.262    0.000    1.712    1.712\n    ...77|t1         -1.046    0.144   -7.264    0.000   -1.046   -1.046\n    ...77|t2         -0.487    0.123   -3.975    0.000   -0.487   -0.487\n    ...77|t3          0.098    0.118    0.835    0.403    0.098    0.098\n    ...77|t4          0.641    0.127    5.062    0.000    0.641    0.641\n    ...77|t5          1.257    0.158    7.948    0.000    1.257    1.257\n    ...77|t6          2.111    0.285    7.411    0.000    2.111    2.111\n    ...78|t1         -1.211    0.155   -7.826    0.000   -1.211   -1.211\n    ...78|t2         -0.391    0.121   -3.241    0.001   -0.391   -0.391\n    ...78|t3          0.209    0.118    1.763    0.078    0.209    0.209\n    ...78|t4          1.046    0.144    7.264    0.000    1.046    1.046\n    ...78|t5          1.624    0.195    8.320    0.000    1.624    1.624\n    ...78|t6          2.111    0.285    7.411    0.000    2.111    2.111\n    ...79|t1         -1.307    0.162   -8.058    0.000   -1.307   -1.307\n    ...79|t2         -0.463    0.122   -3.792    0.000   -0.463   -0.463\n    ...79|t3         -0.033    0.117   -0.279    0.781   -0.033   -0.033\n    ...79|t4          0.695    0.128    5.418    0.000    0.695    0.695\n    ...79|t5          1.712    0.207    8.262    0.000    1.712    1.712\n    ...79|t6          2.111    0.285    7.411    0.000    2.111    2.111\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   ....76             0.273                               0.273    0.273\n   ....77             0.188                               0.188    0.188\n   ....78             0.207                               0.207    0.207\n   ....79             0.327                               0.327    0.327\n    Agentic           0.727    0.051   14.358    0.000    1.000    1.000\n\nScales y*:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n    ...76             1.000                               1.000    1.000\n    ...77             1.000                               1.000    1.000\n    ...78             1.000                               1.000    1.000\n    ...79             1.000                               1.000    1.000\n\n\n\n\nCommunal\n\n```{r}\nnames(EMData)[[80]]\ntable(EMData.Anonymous$...80)\nnames(EMData)[[81]]\ntable(EMData.Anonymous$...81)\nnames(EMData)[[84]]\ntable(EMData.Anonymous$...84)\nCB <- cfa('Communal =~ ...80 + ...81 + ...84', data=EMData.Anonymous, ordered = TRUE)\nsummary(CB, fit.measures = TRUE, standardized = TRUE)\n```\n\n[1] \"28. You lack career guidance and support [You are expected to be unselfish]\"\n\n 1  2  3  4  5  6  7 \n 5 13 24 25 25 19  4 \n[1] \"28. You lack career guidance and support [In your interactions with others you are expected to consider others opinions over your own]\"\n\n 1  2  3  4  5  6  7 \n 9 21 32 28 20  4  1 \n[1] \"28. You lack career guidance and support [Your community expects you to put others interests ahead of your own]\"\n\n 1  2  3  4  5  6  7 \n12 21 24 25 20 11  2 \nlavaan 0.6-12 ended normally after 13 iterations\n\n  Estimator                                       DWLS\n  Optimization method                           NLMINB\n  Number of model parameters                        21\n\n  Number of observations                           115\n\nModel Test User Model:\n                                              Standard      Robust\n  Test Statistic                                 0.000       0.000\n  Degrees of freedom                                 0           0\n\nModel Test Baseline Model:\n\n  Test statistic                               511.919     427.939\n  Degrees of freedom                                 3           3\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.198\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000       1.000\n  Tucker-Lewis Index (TLI)                       1.000       1.000\n                                                                  \n  Robust Comparative Fit Index (CFI)                            NA\n  Robust Tucker-Lewis Index (TLI)                               NA\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000       0.000\n  90 Percent confidence interval - lower         0.000       0.000\n  90 Percent confidence interval - upper         0.000       0.000\n  P-value RMSEA <= 0.05                             NA          NA\n                                                                  \n  Robust RMSEA                                                  NA\n  90 Percent confidence interval - lower                     0.000\n  90 Percent confidence interval - upper                     0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.000       0.000\n\nParameter Estimates:\n\n  Standard errors                           Robust.sem\n  Information                                 Expected\n  Information saturated (h1) model        Unstructured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  Communal =~                                                           \n    ...80             1.000                               0.823    0.823\n    ...81             0.908    0.078   11.572    0.000    0.747    0.747\n    ...84             0.983    0.083   11.823    0.000    0.808    0.808\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   ....80             0.000                               0.000    0.000\n   ....81             0.000                               0.000    0.000\n   ....84             0.000                               0.000    0.000\n    Communal          0.000                               0.000    0.000\n\nThresholds:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n    ...80|t1         -1.712    0.207   -8.262    0.000   -1.712   -1.712\n    ...80|t2         -1.009    0.142   -7.110    0.000   -1.009   -1.009\n    ...80|t3         -0.345    0.120   -2.872    0.004   -0.345   -0.345\n    ...80|t4          0.209    0.118    1.763    0.078    0.209    0.209\n    ...80|t5          0.842    0.134    6.289    0.000    0.842    0.842\n    ...80|t6          1.815    0.223    8.129    0.000    1.815    1.815\n    ...81|t1         -1.417    0.172   -8.235    0.000   -1.417   -1.417\n    ...81|t2         -0.641    0.127   -5.062    0.000   -0.641   -0.641\n    ...81|t3          0.098    0.118    0.835    0.403    0.098    0.098\n    ...81|t4          0.781    0.131    5.945    0.000    0.781    0.781\n    ...81|t5          1.712    0.207    8.262    0.000    1.712    1.712\n    ...81|t6          2.378    0.369    6.451    0.000    2.378    2.378\n    ...84|t1         -1.257    0.158   -7.948    0.000   -1.257   -1.257\n    ...84|t2         -0.562    0.124   -4.521    0.000   -0.562   -0.562\n    ...84|t3         -0.011    0.117   -0.093    0.926   -0.011   -0.011\n    ...84|t4          0.562    0.124    4.521    0.000    0.562    0.562\n    ...84|t5          1.211    0.155    7.826    0.000    1.211    1.211\n    ...84|t6          2.111    0.285    7.411    0.000    2.111    2.111\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   ....80             0.323                               0.323    0.323\n   ....81             0.443                               0.443    0.443\n   ....84             0.347                               0.347    0.347\n    Communal          0.677    0.072    9.442    0.000    1.000    1.000\n\nScales y*:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n    ...80             1.000                               1.000    1.000\n    ...81             1.000                               1.000    1.000\n    ...84             1.000                               1.000    1.000\n\n\n\n\nMentoring\n\n```{r}\nnames(EMData)[[13]]\ntable(EMData.Anonymous$...13)\nnames(EMData)[[14]]\ntable(EMData.Anonymous$...14)\nnames(EMData)[[15]]\ntable(EMData.Anonymous$...15)\nM <- cfa('Mentoring =~ ...13 + ...14 + ...15', data=EMData.Anonymous, ordered = TRUE)\nsummary(M, fit.measures = TRUE, standardized = TRUE)\n```\n\n[1] \"5. On a scale of 7 to 1, with 7 being very regularly and 1 being very rarely, how regularly do you:  [Encourage entrepreneurs to start businesses]\"\n\n 1  2  3  4  5  6  7 \n21 27 12 14 10 26  5 \n[1] \"5. On a scale of 7 to 1, with 7 being very regularly and 1 being very rarely, how regularly do you:  [Reassure other entrepreneurs when things are not going well]\"\n\n 1  2  3  4  5  6  7 \n22 21 10 19 33  4  6 \n[1] \"5. On a scale of 7 to 1, with 7 being very regularly and 1 being very rarely, how regularly do you:  [Help other entrepreneurs have confidence they can succeed]\"\n\n 1  2  3  4  5  6  7 \n21 29 27 11 21  5  1 \nlavaan 0.6-12 ended normally after 10 iterations\n\n  Estimator                                       DWLS\n  Optimization method                           NLMINB\n  Number of model parameters                        21\n\n  Number of observations                           115\n\nModel Test User Model:\n                                              Standard      Robust\n  Test Statistic                                 0.000       0.000\n  Degrees of freedom                                 0           0\n\nModel Test Baseline Model:\n\n  Test statistic                              1726.590    1624.559\n  Degrees of freedom                                 3           3\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.063\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000       1.000\n  Tucker-Lewis Index (TLI)                       1.000       1.000\n                                                                  \n  Robust Comparative Fit Index (CFI)                            NA\n  Robust Tucker-Lewis Index (TLI)                               NA\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000       0.000\n  90 Percent confidence interval - lower         0.000       0.000\n  90 Percent confidence interval - upper         0.000       0.000\n  P-value RMSEA <= 0.05                             NA          NA\n                                                                  \n  Robust RMSEA                                                  NA\n  90 Percent confidence interval - lower                     0.000\n  90 Percent confidence interval - upper                     0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.000       0.000\n\nParameter Estimates:\n\n  Standard errors                           Robust.sem\n  Information                                 Expected\n  Information saturated (h1) model        Unstructured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  Mentoring =~                                                          \n    ...13             1.000                               0.981    0.981\n    ...14             0.879    0.058   15.193    0.000    0.862    0.862\n    ...15             0.844    0.056   15.063    0.000    0.827    0.827\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   ....13             0.000                               0.000    0.000\n   ....14             0.000                               0.000    0.000\n   ....15             0.000                               0.000    0.000\n    Mentoring         0.000                               0.000    0.000\n\nThresholds:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n    ...13|t1         -0.905    0.137   -6.626    0.000   -0.905   -0.905\n    ...13|t2         -0.209    0.118   -1.763    0.078   -0.209   -0.209\n    ...13|t3          0.055    0.117    0.464    0.643    0.055    0.055\n    ...13|t4          0.368    0.120    3.057    0.002    0.368    0.368\n    ...13|t5          0.614    0.126    4.882    0.000    0.614    0.614\n    ...13|t6          1.712    0.207    8.262    0.000    1.712    1.712\n    ...14|t1         -0.873    0.135   -6.459    0.000   -0.873   -0.873\n    ...14|t2         -0.322    0.120   -2.688    0.007   -0.322   -0.322\n    ...14|t3         -0.098    0.118   -0.835    0.403   -0.098   -0.098\n    ...14|t4          0.322    0.120    2.688    0.007    0.322    0.322\n    ...14|t5          1.360    0.167    8.155    0.000    1.360    1.360\n    ...14|t6          1.624    0.195    8.320    0.000    1.624    1.624\n    ...15|t1         -0.905    0.137   -6.626    0.000   -0.905   -0.905\n    ...15|t2         -0.164    0.118   -1.392    0.164   -0.164   -0.164\n    ...15|t3          0.439    0.122    3.608    0.000    0.439    0.439\n    ...15|t4          0.723    0.129    5.595    0.000    0.723    0.723\n    ...15|t5          1.624    0.195    8.320    0.000    1.624    1.624\n    ...15|t6          2.378    0.369    6.451    0.000    2.378    2.378\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   ....13             0.039                               0.039    0.039\n   ....14             0.258                               0.258    0.258\n   ....15             0.315                               0.315    0.315\n    Mentoring         0.961    0.077   12.508    0.000    1.000    1.000\n\nScales y*:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n    ...13             1.000                               1.000    1.000\n    ...14             1.000                               1.000    1.000\n    ...15             1.000                               1.000    1.000\n\n\n\n\nSocial Influence\n\n```{r}\nnames(EMData)[[37]]\ntable(EMData.Anonymous$...37)\nnames(EMData)[[38]]\ntable(EMData.Anonymous$...38)\nnames(EMData)[[39]]\ntable(EMData.Anonymous$...39)\nSI <- cfa('Social.Influence =~ ...37 + ...38 + ...39', data=EMData.Anonymous, ordered = TRUE)\nsummary(SI, fit.measures = TRUE, standardized = TRUE)\n```\n\n[1] \"11. On a scale of 7 to 1, with 7 being strongly agree and 1 being strongly disagree, please indicate your level of agreement with the following items relating to your community:  [The respect you have in the community helps your business]\"\n\n 1  2  3  4  5  6 \n26 38 31  5  8  7 \n[1] \"11. On a scale of 7 to 1, with 7 being strongly agree and 1 being strongly disagree, please indicate your level of agreement with the following items relating to your community:  [Your understanding of the community helps your business]\"\n\n 1  2  3  4  5  6 \n27 40 28  6  7  7 \n[1] \"11. On a scale of 7 to 1, with 7 being strongly agree and 1 being strongly disagree, please indicate your level of agreement with the following items relating to your community:  [Your influence in the community helps your business]\"\n\n 1  2  3  4  5  6  7 \n30 38 30  4  7  5  1 \nlavaan 0.6-12 ended normally after 12 iterations\n\n  Estimator                                       DWLS\n  Optimization method                           NLMINB\n  Number of model parameters                        19\n\n  Number of observations                           115\n\nModel Test User Model:\n                                              Standard      Robust\n  Test Statistic                                 0.000       0.000\n  Degrees of freedom                                 0           0\n\nModel Test Baseline Model:\n\n  Test statistic                              3636.699    2922.199\n  Degrees of freedom                                 3           3\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  1.245\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    1.000       1.000\n  Tucker-Lewis Index (TLI)                       1.000       1.000\n                                                                  \n  Robust Comparative Fit Index (CFI)                            NA\n  Robust Tucker-Lewis Index (TLI)                               NA\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000       0.000\n  90 Percent confidence interval - lower         0.000       0.000\n  90 Percent confidence interval - upper         0.000       0.000\n  P-value RMSEA <= 0.05                             NA          NA\n                                                                  \n  Robust RMSEA                                                  NA\n  90 Percent confidence interval - lower                     0.000\n  90 Percent confidence interval - upper                     0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.000       0.000\n\nParameter Estimates:\n\n  Standard errors                           Robust.sem\n  Information                                 Expected\n  Information saturated (h1) model        Unstructured\n\nLatent Variables:\n                      Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n  Social.Influence =~                                                      \n    ...37                1.000                               0.928    0.928\n    ...38                1.055    0.031   33.595    0.000    0.979    0.979\n    ...39                0.942    0.029   32.441    0.000    0.874    0.874\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   ....37             0.000                               0.000    0.000\n   ....38             0.000                               0.000    0.000\n   ....39             0.000                               0.000    0.000\n    Social.Influnc    0.000                               0.000    0.000\n\nThresholds:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n    ...37|t1         -0.752    0.130   -5.771    0.000   -0.752   -0.752\n    ...37|t2          0.142    0.118    1.207    0.228    0.142    0.142\n    ...37|t3          0.939    0.138    6.790    0.000    0.939    0.939\n    ...37|t4          1.124    0.149    7.558    0.000    1.124    1.124\n    ...37|t5          1.548    0.186    8.325    0.000    1.548    1.548\n    ...38|t1         -0.723    0.129   -5.595    0.000   -0.723   -0.723\n    ...38|t2          0.209    0.118    1.763    0.078    0.209    0.209\n    ...38|t3          0.939    0.138    6.790    0.000    0.939    0.939\n    ...38|t4          1.166    0.152    7.696    0.000    1.166    1.166\n    ...38|t5          1.548    0.186    8.325    0.000    1.548    1.548\n    ...39|t1         -0.641    0.127   -5.062    0.000   -0.641   -0.641\n    ...39|t2          0.231    0.119    1.948    0.051    0.231    0.231\n    ...39|t3          1.046    0.144    7.264    0.000    1.046    1.046\n    ...39|t4          1.211    0.155    7.826    0.000    1.211    1.211\n    ...39|t5          1.624    0.195    8.320    0.000    1.624    1.624\n    ...39|t6          2.378    0.369    6.451    0.000    2.378    2.378\n\nVariances:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n   ....37             0.139                               0.139    0.139\n   ....38             0.041                               0.041    0.041\n   ....39             0.236                               0.236    0.236\n    Social.Influnc    0.861    0.034   25.088    0.000    1.000    1.000\n\nScales y*:\n                   Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n    ...37             1.000                               1.000    1.000\n    ...38             1.000                               1.000    1.000\n    ...39             1.000                               1.000    1.000\n\n\n\n```{r}\nlibrary(lavaanPlot)\nlavaanPlot(model = SI, node_options = list(shape = \"box\", fontname = \"Helvetica\"), edge_options = list(color = \"grey\"), coefs = TRUE, covs = TRUE)\n```\n\n\n\n\n\n\n\nAn SEM\nNow let me combine those measurement models to produce a set of two structural equations. I wish to explain income and employment given these factors.\n\n```{r, warning=FALSE, message=FALSE}\nnames(EMData)[c(5,59)]\nStruct <- sem('Agentic =~ ...76 + ...77 + ...78 + ...79\n          Communal =~ ...80 + ...81 + ...84\n          Mentoring =~ ...13 + ...14 + ...15\n          Social.Influence =~ ...37 + ...38 + ...39\n          ...59 ~ Agentic + Communal + Mentoring + Social.Influence\n          ...5 ~ Agentic + Communal + Mentoring + Social.Influence', data=EMData.Anonymous, ordered = c(\"...13\",\"...14\", \"...15\", \"...80\",\"...81\", \"...84\", \"...76\",\"...77\", \"...78\", \"...79\",\"...37\", \"...38\", \"...39\"))\nsummary(Struct, fit.measures=TRUE, standardized=TRUE)\n```\n\n[1] \"3. What is the current income generated from the business? (USD PM)\"\n[2] \"20. How many people do you manage in your business?\"                \nlavaan 0.6-12 ended normally after 112 iterations\n\n  Estimator                                       DWLS\n  Optimization method                           NLMINB\n  Number of model parameters                       108\n\n  Number of observations                           115\n\nModel Test User Model:\n                                              Standard      Robust\n  Test Statistic                               176.253     255.843\n  Degrees of freedom                                77          77\n  P-value (Chi-square)                           0.000       0.000\n  Scaling correction factor                                  0.804\n  Shift parameter                                           36.682\n    simple second-order correction                                \n\nModel Test Baseline Model:\n\n  Test statistic                              9489.339    3694.019\n  Degrees of freedom                               105         105\n  P-value                                        0.000       0.000\n  Scaling correction factor                                  2.615\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.989       0.950\n  Tucker-Lewis Index (TLI)                       0.986       0.932\n                                                                  \n  Robust Comparative Fit Index (CFI)                            NA\n  Robust Tucker-Lewis Index (TLI)                               NA\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.106       0.143\n  90 Percent confidence interval - lower         0.086       0.124\n  90 Percent confidence interval - upper         0.127       0.162\n  P-value RMSEA <= 0.05                          0.000       0.000\n                                                                  \n  Robust RMSEA                                                  NA\n  90 Percent confidence interval - lower                        NA\n  90 Percent confidence interval - upper                        NA\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.075       0.075\n\nParameter Estimates:\n\n  Standard errors                           Robust.sem\n  Information                                 Expected\n  Information saturated (h1) model        Unstructured\n\nLatent Variables:\n                      Estimate   Std.Err  z-value  P(>|z|)   Std.lv   Std.all\n  Agentic =~                                                                 \n    ...76                 1.000                                0.812    0.812\n    ...77                 1.110    0.051   21.573    0.000     0.901    0.901\n    ...78                 1.076    0.044   24.545    0.000     0.873    0.873\n    ...79                 1.097    0.047   23.283    0.000     0.890    0.890\n  Communal =~                                                                \n    ...80                 1.000                                0.715    0.715\n    ...81                 1.233    0.126    9.803    0.000     0.881    0.881\n    ...84                 1.103    0.091   12.190    0.000     0.788    0.788\n  Mentoring =~                                                               \n    ...13                 1.000                                0.961    0.961\n    ...14                 0.908    0.057   15.897    0.000     0.872    0.872\n    ...15                 0.876    0.054   16.228    0.000     0.842    0.842\n  Social.Influence =~                                                        \n    ...37                 1.000                                0.948    0.948\n    ...38                 1.004    0.030   33.882    0.000     0.952    0.952\n    ...39                 0.941    0.029   32.029    0.000     0.892    0.892\n\nRegressions:\n                   Estimate   Std.Err  z-value  P(>|z|)   Std.lv   Std.all\n  ...59 ~                                                                 \n    Agentic           -0.087    0.063   -1.386    0.166    -0.071   -0.052\n    Communal           0.119    0.108    1.100    0.271     0.085    0.062\n    Mentoring          0.166    0.075    2.213    0.027     0.159    0.116\n    Social.Influnc    -0.422    0.055   -7.620    0.000    -0.400   -0.292\n  ...5 ~                                                                  \n    Agentic           51.893   19.705    2.633    0.008    42.118    0.205\n    Communal          -9.427   20.599   -0.458    0.647    -6.736   -0.033\n    Mentoring          1.218   15.046    0.081    0.935     1.170    0.006\n    Social.Influnc    35.014   16.427    2.131    0.033    33.195    0.161\n\nCovariances:\n                   Estimate   Std.Err  z-value  P(>|z|)   Std.lv   Std.all\n  Agentic ~~                                                              \n    Communal           0.259    0.044    5.821    0.000     0.447    0.447\n    Mentoring          0.203    0.062    3.269    0.001     0.261    0.261\n    Social.Influnc     0.247    0.056    4.423    0.000     0.321    0.321\n  Communal ~~                                                             \n    Mentoring          0.134    0.057    2.329    0.020     0.195    0.195\n    Social.Influnc     0.308    0.062    4.963    0.000     0.454    0.454\n  Mentoring ~~                                                            \n    Social.Influnc     0.067    0.074    0.904    0.366     0.074    0.074\n ....59 ~~                                                                \n   ....5              78.509   19.599    4.006    0.000    78.509    0.304\n\nIntercepts:\n                   Estimate   Std.Err  z-value  P(>|z|)   Std.lv   Std.all\n   ....76              0.000                                0.000    0.000\n   ....77              0.000                                0.000    0.000\n   ....78              0.000                                0.000    0.000\n   ....79              0.000                                0.000    0.000\n   ....80              0.000                                0.000    0.000\n   ....81              0.000                                0.000    0.000\n   ....84              0.000                                0.000    0.000\n   ....13              0.000                                0.000    0.000\n   ....14              0.000                                0.000    0.000\n   ....15              0.000                                0.000    0.000\n   ....37              0.000                                0.000    0.000\n   ....38              0.000                                0.000    0.000\n   ....39              0.000                                0.000    0.000\n   ....59              0.435    0.305    1.427    0.154     0.435    0.317\n   ....5             380.609   23.778   16.007    0.000   380.609    1.849\n    Agentic            0.000                                0.000    0.000\n    Communal           0.000                                0.000    0.000\n    Mentoring          0.000                                0.000    0.000\n    Social.Influnc     0.000                                0.000    0.000\n\nThresholds:\n                   Estimate   Std.Err  z-value  P(>|z|)   Std.lv   Std.all\n    ...76|t1          -0.873    0.135   -6.459    0.000    -0.873   -0.873\n    ...76|t2          -0.391    0.121   -3.241    0.001    -0.391   -0.391\n    ...76|t3           0.098    0.118    0.835    0.403     0.098    0.098\n    ...76|t4           0.588    0.125    4.702    0.000     0.588    0.588\n    ...76|t5           0.939    0.138    6.790    0.000     0.939    0.939\n    ...76|t6           1.712    0.207    8.262    0.000     1.712    1.712\n    ...77|t1          -1.046    0.144   -7.264    0.000    -1.046   -1.046\n    ...77|t2          -0.487    0.123   -3.975    0.000    -0.487   -0.487\n    ...77|t3           0.098    0.118    0.835    0.403     0.098    0.098\n    ...77|t4           0.641    0.127    5.062    0.000     0.641    0.641\n    ...77|t5           1.257    0.158    7.948    0.000     1.257    1.257\n    ...77|t6           2.111    0.285    7.411    0.000     2.111    2.111\n    ...78|t1          -1.211    0.155   -7.826    0.000    -1.211   -1.211\n    ...78|t2          -0.391    0.121   -3.241    0.001    -0.391   -0.391\n    ...78|t3           0.209    0.118    1.763    0.078     0.209    0.209\n    ...78|t4           1.046    0.144    7.264    0.000     1.046    1.046\n    ...78|t5           1.624    0.195    8.320    0.000     1.624    1.624\n    ...78|t6           2.111    0.285    7.411    0.000     2.111    2.111\n    ...79|t1          -1.307    0.162   -8.058    0.000    -1.307   -1.307\n    ...79|t2          -0.463    0.122   -3.792    0.000    -0.463   -0.463\n    ...79|t3          -0.033    0.117   -0.279    0.781    -0.033   -0.033\n    ...79|t4           0.695    0.128    5.418    0.000     0.695    0.695\n    ...79|t5           1.712    0.207    8.262    0.000     1.712    1.712\n    ...79|t6           2.111    0.285    7.411    0.000     2.111    2.111\n    ...80|t1          -1.712    0.207   -8.262    0.000    -1.712   -1.712\n    ...80|t2          -1.009    0.142   -7.110    0.000    -1.009   -1.009\n    ...80|t3          -0.345    0.120   -2.872    0.004    -0.345   -0.345\n    ...80|t4           0.209    0.118    1.763    0.078     0.209    0.209\n    ...80|t5           0.842    0.134    6.289    0.000     0.842    0.842\n    ...80|t6           1.815    0.223    8.129    0.000     1.815    1.815\n    ...81|t1          -1.417    0.172   -8.235    0.000    -1.417   -1.417\n    ...81|t2          -0.641    0.127   -5.062    0.000    -0.641   -0.641\n    ...81|t3           0.098    0.118    0.835    0.403     0.098    0.098\n    ...81|t4           0.781    0.131    5.945    0.000     0.781    0.781\n    ...81|t5           1.712    0.207    8.262    0.000     1.712    1.712\n    ...81|t6           2.378    0.369    6.451    0.000     2.378    2.378\n    ...84|t1          -1.257    0.158   -7.948    0.000    -1.257   -1.257\n    ...84|t2          -0.562    0.124   -4.521    0.000    -0.562   -0.562\n    ...84|t3          -0.011    0.117   -0.093    0.926    -0.011   -0.011\n    ...84|t4           0.562    0.124    4.521    0.000     0.562    0.562\n    ...84|t5           1.211    0.155    7.826    0.000     1.211    1.211\n    ...84|t6           2.111    0.285    7.411    0.000     2.111    2.111\n    ...13|t1          -0.905    0.137   -6.626    0.000    -0.905   -0.905\n    ...13|t2          -0.209    0.118   -1.763    0.078    -0.209   -0.209\n    ...13|t3           0.055    0.117    0.464    0.643     0.055    0.055\n    ...13|t4           0.368    0.120    3.057    0.002     0.368    0.368\n    ...13|t5           0.614    0.126    4.882    0.000     0.614    0.614\n    ...13|t6           1.712    0.207    8.262    0.000     1.712    1.712\n    ...14|t1          -0.873    0.135   -6.459    0.000    -0.873   -0.873\n    ...14|t2          -0.322    0.120   -2.688    0.007    -0.322   -0.322\n    ...14|t3          -0.098    0.118   -0.835    0.403    -0.098   -0.098\n    ...14|t4           0.322    0.120    2.688    0.007     0.322    0.322\n    ...14|t5           1.360    0.167    8.155    0.000     1.360    1.360\n    ...14|t6           1.624    0.195    8.320    0.000     1.624    1.624\n    ...15|t1          -0.905    0.137   -6.626    0.000    -0.905   -0.905\n    ...15|t2          -0.164    0.118   -1.392    0.164    -0.164   -0.164\n    ...15|t3           0.439    0.122    3.608    0.000     0.439    0.439\n    ...15|t4           0.723    0.129    5.595    0.000     0.723    0.723\n    ...15|t5           1.624    0.195    8.320    0.000     1.624    1.624\n    ...15|t6           2.378    0.369    6.451    0.000     2.378    2.378\n    ...37|t1          -0.752    0.130   -5.771    0.000    -0.752   -0.752\n    ...37|t2           0.142    0.118    1.207    0.228     0.142    0.142\n    ...37|t3           0.939    0.138    6.790    0.000     0.939    0.939\n    ...37|t4           1.124    0.149    7.558    0.000     1.124    1.124\n    ...37|t5           1.548    0.186    8.325    0.000     1.548    1.548\n    ...38|t1          -0.723    0.129   -5.595    0.000    -0.723   -0.723\n    ...38|t2           0.209    0.118    1.763    0.078     0.209    0.209\n    ...38|t3           0.939    0.138    6.790    0.000     0.939    0.939\n    ...38|t4           1.166    0.152    7.696    0.000     1.166    1.166\n    ...38|t5           1.548    0.186    8.325    0.000     1.548    1.548\n    ...39|t1          -0.641    0.127   -5.062    0.000    -0.641   -0.641\n    ...39|t2           0.231    0.119    1.948    0.051     0.231    0.231\n    ...39|t3           1.046    0.144    7.264    0.000     1.046    1.046\n    ...39|t4           1.211    0.155    7.826    0.000     1.211    1.211\n    ...39|t5           1.624    0.195    8.320    0.000     1.624    1.624\n    ...39|t6           2.378    0.369    6.451    0.000     2.378    2.378\n\nVariances:\n                   Estimate   Std.Err  z-value  P(>|z|)   Std.lv   Std.all\n   ....76              0.341                                0.341    0.341\n   ....77              0.188                                0.188    0.188\n   ....78              0.238                                0.238    0.238\n   ....79              0.207                                0.207    0.207\n   ....80              0.489                                0.489    0.489\n   ....81              0.224                                0.224    0.224\n   ....84              0.378                                0.378    0.378\n   ....13              0.077                                0.077    0.077\n   ....14              0.239                                0.239    0.239\n   ....15              0.292                                0.292    0.292\n   ....37              0.101                                0.101    0.101\n   ....38              0.094                                0.094    0.094\n   ....39              0.204                                0.204    0.204\n   ....59              1.711    0.147   11.666    0.000     1.711    0.910\n   ....5           38984.352 4516.263    8.632    0.000 38984.352    0.920\n    Agentic            0.659    0.048   13.761    0.000     1.000    1.000\n    Communal           0.511    0.068    7.492    0.000     1.000    1.000\n    Mentoring          0.923    0.072   12.824    0.000     1.000    1.000\n    Social.Influnc     0.899    0.033   27.070    0.000     1.000    1.000\n\nScales y*:\n                   Estimate   Std.Err  z-value  P(>|z|)   Std.lv   Std.all\n    ...76              1.000                                1.000    1.000\n    ...77              1.000                                1.000    1.000\n    ...78              1.000                                1.000    1.000\n    ...79              1.000                                1.000    1.000\n    ...80              1.000                                1.000    1.000\n    ...81              1.000                                1.000    1.000\n    ...84              1.000                                1.000    1.000\n    ...13              1.000                                1.000    1.000\n    ...14              1.000                                1.000    1.000\n    ...15              1.000                                1.000    1.000\n    ...37              1.000                                1.000    1.000\n    ...38              1.000                                1.000    1.000\n    ...39              1.000                                1.000    1.000\n\n\n\n```{r}\nlavaanPlot(model=Struct, node_options = list(shape = \"box\", fontname = \"Helvetica\"), edge_options = list(color = \"grey\"), coefs = TRUE, covs = TRUE)\n```"
  },
  {
    "objectID": "posts/week-6/index.html#some-examples-from-survival-time",
    "href": "posts/week-6/index.html#some-examples-from-survival-time",
    "title": "Week 6: Measurement and Survival",
    "section": "Some Examples from Survival Time",
    "text": "Some Examples from Survival Time\nJob durations and the duration of interstate peace."
  },
  {
    "objectID": "posts/week-7/index.html",
    "href": "posts/week-7/index.html",
    "title": "Week 7: Survival, Power, and Planning",
    "section": "",
    "text": "The slides are here..\nOur seventh class meeting will focus on Chapter 9 and Chapter 11 of Handbook of Regression Modeling in People Analytics."
  },
  {
    "objectID": "posts/week-7/index.html#the-skinny",
    "href": "posts/week-7/index.html#the-skinny",
    "title": "Week 7: Survival, Power, and Planning",
    "section": "The Skinny",
    "text": "The Skinny\nTwo major topics: survival analysis with some discussion of parametric survival analysis and power and study planning."
  },
  {
    "objectID": "posts/week-7/index.html#parametric-survival-analysis",
    "href": "posts/week-7/index.html#parametric-survival-analysis",
    "title": "Week 7: Survival, Power, and Planning",
    "section": "Parametric Survival Analysis",
    "text": "Parametric Survival Analysis\nMost of this is typically done using the flexsurv package in R. There is a great post on parametric survival analysis. We could use AIC/BIC for comparison of the various distributions."
  },
  {
    "objectID": "posts/week-7/index.html#some-models",
    "href": "posts/week-7/index.html#some-models",
    "title": "Week 7: Survival, Power, and Planning",
    "section": "Some Models",
    "text": "Some Models\n\n```{r, message=FALSE, warning=FALSE}\nlibrary(haven)\nlibrary(tidyverse)\noptions(scipen=7)\nBKT.Data <- read_dta(\"https://github.com/robertwwalker/xaringan/raw/master/CMF-Week-6/img/bkt98ajps.dta\")\ncloglog_model <- glm(dispute ~ dem + growth+allies+contig+capratio+trade+as.factor(py),\n  data = BKT.Data, family=binomial(link = \"cloglog\"))\nsummary(cloglog_model)\n```\n\n\nCall:\nglm(formula = dispute ~ dem + growth + allies + contig + capratio + \n    trade + as.factor(py), family = binomial(link = \"cloglog\"), \n    data = BKT.Data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.4183  -0.2282  -0.1420  -0.0828   3.9654  \n\nCoefficients:\n                  Estimate Std. Error z value Pr(>|z|)    \n(Intercept)      -1.113947   0.082348 -13.527  < 2e-16 ***\ndem              -0.049384   0.007288  -6.776 1.24e-11 ***\ngrowth           -0.008082   0.007612  -1.062 0.288357    \nallies           -0.430045   0.078126  -5.504 3.70e-08 ***\ncontig            0.554479   0.077111   7.191 6.45e-13 ***\ncapratio         -0.003023   0.000405  -7.464 8.39e-14 ***\ntrade           -12.503176   9.960629  -1.255 0.209385    \nas.factor(py)2   -1.797014   0.130842 -13.734  < 2e-16 ***\nas.factor(py)3   -1.910023   0.145076 -13.166  < 2e-16 ***\nas.factor(py)4   -2.130485   0.165704 -12.857  < 2e-16 ***\nas.factor(py)5   -2.378740   0.193902 -12.268  < 2e-16 ***\nas.factor(py)6   -2.836553   0.246354 -11.514  < 2e-16 ***\nas.factor(py)7   -2.831567   0.246584 -11.483  < 2e-16 ***\nas.factor(py)8   -2.757412   0.239957 -11.491  < 2e-16 ***\nas.factor(py)9   -2.701098   0.239710 -11.268  < 2e-16 ***\nas.factor(py)10  -2.990651   0.280582 -10.659  < 2e-16 ***\nas.factor(py)11  -3.207771   0.319289 -10.047  < 2e-16 ***\nas.factor(py)12  -3.077872   0.305041 -10.090  < 2e-16 ***\nas.factor(py)13  -3.670483   0.410654  -8.938  < 2e-16 ***\nas.factor(py)14  -4.310370   0.578624  -7.449 9.38e-14 ***\nas.factor(py)15  -4.329282   0.578841  -7.479 7.48e-14 ***\nas.factor(py)16  -3.209284   0.336350  -9.542  < 2e-16 ***\nas.factor(py)17  -3.973393   0.501858  -7.917 2.43e-15 ***\nas.factor(py)18  -5.339488   1.000841  -5.335 9.55e-08 ***\nas.factor(py)19  -3.367405   0.381031  -8.838  < 2e-16 ***\nas.factor(py)20  -3.704984   0.449627  -8.240  < 2e-16 ***\nas.factor(py)21  -4.552930   0.708058  -6.430 1.27e-10 ***\nas.factor(py)22  -4.069630   0.579535  -7.022 2.18e-12 ***\nas.factor(py)23  -3.894665   0.579557  -6.720 1.82e-11 ***\nas.factor(py)24  -3.761143   0.579762  -6.487 8.73e-11 ***\nas.factor(py)25 -17.294682 317.920692  -0.054 0.956617    \nas.factor(py)26  -2.758181   0.411827  -6.697 2.12e-11 ***\nas.factor(py)27 -17.282784 369.870471  -0.047 0.962731    \nas.factor(py)28 -17.251363 368.516083  -0.047 0.962662    \nas.factor(py)29  -3.702860   0.709164  -5.221 1.78e-07 ***\nas.factor(py)30  -3.209144   0.579970  -5.533 3.14e-08 ***\nas.factor(py)31  -4.201176   1.002675  -4.190 2.79e-05 ***\nas.factor(py)32  -3.364920   0.710748  -4.734 2.20e-06 ***\nas.factor(py)33  -3.869193   1.001648  -3.863 0.000112 ***\nas.factor(py)34  -3.112066   0.709943  -4.384 1.17e-05 ***\nas.factor(py)35  -3.651929   1.003808  -3.638 0.000275 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 7719.2  on 20989  degrees of freedom\nResidual deviance: 5108.1  on 20949  degrees of freedom\nAIC: 5190.1\n\nNumber of Fisher Scoring iterations: 17"
  },
  {
    "objectID": "posts/week-7/index.html#onset",
    "href": "posts/week-7/index.html#onset",
    "title": "Week 7: Survival, Power, and Planning",
    "section": "Onset",
    "text": "Onset\n\n```{r}\ncloglog_model <- BKT.Data %>% filter(contdisp!=1) %>% glm(dispute ~ dem + growth+allies+contig+capratio+trade+as.factor(py),\n  data = ., family=binomial(link = \"cloglog\"))\nsummary(cloglog_model)\n```\n\n\nCall:\nglm(formula = dispute ~ dem + growth + allies + contig + capratio + \n    trade + as.factor(py), family = binomial(link = \"cloglog\"), \n    data = .)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-0.7871  -0.2159  -0.1389  -0.0865   3.7395  \n\nCoefficients:\n                   Estimate  Std. Error z value Pr(>|z|)    \n(Intercept)      -3.6245986   0.1777915 -20.387  < 2e-16 ***\ndem              -0.0386198   0.0099036  -3.900 9.64e-05 ***\ngrowth           -0.0385768   0.0119971  -3.216 0.001302 ** \nallies           -0.3649215   0.1110691  -3.286 0.001018 ** \ncontig            0.9723362   0.1216458   7.993 1.32e-15 ***\ncapratio         -0.0022631   0.0005188  -4.362 1.29e-05 ***\ntrade            -3.3539539   9.4666133  -0.354 0.723119    \nas.factor(py)2    0.4397894   0.1883996   2.334 0.019578 *  \nas.factor(py)3    0.3491778   0.1987540   1.757 0.078946 .  \nas.factor(py)4    0.1389052   0.2145618   0.647 0.517379    \nas.factor(py)5   -0.0975959   0.2370175  -0.412 0.680510    \nas.factor(py)6   -0.5558984   0.2812503  -1.977 0.048095 *  \nas.factor(py)7   -0.5483215   0.2818479  -1.945 0.051721 .  \nas.factor(py)8   -0.4806042   0.2759244  -1.742 0.081544 .  \nas.factor(py)9   -0.4085754   0.2758451  -1.481 0.138560    \nas.factor(py)10  -0.6910220   0.3121538  -2.214 0.026848 *  \nas.factor(py)11  -0.9041806   0.3477067  -2.600 0.009311 ** \nas.factor(py)12  -0.7831121   0.3340890  -2.344 0.019077 *  \nas.factor(py)13  -1.3767286   0.4329015  -3.180 0.001472 ** \nas.factor(py)14  -2.0004550   0.5944831  -3.365 0.000765 ***\nas.factor(py)15  -2.0216113   0.5948272  -3.399 0.000677 ***\nas.factor(py)16  -0.9003473   0.3631977  -2.479 0.013177 *  \nas.factor(py)17  -1.6627516   0.5202523  -3.196 0.001393 ** \nas.factor(py)18  -3.0275260   1.0095413  -2.999 0.002709 ** \nas.factor(py)19  -1.0567304   0.4048386  -2.610 0.009048 ** \nas.factor(py)20  -1.3950385   0.4701169  -2.967 0.003003 ** \nas.factor(py)21  -2.2331641   0.7205377  -3.099 0.001940 ** \nas.factor(py)22  -1.7502751   0.5957590  -2.938 0.003305 ** \nas.factor(py)23  -1.5705581   0.5959611  -2.635 0.008405 ** \nas.factor(py)24  -1.4617157   0.5962646  -2.451 0.014228 *  \nas.factor(py)25 -14.9967931 317.2228798  -0.047 0.962294    \nas.factor(py)26  -0.4515102   0.4345600  -1.039 0.298802    \nas.factor(py)27 -14.9541479 369.7841411  -0.040 0.967742    \nas.factor(py)28 -14.9142319 368.2468025  -0.041 0.967694    \nas.factor(py)29  -1.3757208   0.7239855  -1.900 0.057406 .  \nas.factor(py)30  -0.8906066   0.5982936  -1.489 0.136599    \nas.factor(py)31  -1.9082656   1.0131416  -1.884 0.059631 .  \nas.factor(py)32  -1.0835368   0.7247409  -1.495 0.134897    \nas.factor(py)33  -1.6244040   1.0113319  -1.606 0.108229    \nas.factor(py)34  -0.8541733   0.7242140  -1.179 0.238220    \nas.factor(py)35  -1.3772737   1.0150861  -1.357 0.174843    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 3978.5  on 20447  degrees of freedom\nResidual deviance: 3464.1  on 20407  degrees of freedom\nAIC: 3546.1\n\nNumber of Fisher Scoring iterations: 17"
  },
  {
    "objectID": "posts/week-7/index.html#taylor-smoothing",
    "href": "posts/week-7/index.html#taylor-smoothing",
    "title": "Week 7: Survival, Power, and Planning",
    "section": "Taylor Smoothing",
    "text": "Taylor Smoothing\n\n```{r}\ncloglog_model <- BKT.Data %>% filter(prefail<1) %>% glm(dispute ~ dem + growth+allies+contig+capratio+trade+poly(py, 3),\n  data = ., family=binomial(link = \"cloglog\"))\nsummary(cloglog_model)\n```\n\n\nCall:\nglm(formula = dispute ~ dem + growth + allies + contig + capratio + \n    trade + poly(py, 3), family = binomial(link = \"cloglog\"), \n    data = .)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-0.6249  -0.1580  -0.1094  -0.0818   3.7823  \n\nCoefficients:\n                Estimate  Std. Error z value       Pr(>|z|)    \n(Intercept)   -4.9452420   0.1576014 -31.378        < 2e-16 ***\ndem           -0.0450710   0.0131444  -3.429       0.000606 ***\ngrowth        -0.0269900   0.0171645  -1.572       0.115850    \nallies        -0.4246004   0.1602099  -2.650       0.008043 ** \ncontig         1.0788756   0.1698279   6.353 0.000000000211 ***\ncapratio      -0.0019260   0.0006007  -3.206       0.001344 ** \ntrade         -3.0060629  11.5462355  -0.260       0.794594    \npoly(py, 3)1 -57.1154511  10.9618803  -5.210 0.000000188465 ***\npoly(py, 3)2  55.4155210  10.6438428   5.206 0.000000192596 ***\npoly(py, 3)3  -5.0958348   9.9455269  -0.512       0.608389    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2183.3  on 16990  degrees of freedom\nResidual deviance: 1941.7  on 16981  degrees of freedom\nAIC: 1961.7\n\nNumber of Fisher Scoring iterations: 9\n\n\nA plot of the baseline hazard.\n\n```{r}\nScenario.1 <- data.frame(dem = 0, growth=0, allies=0, contig=0, capratio=1, trade=0.002, py=seq(1,35))\nScenario.2 <- data.frame(dem = 0, growth=0, allies=0, contig=1, capratio=1, trade=0.002, py=seq(1,35))\nRes.1 <- predict(cloglog_model, newdata=Scenario.1, type= \"response\")\nRes.2 <- predict(cloglog_model, newdata=Scenario.2, type= \"response\")\ndata.frame(No = Res.1, Yes = Res.2, Scenario.1) %>% pivot_longer(., cols=c(No, Yes)) %>% ggplot() + aes(x=py, y=value, color=name) + labs(x=\"Years of Peace\", color=\"Contiguous?\") + geom_step() + hrbrthemes::theme_ipsum_rc()\n```\n\n\n\n\n\n```{r}\nScenario.1 <- data.frame(dem = 0, growth=0, allies=0, contig=1, capratio=1, trade=0.002, py=seq(1,35))\nScenario.2 <- data.frame(dem = 0, growth=0, allies=1, contig=1, capratio=1, trade=0.002, py=seq(1,35))\nRes.1 <- predict(cloglog_model, newdata=Scenario.1, type= \"response\")\nRes.2 <- predict(cloglog_model, newdata=Scenario.2, type= \"response\")\ndata.frame(No = Res.1, Yes = Res.2, Scenario.1) %>% pivot_longer(., cols=c(No, Yes)) %>% ggplot() + aes(x=py, y=value, color=name) + labs(x=\"Years of Peace\", color=\"Allies?\") + geom_step() + hrbrthemes::theme_ipsum_rc()\n```"
  },
  {
    "objectID": "posts/week-7/index.html#geom_line-smooths-it-out",
    "href": "posts/week-7/index.html#geom_line-smooths-it-out",
    "title": "Week 7: Survival, Power, and Planning",
    "section": "geom_line smooths it out",
    "text": "geom_line smooths it out\n\n```{r}\nScenario.1 <- data.frame(dem = 0, growth=0, allies=0, contig=1, capratio=1, trade=0.002, py=seq(1,35))\nScenario.2 <- data.frame(dem = 0, growth=0, allies=1, contig=1, capratio=1, trade=0.002, py=seq(1,35))\nRes.1 <- predict(cloglog_model, newdata=Scenario.1, type= \"response\")\nRes.2 <- predict(cloglog_model, newdata=Scenario.2, type= \"response\")\ndata.frame(No = Res.1, Yes = Res.2, Scenario.1) %>% pivot_longer(., cols=c(No, Yes)) %>% ggplot() + aes(x=py, y=value, color=name) + labs(x=\"Years of Peace\", color=\"Allies?\") + geom_line() + hrbrthemes::theme_ipsum_rc()\n```"
  },
  {
    "objectID": "posts/week-8/index.html",
    "href": "posts/week-8/index.html",
    "title": "Week 8: Introducing Time",
    "section": "",
    "text": "The slides are here..\nOur eighth class meeting will focus on Chapter 1 and Chapter 2 of Forecasting: Principles and Practice [3rd edition]."
  },
  {
    "objectID": "posts/week-9/index.html",
    "href": "posts/week-9/index.html",
    "title": "Week 9: Features and Decomposing Time",
    "section": "",
    "text": "The slides are here..\nOur ninth class meeting will focus on Chapter 3 and Chapter 4 of Forecasting: Principles and Practice [3rd edition].\nAll of the data for today, including computations, can be acquired using"
  },
  {
    "objectID": "posts/week-9/index.html#decompositions",
    "href": "posts/week-9/index.html#decompositions",
    "title": "Week 9: Features and Decomposing Time",
    "section": "Decompositions",
    "text": "Decompositions\n\nClassical Decomposition\nThe key difference between the two decompositions, and there are others, is the existence [or lack thereof] of a window. In the classical decomposition, there is almost no flexibility.\n\n```{r}\nInflation.Expectations %>% \n  as_tsibble(index=date) %>% \n  model(stl = classical_decomposition(`Median one-year ahead expected inflation rate` ~ season(12))) %>%\n  components() %>% \n  autoplot()\n```\n\n\n\n\n\n\nSTL Decomposition\nThe key new element to the STL decomposition is the window argument. Over how many time periods should the trend/season be calculated. If one wishes to average over all periods, window=\"periodic\" is the necessary syntax.\n\n```{r}\nInflation.Expectations %>% \n  as_tsibble(index=date) %>% \n  model(STL(`Median one-year ahead expected inflation rate`)) %>%\n  components() %>% \n  autoplot()\n```"
  },
  {
    "objectID": "posts/week-10/index.html",
    "href": "posts/week-10/index.html",
    "title": "Week 10: Finishing Features, the Toolbox and Evaluation",
    "section": "",
    "text": "The slides are here..\nOur tenth class meeting will focus on Chapter 5 and Chapter 6 of Forecasting: Principles and Practice [3rd edition].\nAll of the data for today and for last time, including computations, can be acquired using\nand"
  },
  {
    "objectID": "posts/week-10/index.html#decompositions",
    "href": "posts/week-10/index.html#decompositions",
    "title": "Week 10: Finishing Features, the Toolbox and Evaluation",
    "section": "Decompositions",
    "text": "Decompositions\n\nClassical Decomposition\nThe key difference between the two decompositions, and there are others, is the existence [or lack thereof] of a window. In the classical decomposition, there is almost no flexibility. The flexibility is defined by the Specials. In this case, there is only one and it is the seasonal period [so there are basically none].\n\n\n\nSpecials for classical_decomposition\n\n\n\n```{r}\nInflation.Expectations %>% \n  as_tsibble(index=date) %>% \n  model(stl = classical_decomposition(`Median one-year ahead expected inflation rate` ~ season(12))) %>%\n  components() %>% \n  autoplot()\n```\n\n\n\n\n\n\nSTL Decomposition\nThe key new element to the STL decomposition is the window argument. Over how many time periods should the trend/season be calculated. If one wishes to average over all periods, window=\"periodic\" is the necessary syntax. For the STL decomposition, there are quite a few.\n\n\n\nSTL Specials\n\n\nSeason and trend are fairly intuitive. The lowpass filter, less so. First, the subseries is the individual seasonal period; in this case, there are 12 subseries because we have monthly data.\n\n```{r}\nInflation.Expectations %>% \n  as_tsibble(index=date) %>% \n  model(STL(`Median one-year ahead expected inflation rate` ~ season(12, window = \"periodic\"))) %>%\n  components() %>% \n  autoplot()\n```"
  },
  {
    "objectID": "posts/week-10/index.html#principal-components",
    "href": "posts/week-10/index.html#principal-components",
    "title": "Week 10: Finishing Features, the Toolbox and Evaluation",
    "section": "Principal Components",
    "text": "Principal Components\nOne way of deploying the full set of features is to utilize principal components on them to determine which series are most alike in relation to the others. Let’s try this with the employment data.\nFirst, let me take the version of the updated employment data and subject them to the full set of features.\n\n```{r}\nemployment_features <- US.Employment.T %>%\n  features(Employed, feature_set(pkgs = \"feasts\"))\n```\n\nNext, I want to take the features and subject them to principal components. There is one issue in doing this, to scale them, we will need to find the column(s) that have no implicit variation. Though there are 49 columns to the features, there are only 43 remaining columns after removing those with no variation. The apply part goes through and omits columns with zero variance. The 2 applies the function to columns and only keeps those with non-zero variance.\n\n```{r}\n# Clean up\nEF <- employment_features %>% select(-Title) %>% .[,apply(., 2, var, na.rm=TRUE) != 0]\nncol(employment_features)\nncol(EF)\n```\n\n[1] 49\n[1] 43\n\n\nNow to the scaling.\n\n```{r}\nlibrary(broom)\npcs <- EF %>%\n  prcomp(scale=TRUE) %>%\n  augment(EF)\npcs$Title <- employment_features$Title\n```\n\nThen I want to plot them. Unlike the example in the text, these do not have numerous categorical identifiers, I will use a plotly to get the titles of the series as the identifier to explore.\n\n```{r}\nplot1 <- pcs %>%\n  ggplot(aes(x = .fittedPC1, y = .fittedPC2, label = Title)) +\n  geom_point() +\n  theme_minimal()\nlibrary(plotly)\nggplotly(plot1)\n```\n\n\n\n\n\nThe common question exposed by principal components is expressed here. What are these components actually measuring? That is largely unknown except that the first principal component is the strongest axis of differentiation for the data. But what features are included in it? That is black boxed."
  },
  {
    "objectID": "posts/week-11/index.html",
    "href": "posts/week-11/index.html",
    "title": "Week 11: Models, Models, Models",
    "section": "",
    "text": "The slides are here..\nOur eleventh class meeting will focus on Chapter 7 Chapter 8 and Chapter 9 of Forecasting: Principles and Practice [3rd edition].\nAll of the data for today can be acquired using\n\n```{r}\nload(url(\"https://github.com/robertwwalker/xaringan/raw/main/CMF-Week-11/data/FullWorkspace.RData\"))\n```"
  },
  {
    "objectID": "posts/week-12/index.html",
    "href": "posts/week-12/index.html",
    "title": "Week 12: Models, Models, Models",
    "section": "",
    "text": "The slides are here..\nOur twelfth class meeting will focus on Chapter 9 of Forecasting: Principles and Practice [3rd edition]. We will then move to chapter 10 and chapter 11 and chapters 12 and 13.\nAll of the data for today can be acquired using\n\n```{r}\nload(url(\"https://github.com/robertwwalker/xaringan/raw/main/CMF-Week-11/data/FullWorkspace.RData\"))\n```"
  },
  {
    "objectID": "posts/week-13/index.html",
    "href": "posts/week-13/index.html",
    "title": "Week 13: Models, Models, Models, part 2",
    "section": "",
    "text": "The slides are here..\nOur thirteenth class meeting will focus on the end of Chapter 9 of Forecasting: Principles and Practice [3rd edition]. We will then move to chapter 10 and chapter 11 and chapters 12 and 13 to conclude our treatment of models of forecasting.\nAll of the data for today can be acquired using\n\n```{r}\nload(url(\"https://github.com/robertwwalker/xaringan/raw/main/CMF-Week-11/data/FullWorkspace.RData\"))\n```\n\nThe key topics:\n\nSeasonal ARIMA models.\nRegression with ARMA/ARIMA errors\nFourier terms [10.5]\nLagged predictors [and dependent variables]\nHierarchy and grouped time series\nBottom-up, top-down and middle-out\nreconcile\nComplex seasonality\nProphet\nNeural nets\nBagging and bootstrapping\nWeeks/days/sub-days\nCroston’s method and counts\nOutliers and missing values"
  },
  {
    "objectID": "posts/Example-Stock-Price/index.html",
    "href": "posts/Example-Stock-Price/index.html",
    "title": "A Forecasting Example",
    "section": "",
    "text": "I want to set up an example that runs completely from beginning to end working with stock market data establishing one approach to daily data. The book shows what happens if we want to model trading days as a sequence that doesn’t actually reflect time per se. I want to take a different approach. First, I will need some data. Let me work with Apple’s stock market data in OHLC format. I can get these from tidyquant.\n\n```{r}\nlibrary(tidyverse)\nlibrary(tidyquant)\nlibrary(fpp3)\n# Get Apple stock price\nAAPL <- tq_get(\"AAPL\")\n```\n\nNow I need to create the full sequence of dates that includes days that markets are closed. I want to left join the original data to this to get Apple’s data on a complete set of calendar dates as Full.Set.\n\n```{r}\n# Create unpopulated calendar\ncalendar.set <- data.frame(date=seq.Date(from=min(AAPL$date), to=max(AAPL$date), by=\"1 day\"))\n# Join together the calendar and AAPL OHLC data\nFull.Set <- left_join(calendar.set, AAPL)\n# Create 5 day moving average to model using slider::slide_dbl\nlibrary(slider)\nFull.Set <- Full.Set %>% mutate(MA5 = slide_dbl(close, mean, na.rm=TRUE, .before=4)) %>% as_tsibble(index=date)\n```\n\nFrom here, I can create a training set and a test set.\n\n```{r}\n# Split a training set before November 30, 2022\nTrain <- Full.Set %>% filter(date < \"2022-11-30\") %>% as_tsibble(index=date)\nTest <- anti_join(Full.Set, Train) %>% as_tsibble(index=date)\n# Plot the whole thing\nFull.Set %>% autoplot(MA5)\n```\n\n\n\n\nNow estimate two models.\n\n```{r}\nMods <- Train %>% model(AM5=ARIMA(MA5), ETS5=ETS(MA5))\nMods %>% glimpse()\nMods %>% glance()\n```\n\nRows: 1\nColumns: 2\n$ AM5  <model> [ARIMA(1,1,4)(0,0,1)[7] w/ drift]\n$ ETS5 <model> [ETS(M,Ad,N)]\n# A tibble: 2 × 11\n  .model    sigma2 log_lik    AIC   AICc    BIC ar_roots  ma_roots    MSE  AMSE\n  <chr>      <dbl>   <dbl>  <dbl>  <dbl>  <dbl> <list>    <list>    <dbl> <dbl>\n1 AM5    0.284      -2854.  5724.  5724.  5773. <cpl [1]> <cpl>    NA     NA   \n2 ETS5   0.0000447 -10610. 21231. 21231. 21268. <NULL>    <NULL>    0.331  1.17\n# … with 1 more variable: MAE <dbl>\n\n\nAs we can see, the ARIMA is of a single difference with 1 AR and 4 MA terms as well as a 7 day seasonal moving average term. The ETS contains no seasons, multiplicative errors, and additive trends.\n\n```{r}\n# Forecast for 30 days\nFC <- Mods %>% forecast(h=30)\n```\n\nHow well does it do?\n\n```{r}\n# Model Assessment\naccuracy(FC, Test)\n```\n\n# A tibble: 2 × 10\n  .model .type     ME  RMSE   MAE    MPE  MAPE  MASE RMSSE  ACF1\n  <chr>  <chr>  <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl>\n1 AM5    Test  -3.82   7.30  5.43 -2.94   4.04   NaN   NaN 0.914\n2 ETS5   Test   0.385  5.39  4.93  0.108  3.57   NaN   NaN 0.908\n\n\nThe ETS model seems to do better on this test set.\n\n```{r}\n# Show the forecast  needs alpha to see them\nPlot1 <- FC %>% autoplot(., alpha=0.2) + hrbrthemes::theme_ipsum_es() + labs(y=\"Forecast AAPL\")\nPlot1\n```\n\n\n\n\nThe ETS bends lower and this better fits the data over the last 30 days.\n\n```{r}\n# Show the forecast  needs alpha to see them\nPlot1 <- FC %>% autoplot(., alpha=0.4) + hrbrthemes::theme_ipsum_es() + labs(y=\"Forecast AAPL\") + geom_point(data=Test, aes(x=date, y=MA5), color=\"blue\", size=2, alpha=0.5) + guides(level=\"none\") \nPlot1\n```\n\n\n\n\nI don’t really gain much from showing the whole training set. It really is a lot of data.\n\n```{r}\n# Adapt the forecast to some data.\nPlot2 <- FC %>% autoplot(.) + geom_line(data=Train, aes(x=date, y=MA5)) + hrbrthemes::theme_ipsum_es() + labs(y=\"AAPL\")\nPlot2\n```\n\n\n\n\nA bit more plotting.\n\n```{r}\nlibrary(patchwork)\n# Plot1 + Plot2\n# Zoomed in.\nD2021P <- Full.Set %>% filter(date > \"2022-10-01\")\nPlot2 <- FC %>% autoplot(., alpha=0.2) + geom_line(data=D2021P, aes(x=date, y=MA5)) + hrbrthemes::theme_ipsum_es() + labs(y=\"AAPL\") + guides(level=\"none\")\nPlot1 + Plot2\n```\n\n\n\n\nShow it in zoom.\n\n```{r}\nPlot2\n```\n\n\n\n\n\nA Cross-Validation\n\n```{r}\napple_tr <- Full.Set %>%\n  stretch_tsibble(.init = 3350, .step = 30) %>%\n  relocate(date, .id)\napple_tr %>% model(AM5=ARIMA(MA5)) %>% \n  forecast(h=30) %>% \n  accuracy(Full.Set)\napple_tr %>% model(ETS5=ETS(MA5)) %>% \n  forecast(h=30) %>% \n  accuracy(Full.Set)\n```\n\n# A tibble: 1 × 10\n  .model .type    ME  RMSE   MAE   MPE  MAPE  MASE RMSSE  ACF1\n  <chr>  <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1 AM5    Test  -3.68  10.8  8.52 -2.62  5.63  5.02  3.80 0.956\n# A tibble: 1 × 10\n  .model .type    ME  RMSE   MAE   MPE  MAPE  MASE RMSSE  ACF1\n  <chr>  <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1 ETS5   Test  -2.97  9.98  7.70 -2.16  5.15  4.54  3.50 0.959\n\n\nThe ETS model seems to fit better across the cross-validated sets also. Some forecasts on the data.\n\n```{r}\nAFR <- apple_tr %>% model(AM5=ARIMA(MA5), ETS5=ETS(MA5)) %>% forecast(h=30)\n```\n\nPutting all the data together requires a bit of manipulating.\n\n```{r, width=\"900px\", height=\"600px\"}\nFS1 <- Full.Set %>% select(date, MA5) %>% mutate(Truth = MA5) %>% select(-MA5) %>% as_tsibble(index=date)\nAFR2 <- AFR %>% left_join(., FS1) %>% filter(.id<11)\nAFR2 %>% autoplot(alpha=0.7) + \nfacet_wrap(vars(.id), scales = \"free_x\") + hrbrthemes::theme_ipsum_rc() + guides(level=\"none\")\n```\n\n\n\n\nThis is a solid view of the forecasts.\n\n```{r}\nAFR2 %>% autoplot(alpha=0.5) +  facet_wrap(vars(.id), scales = \"free_x\") + hrbrthemes::theme_ipsum_rc() + guides(level=\"none\") + geom_line(data=AFR2, aes(x=date, y=Truth), size=1, alpha=0.2)\n```\n\n\n\n\n\n```{r}\nAFR2 %>% filter(.id==1) %>% autoplot(alpha=0.5) + hrbrthemes::theme_ipsum_rc() + guides(level=\"none\") + geom_line(data={AFR2 %>% filter(.id==1)}, aes(x=date, y=Truth), size=1, alpha=0.2)\n```\n\n\n\n\n\n```{r}\nAFR2 %>% filter(.id==2) %>% autoplot(alpha=0.5) + hrbrthemes::theme_ipsum_rc() + guides(level=\"none\") + geom_line(data={AFR2 %>% filter(.id==2)}, aes(x=date, y=Truth), size=1, alpha=0.2)\n```\n\n\n\n\n\n```{r}\nAFR2 %>% filter(.id==3) %>% autoplot(alpha=0.5) + hrbrthemes::theme_ipsum_rc() + guides(level=\"none\") + geom_line(data={AFR2 %>% filter(.id==3)}, aes(x=date, y=Truth), size=1, alpha=0.2)\n```\n\n\n\n\n\n```{r}\nAFR2 %>% filter(.id==4) %>% autoplot(alpha=0.5) + hrbrthemes::theme_ipsum_rc() + guides(level=\"none\") + geom_line(data={AFR2 %>% filter(.id==4)}, aes(x=date, y=Truth), size=1, alpha=0.2)\n```\n\n\n\n\n\n```{r}\nAFR2 %>% filter(.id==5) %>% autoplot(alpha=0.5) + hrbrthemes::theme_ipsum_rc() + guides(level=\"none\") + geom_line(data={AFR2 %>% filter(.id==5)}, aes(x=date, y=Truth), size=1, alpha=0.2)\n```\n\n\n\n\n\n```{r}\nAFR2 %>% filter(.id==6) %>% autoplot(alpha=0.5) + hrbrthemes::theme_ipsum_rc() + guides(level=\"none\") + geom_line(data={AFR2 %>% filter(.id==6)}, aes(x=date, y=Truth), size=1, alpha=0.2)\n```"
  }
]