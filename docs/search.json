[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "TLDR: The course is what I think of as the natural successor for Data Analysis, Modeling, and Decision Making that extends basic regression to a broader universe of data types and the particular troubles in forecasting data observed over time.\nIt relies on two texts that exist in both paper and digital [with free access] forms. The first half of the course is built around The Handbook of Regression Modeling in People Analytics: With Examples in R, Python, and Julia by Keith McNulty [Global Director of Talent Sciences at McKinsey and Company] that covers models for discrete data types [binary, ordered, nominal choices, counts of events, and survival/duration analysis].\nThe second half of the course relies on the excellent Forecasting, Principles and Practice, 3rd Edition, by Rob J. Hyndman and George Athanasopoulos of Monash University in Australia that is entirely supported by R libraries for time series problems.\nThe lectures will focus on intuition and the mathematical logic but the goal is to put the tools into practice. To this end, the expectations are weekly homework exercises to insure that we can actually do what we are presented but there are two key summary deliverables: a project employing a detailed application of choice models near the middle and a project in time series forecasting due at the end of the term. Both are to be presented at the end of the term on date to be arranged to make up for the cancellation the first week."
  },
  {
    "objectID": "about.html#instructor",
    "href": "about.html#instructor",
    "title": "About",
    "section": "Instructor",
    "text": "Instructor\nRobert W. Walker is Associate Professor of Quantitative Methods in the Atkinson Graduate School of Management at Willamette University. He earned a Ph. D. in political science from the University of Rochester in 2005 and has previously held teaching positions at Dartmouth College, Rice University, Texas A&M University, and Washington University in Saint Louis. His current research develops and applies semi-Markov processes to time-series, cross-section data in international relations and international/comparative political economy. He teaches courses in quantitative methods/applied statistics and microeconomic strategy and previously taught four iterations in the U. S. National Science Foundation funded Empirical Implications of Theoretical Models sequence at Washington University in Saint Louis. His work with Curt Signorino and Muhammet Bas was awarded the Miller Prize for the best article in Political Analysis in 2009."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Models of Choice and Forecasting",
    "section": "",
    "text": "R\n\n\n\n\n\n\n\nRobert W. Walker\n\n\nSep 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\nRobert W. Walker\n\n\nSep 6, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\nRobert W. Walker\n\n\nSep 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\nRobert W. Walker\n\n\nAug 29, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\nRobert W. Walker\n\n\nAug 26, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/week-1/index.html",
    "href": "posts/week-1/index.html",
    "title": "Week 1: R, Inference, and Regression: A Review",
    "section": "",
    "text": "The slides are here. The first class video is available from youtube here\nOur first class meeting will focus on Chapters 1, 2, 3; I suspect we will leave Chapter 4 of Handbook of Regression Modeling in People Analytics for next time.\nUPDATE: We got through Chapters 1 and 2. 3 and 4 will come next meeting."
  },
  {
    "objectID": "posts/week-1/index.html#hypothesis-tests-and-confidence-intervals",
    "href": "posts/week-1/index.html#hypothesis-tests-and-confidence-intervals",
    "title": "Week 1: R, Inference, and Regression: A Review",
    "section": "Hypothesis Tests and Confidence Intervals",
    "text": "Hypothesis Tests and Confidence Intervals"
  },
  {
    "objectID": "posts/week-1/index.html#an-hypothesis-test",
    "href": "posts/week-1/index.html#an-hypothesis-test",
    "title": "Week 1: R, Inference, and Regression: A Review",
    "section": "An Hypothesis Test",
    "text": "An Hypothesis Test\nI will work with the speed variable. The hypothesis to advance is that 17 or greater is the true average speed. The alternative must then be that the average speed is less than 17. Knowing only the sample size, I can figure out what \\(t\\) must be to reject 17 or greater and conclude that the true average must be less with 90% probability. The sample mean would have to be at least qt(0.1, 49) standard errors below 17 to rule out a mean of 17 or greater. Now let’s see what we have. Let me skim the data for the relevant information.\n\nlibrary(skimr)\nskim(cars)\n\n\nData summary\n\n\nName\ncars\n\n\nNumber of rows\n50\n\n\nNumber of columns\n2\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nspeed\n0\n1\n15.40\n5.29\n4\n12\n15\n19\n25\n▂▅▇▇▃\n\n\ndist\n0\n1\n42.98\n25.77\n2\n26\n36\n56\n120\n▅▇▅▂▁\n\n\n\n\n\nDoing the math by hand, I get:\n\\[ t = \\frac{\\overline{x} - \\mu}{\\frac{s}{\\sqrt{n}}} = \\frac{15.4 - 17}{\\frac{5.29}{\\sqrt{50}}} = -2.14 \\]\nInterpreting the result, the sample mean is 2.14 standard errors below the hypothetical mean of 17. The probability of a sample mean of 15.4 [or smaller] given a true average of 17, this standard deviation and sample size is pt(-2.14, 49) = 0.0186798. Notice that probability is less than 0.1; thus with at least 90% confidence, the true mean is not 17 or greater and thus must be smaller. Assuming the hypothetical mean [17 or greater] is true, the likelihood of generating a sample mean of 15.4 is only 0.0187 and this is far less than the 10% permissible outside of 90% confidence. Indeed, any sample mean more than 1.299 standard errors below 17 would be too small to sustain the belief that the true mean is 17 or greater because qt(0.1, 49) is -1.299. Put in the original metric, any sample mean below 16.0285747 would require a rejection of the claim that the true mean is 17 or greater with 90% confidence."
  },
  {
    "objectID": "posts/week-1/index.html#the-confidence-interval",
    "href": "posts/week-1/index.html#the-confidence-interval",
    "title": "Week 1: R, Inference, and Regression: A Review",
    "section": "The Confidence Interval",
    "text": "The Confidence Interval\nThe confidence interval is always centered on the sample mean. Rearranging the equation above and solving for \\(\\mu\\) given the \\(t\\) above, we get\n\\[ \\mu = \\overline{x} - t(\\frac{s}{\\sqrt{n}}) = 15.4 - (-1.299*\\frac{5.29}{\\sqrt{50}}) = 16.37143 \\]\nWith 90% confidence, given this sample mean, the true value should be less than 16.37143."
  },
  {
    "objectID": "posts/week-1/index.html#the-native-t.test",
    "href": "posts/week-1/index.html#the-native-t.test",
    "title": "Week 1: R, Inference, and Regression: A Review",
    "section": "The native t.test",
    "text": "The native t.test\n\nt.test(cars$speed, conf.level = 0.9, alternative = \"less\", mu=17)\n\n\n    One Sample t-test\n\ndata:  cars$speed\nt = -2.1397, df = 49, p-value = 0.01869\nalternative hypothesis: true mean is less than 17\n90 percent confidence interval:\n     -Inf 16.37143\nsample estimates:\nmean of x \n     15.4"
  },
  {
    "objectID": "posts/week-1/index.html#simplifying",
    "href": "posts/week-1/index.html#simplifying",
    "title": "Week 1: R, Inference, and Regression: A Review",
    "section": "Simplifying?",
    "text": "Simplifying?\n\\[ t(\\frac{s}{\\sqrt{n}}) = \\overline{x} - \\mu \\] can lead to either:\n\\[  \\overline{x} - t(\\frac{s}{\\sqrt{n}}) = \\mu \\]\nor\n\\[ \\overline{x} = \\mu + t(\\frac{s}{\\sqrt{n}}) \\]\nSo a minus \\(t\\) will be below \\(\\mu\\) but above \\(\\overline{x}\\) and a positive \\(t\\) will be above \\(\\mu\\) but below \\(\\overline{x}\\).\n1. An hypothesis test given \\(\\mu\\) with an alternative that is less must then render an upper bound given \\(\\overline{x}\\).\n2. An hypothesis test given \\(\\mu\\) with an alternative that is greater must then render a lower bound given \\(\\overline{x}\\)."
  },
  {
    "objectID": "posts/week-1/index.html#a-graphical-representation",
    "href": "posts/week-1/index.html#a-graphical-representation",
    "title": "Week 1: R, Inference, and Regression: A Review",
    "section": "A graphical representation",
    "text": "A graphical representation\nGiven a sample size \\(n\\), some unknown constant \\(\\mu\\) and satisfaction of Lindeberg’s condition, the sampling distribution of the sample mean follows a \\(t\\) distribution with degrees of freedom \\(n-1\\). To render a graphical representation, let’s arbitrarily set n to 50, as in the above example. Here is a plot.\n\nplot(seq(-5,5, by=0.01), dt(seq(-5,5, by=0.01), df=49), xlab=expression(paste(\"x-bar -\",mu,\" (measured in std. errors of the mean)\", sep=\"\")), ylab=\"Density\", type=\"l\")\n\n\n\n\n\nInverting the scale transformation\nWe can now reverse the scale by the standard error of the mean. In the above example, it is 0.7478. Measured in miles per hour, we obtain:\n\nplot(seq(-5,5, by=0.01)*0.7478, dt(seq(-5,5, by=0.01), df=49), xlab=expression(paste(\"x-bar -\",mu,\" (measured in mph)\", sep=\"\")), ylab=\"Density\", type=\"l\")\n\n\n\n\nNow we will take the concrete example above.\n\n\nThe Hypothesis Test\nWe claim that the true mean is 17 or greater. Now we need center the distribution above as though the claim is true.\n\nplot(x=17+seq(-5,5, by=0.01)*0.7478, dt(seq(-5,5, by=0.01), df=49), xlab=expression(paste(mu,\" (measured in mph)\", sep=\"\")), ylab=\"Density\", type=\"l\")\nabline(v=17, col=\"red\")\npolygon(x = c(17+seq(0,5, by=0.01)*0.7478, 21), y = c(0, dt(seq(0,5, by=0.01), df=49)), col = \"red\")\n\n\n\n\nThe sample mean is estimated to be 15.4. How likely is that?\n\nplot(x=17+seq(-5,5, by=0.01)*0.7478, dt(seq(-5,5, by=0.01), df=49), xlab=expression(paste(\"x-bar -\",mu,\" (measured in mph)\", sep=\"\")), ylab=\"Density\", type=\"l\")\nabline(v=17, col=\"red\")\nabline(v=15.4, col=\"blue\")\npolygon(x = c(17+seq(0,5, by=0.01)*0.7478, 21), y = c(0, dt(seq(0,5, by=0.01), df=49)), col = \"red\")\npolygon(x = c(12, 17+seq(-5,-2.14, by=0.01)*0.7478), y = c(dt(seq(-5,-2.14, by=0.01), df=49), 0), col = \"blue\")\nabline(h=0, col=\"black\")\nabline(v=17 + qt(0.1, df=49)*0.7874, col=\"black\", lty=3)\n\n\n\n\nThe probability of seeing such a small sample mean if the true average is 17 is only 0.01869. The probability above the dotted black line is 0.9 with 0.1 below. WIth 90% confidence, anything below this would be sufficient evidence to reject the claim that the true average is 17 or above."
  },
  {
    "objectID": "posts/week-1/index.html#the-confidence-interval-1",
    "href": "posts/week-1/index.html#the-confidence-interval-1",
    "title": "Week 1: R, Inference, and Regression: A Review",
    "section": "The Confidence Interval",
    "text": "The Confidence Interval\nLet’s take the sample mean as the center and work out a confidence interval at 90%. It’s exactly the 16.37143 gives above.\n\nplot(x=15.4+seq(-5,5, by=0.01)*0.7478, dt(seq(-5,5, by=0.01), df=49), xlab=expression(paste(mu,\" | x-bar (measured in mph)\", sep=\"\")), ylab=\"Density\", type=\"l\")\nabline(v=15.4, col=\"blue\")\nabline(v=15.4 - qt(0.1, df=49)*0.7478, col=\"black\", lty=3)\npolygon(x = c(11, 15.4+seq(-5,1.3, by=0.01)*0.7478), y = c(dt(seq(-5,1.3, by=0.01), df=49), 0), col = \"blue\")\n\n\n\n\nAs an aside, 17 has exactly 0.01869 probability above it shown in orange.\n\nplot(x=15.4+seq(-5,5, by=0.01)*0.7478, dt(seq(-5,5, by=0.01), df=49), xlab=expression(paste(mu,\" | x-bar (measured in mph)\", sep=\"\")), ylab=\"Density\", type=\"l\")\nabline(v=15.4, col=\"blue\")\nabline(v=15.4 - qt(0.1, df=49)*0.7478, col=\"black\", lty=3)\npolygon(x = c(11, 15.4+seq(-5,1.3, by=0.01)*0.7478), y = c(dt(seq(-5,1.3, by=0.01), df=49), 0), col = \"blue\")\npolygon(x = c(15.4+seq(2.14,5, by=0.01)*0.7478, 17), y = c(dt(seq(2.14,5, by=0.01), df=49), 0), col = \"orange\")"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome to Models of Choice and Forecasting",
    "section": "",
    "text": "I hope you are ready! The syllabus can be found here.\nWe will work with two books.\nTo hit the ground running, you will probably need two key bits of software: R and RStudio. To wit,\nThe tentative reading plan is:"
  },
  {
    "objectID": "posts/welcome/index.html#people-analytics",
    "href": "posts/welcome/index.html#people-analytics",
    "title": "Welcome to Models of Choice and Forecasting",
    "section": "People Analytics",
    "text": "People Analytics\n\nWeeks 1 and 2: Review of Linear Models and Inferential Statistics [chapters 1-4]\nWeek 3: Binomial Logistic Regression [chapter 5]\nWeek 4: Ordered and Multinomial Logistic Regression [chapters 6 and 7]\nWeek 5: Hierarchical Data [chapter 8]\nWeek 6: Survival Analysis [chapter 9]\nWeek 7: Power Analysis: How much data do I need? and Review [chapter 10]"
  },
  {
    "objectID": "posts/welcome/index.html#forecasting",
    "href": "posts/welcome/index.html#forecasting",
    "title": "Welcome to Models of Choice and Forecasting",
    "section": "Forecasting",
    "text": "Forecasting\n\nWeeks 8 and 9: The Basics, Time as a Variable, and Decomposition [chapters 1-5]\nWeek 10: Judgemental Forecast and Regression\n[chapters 6 and 7]\nWeek 11: Exponential Smoothing and ARIMA\n[chapters 8 and 9]\nWeek 12: Dynamic Regression [chapter 10]\nWeek 13: Hierarchies, advanced forecasting and related issues\n[chapters 11-13]\nWeek 14: Presentations on a people analytics problem and a time series forecast Date TBA: We will have to reschedule this because it represents the cancelled class the first week."
  },
  {
    "objectID": "posts/week-2/index.html",
    "href": "posts/week-2/index.html",
    "title": "Week 2: Inference and Regression: A Review",
    "section": "",
    "text": "The slides are here.. The video is linked here.\nOur second class meeting will focus on Chapter 3; and Chapter 4 of Handbook of Regression Modeling in People Analytics for next time."
  },
  {
    "objectID": "posts/week-2/index.html#hypothesis-tests-and-confidence-intervals",
    "href": "posts/week-2/index.html#hypothesis-tests-and-confidence-intervals",
    "title": "Week 2: Inference and Regression: A Review",
    "section": "Hypothesis Tests and Confidence Intervals",
    "text": "Hypothesis Tests and Confidence Intervals\n\nSingle means with the cars data\nI will work with R’s internal dataset on cars: cars. There are two variables in the dataset, this is what they look like.\n\nplot(cars)"
  },
  {
    "objectID": "posts/week-2/index.html#an-hypothesis-test",
    "href": "posts/week-2/index.html#an-hypothesis-test",
    "title": "Week 2: Inference and Regression: A Review",
    "section": "An Hypothesis Test",
    "text": "An Hypothesis Test\nI will work with the speed variable. The hypothesis to advance is that 17 or greater is the true average speed. The alternative must then be that the average speed is less than 17. Knowing only the sample size, I can figure out what \\(t\\) must be to reject 17 or greater and conclude that the true average must be less with 90% probability. The sample mean would have to be at least qt(0.1, 49) standard errors below 17 to rule out a mean of 17 or greater. Now let’s see what we have. Let me skim the data for the relevant information.\n\nlibrary(skimr)\nskim(cars)\n\n\nData summary\n\n\nName\ncars\n\n\nNumber of rows\n50\n\n\nNumber of columns\n2\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nspeed\n0\n1\n15.40\n5.29\n4\n12\n15\n19\n25\n▂▅▇▇▃\n\n\ndist\n0\n1\n42.98\n25.77\n2\n26\n36\n56\n120\n▅▇▅▂▁\n\n\n\n\n\nDoing the math by hand, I get:\n\\[ t = \\frac{\\overline{x} - \\mu}{\\frac{s}{\\sqrt{n}}} = \\frac{15.4 - 17}{\\frac{5.29}{\\sqrt{50}}} = -2.14 \\]\nInterpreting the result, the sample mean is 2.14 standard errors below the hypothetical mean of 17. The probability of a sample mean of 15.4 [or smaller] given a true average of 17, this standard deviation and sample size is pt(-2.14, 49) = 0.0186798. Notice that probability is less than 0.1; thus with at least 90% confidence, the true mean is not 17 or greater and thus must be smaller. Assuming the hypothetical mean [17 or greater] is true, the likelihood of generating a sample mean of 15.4 is only 0.0187 and this is far less than the 10% permissible outside of 90% confidence. Indeed, any sample mean more than 1.299 standard errors below 17 would be too small to sustain the belief that the true mean is 17 or greater because qt(0.1, 49) is -1.299. Put in the original metric, any sample mean below 16.0285747 would require a rejection of the claim that the true mean is 17 or greater with 90% confidence."
  },
  {
    "objectID": "posts/week-2/index.html#the-confidence-interval",
    "href": "posts/week-2/index.html#the-confidence-interval",
    "title": "Week 2: Inference and Regression: A Review",
    "section": "The Confidence Interval",
    "text": "The Confidence Interval\nThe confidence interval is always centered on the sample mean. Rearranging the equation above and solving for \\(\\mu\\) given the \\(t\\) above, we get\n\\[ \\mu = \\overline{x} - t(\\frac{s}{\\sqrt{n}}) = 15.4 - (-1.299*\\frac{5.29}{\\sqrt{50}}) = 16.37143 \\]\nWith 90% confidence, given this sample mean, the true value should be less than 16.37143."
  },
  {
    "objectID": "posts/week-2/index.html#the-native-t.test",
    "href": "posts/week-2/index.html#the-native-t.test",
    "title": "Week 2: Inference and Regression: A Review",
    "section": "The native t.test",
    "text": "The native t.test\n\nt.test(cars$speed, conf.level = 0.9, alternative = \"less\", mu=17)\n\n\n    One Sample t-test\n\ndata:  cars$speed\nt = -2.1397, df = 49, p-value = 0.01869\nalternative hypothesis: true mean is less than 17\n90 percent confidence interval:\n     -Inf 16.37143\nsample estimates:\nmean of x \n     15.4"
  },
  {
    "objectID": "posts/week-2/index.html#simplifying",
    "href": "posts/week-2/index.html#simplifying",
    "title": "Week 2: Inference and Regression: A Review",
    "section": "Simplifying?",
    "text": "Simplifying?\n\\[ t(\\frac{s}{\\sqrt{n}}) = \\overline{x} - \\mu \\] can lead to either:\n\\[  \\overline{x} - t(\\frac{s}{\\sqrt{n}}) = \\mu \\]\nor\n\\[ \\overline{x} = \\mu + t(\\frac{s}{\\sqrt{n}}) \\]\nSo a minus \\(t\\) will be below \\(\\mu\\) but above \\(\\overline{x}\\) and a positive \\(t\\) will be above \\(\\mu\\) but below \\(\\overline{x}\\).\n1. An hypothesis test given \\(\\mu\\) with an alternative that is less must then render an upper bound given \\(\\overline{x}\\).\n2. An hypothesis test given \\(\\mu\\) with an alternative that is greater must then render a lower bound given \\(\\overline{x}\\)."
  },
  {
    "objectID": "posts/week-2/index.html#a-graphical-representation",
    "href": "posts/week-2/index.html#a-graphical-representation",
    "title": "Week 2: Inference and Regression: A Review",
    "section": "A graphical representation",
    "text": "A graphical representation\nGiven a sample size \\(n\\), some unknown constant \\(\\mu\\) and satisfaction of Lindeberg’s condition, the sampling distribution of the sample mean follows a \\(t\\) distribution with degrees of freedom \\(n-1\\). To render a graphical representation, let’s arbitrarily set n to 50, as in the above example. Here is a plot.\n\nplot(seq(-5,5, by=0.01), dt(seq(-5,5, by=0.01), df=49), xlab=expression(paste(\"x-bar -\",mu,\" (measured in std. errors of the mean)\", sep=\"\")), ylab=\"Density\", type=\"l\")\n\n\n\n\n\nInverting the scale transformation\nWe can now reverse the scale by the standard error of the mean. In the above example, it is 0.7478. Measured in miles per hour, we obtain:\n\nplot(seq(-5,5, by=0.01)*0.7478, dt(seq(-5,5, by=0.01), df=49), xlab=expression(paste(\"x-bar -\",mu,\" (measured in mph)\", sep=\"\")), ylab=\"Density\", type=\"l\")\n\n\n\n\nNow we will take the concrete example above.\n\n\nThe Hypothesis Test\nWe claim that the true mean is 17 or greater. Now we need center the distribution above as though the claim is true.\n\nplot(x=17+seq(-5,5, by=0.01)*0.7478, dt(seq(-5,5, by=0.01), df=49), xlab=expression(paste(mu,\" (measured in mph)\", sep=\"\")), ylab=\"Density\", type=\"l\")\nabline(v=17, col=\"red\")\npolygon(x = c(17+seq(0,5, by=0.01)*0.7478, 21), y = c(0, dt(seq(0,5, by=0.01), df=49)), col = \"red\")\n\n\n\n\nThe sample mean is estimated to be 15.4. How likely is that?\n\nplot(x=17+seq(-5,5, by=0.01)*0.7478, dt(seq(-5,5, by=0.01), df=49), xlab=expression(paste(\"x-bar -\",mu,\" (measured in mph)\", sep=\"\")), ylab=\"Density\", type=\"l\")\nabline(v=17, col=\"red\")\nabline(v=15.4, col=\"blue\")\npolygon(x = c(17+seq(0,5, by=0.01)*0.7478, 21), y = c(0, dt(seq(0,5, by=0.01), df=49)), col = \"red\")\npolygon(x = c(12, 17+seq(-5,-2.14, by=0.01)*0.7478), y = c(dt(seq(-5,-2.14, by=0.01), df=49), 0), col = \"blue\")\nabline(h=0, col=\"black\")\nabline(v=17 + qt(0.1, df=49)*0.7874, col=\"black\", lty=3)\n\n\n\n\nThe probability of seeing such a small sample mean if the true average is 17 is only 0.01869. The probability above the dotted black line is 0.9 with 0.1 below. WIth 90% confidence, anything below this would be sufficient evidence to reject the claim that the true average is 17 or above."
  },
  {
    "objectID": "posts/week-2/index.html#the-confidence-interval-1",
    "href": "posts/week-2/index.html#the-confidence-interval-1",
    "title": "Week 2: Inference and Regression: A Review",
    "section": "The Confidence Interval",
    "text": "The Confidence Interval\nLet’s take the sample mean as the center and work out a confidence interval at 90%. It’s exactly the 16.37143 gives above.\n\nplot(x=15.4+seq(-5,5, by=0.01)*0.7478, dt(seq(-5,5, by=0.01), df=49), xlab=expression(paste(mu,\" | x-bar (measured in mph)\", sep=\"\")), ylab=\"Density\", type=\"l\")\nabline(v=15.4, col=\"blue\")\nabline(v=15.4 - qt(0.1, df=49)*0.7478, col=\"black\", lty=3)\npolygon(x = c(11, 15.4+seq(-5,1.3, by=0.01)*0.7478), y = c(dt(seq(-5,1.3, by=0.01), df=49), 0), col = \"blue\")\n\n\n\n\nAs an aside, 17 has exactly 0.01869 probability above it shown in orange.\n\nplot(x=15.4+seq(-5,5, by=0.01)*0.7478, dt(seq(-5,5, by=0.01), df=49), xlab=expression(paste(mu,\" | x-bar (measured in mph)\", sep=\"\")), ylab=\"Density\", type=\"l\")\nabline(v=15.4, col=\"blue\")\nabline(v=15.4 - qt(0.1, df=49)*0.7478, col=\"black\", lty=3)\npolygon(x = c(11, 15.4+seq(-5,1.3, by=0.01)*0.7478), y = c(dt(seq(-5,1.3, by=0.01), df=49), 0), col = \"blue\")\npolygon(x = c(15.4+seq(2.14,5, by=0.01)*0.7478, 17), y = c(dt(seq(2.14,5, by=0.01), df=49), 0), col = \"orange\")"
  },
  {
    "objectID": "posts/week-2/index.html#a-visual-of-the-data",
    "href": "posts/week-2/index.html#a-visual-of-the-data",
    "title": "Week 2: Inference and Regression: A Review",
    "section": "A Visual of the Data",
    "text": "A Visual of the Data\n\nlibrary(GGally)\n\nLoading required package: ggplot2\n\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\n# display a pairplot of all four columns of data\nGGally::ggpairs(ugtests)\n\n\n\n\n\nmy.lm <- lm(Final ~ Yr1 + Yr2 + Yr3, data=ugtests)\nsummary(my.lm)\n\n\nCall:\nlm(formula = Final ~ Yr1 + Yr2 + Yr3, data = ugtests)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-92.638 -20.349   0.001  18.954  98.489 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 14.14599    5.48006   2.581  0.00999 ** \nYr1          0.07603    0.06538   1.163  0.24519    \nYr2          0.43129    0.03251  13.267  < 2e-16 ***\nYr3          0.86568    0.02914  29.710  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 30.43 on 971 degrees of freedom\nMultiple R-squared:  0.5303,    Adjusted R-squared:  0.5289 \nF-statistic: 365.5 on 3 and 971 DF,  p-value: < 2.2e-16\n\n\n\nconfint(my.lm)\n\n                  2.5 %     97.5 %\n(Intercept)  3.39187185 24.9001071\nYr1         -0.05227936  0.2043318\nYr2          0.36749170  0.4950791\nYr3          0.80850142  0.9228610\n\n\n\npredict(my.lm)\n\n        1         2         3         4         5         6         7         8 \n 82.77839 173.39734 159.84579 148.02242 115.77826 176.31713 168.52573 125.90895 \n        9        10        11        12        13        14        15        16 \n163.15331 133.00197 128.01391 168.83298 167.28471 178.01432 162.81842 183.81939 \n       17        18        19        20        21        22        23        24 \n143.96109 175.00126 148.29571 183.06708 157.92531 119.95460 165.18879 178.26106 \n       25        26        27        28        29        30        31        32 \n193.06715 166.52931 200.39004 182.94018 181.82752 104.19713 146.86195 158.59539 \n       33        34        35        36        37        38        39        40 \n137.30246 155.14171 147.59592 188.38693 149.46722 189.57199 165.36883 207.92058 \n       41        42        43        44        45        46        47        48 \n158.81869 145.13679 127.04145 184.89905 223.48248 190.68129 105.91152 188.47370 \n       49        50        51        52        53        54        55        56 \n169.91737 125.98673 179.91299 155.57364  94.78176 107.90209 142.01859 127.27405 \n       57        58        59        60        61        62        63        64 \n143.04734  90.43469 115.62032 186.36874 164.78997 182.04486 138.72937 173.80034 \n       65        66        67        68        69        70        71        72 \n162.81500 110.89761 157.53103 171.04054 164.29372 151.36275 203.73632 133.85978 \n       73        74        75        76        77        78        79        80 \n158.14239 145.03931 114.09836 136.39042 150.19917  99.56150 145.66277 116.39753 \n       81        82        83        84        85        86        87        88 \n124.72053 138.48918 167.84005 102.19417 187.51815 148.69592 199.82845 146.45894 \n       89        90        91        92        93        94        95        96 \n123.73077 134.25546 133.56188 171.39099 148.68036 160.00402 141.50056 143.96279 \n       97        98        99       100       101       102       103       104 \n126.22552 146.13759 160.82167 168.19393 139.15162 126.63646 142.87183 209.15707 \n      105       106       107       108       109       110       111       112 \n161.60200 169.65369 101.21716 132.64506 133.35245  98.58109 104.99919 122.47906 \n      113       114       115       116       117       118       119       120 \n132.50088 135.04371 169.26422 156.66084 201.79068 102.50589 222.36274 137.85639 \n      121       122       123       124       125       126       127       128 \n117.81825  92.61602 166.14124  90.81172 155.28613 221.52163 117.23941 125.32530 \n      129       130       131       132       133       134       135       136 \n154.28567 125.02463 149.23229 188.75433 167.65071 141.48812 148.73942 128.18946 \n      137       138       139       140       141       142       143       144 \n173.30410 113.50085 146.77068 144.70245 143.32800 172.42426 146.83111 161.80039 \n      145       146       147       148       149       150       151       152 \n155.74509 121.45958 120.19341 196.55029 107.52819 145.84903 129.87730  84.26017 \n      153       154       155       156       157       158       159       160 \n147.22201 172.38103 148.90671 191.37033  92.95881 113.93839 115.32834 217.95006 \n      161       162       163       164       165       166       167       168 \n109.04391 182.47442 173.66693 143.54644 213.24494 154.53040 157.99341 168.20918 \n      169       170       171       172       173       174       175       176 \n142.79439 111.22351 169.00364  95.59458 200.50023 150.77458 103.15936 136.32932 \n      177       178       179       180       181       182       183       184 \n152.35701 121.13371 116.24995 172.44290 105.74986 144.68854 142.16437 103.03841 \n      185       186       187       188       189       190       191       192 \n 84.20453 124.65866 175.16434 181.48777 183.17732 166.12881 109.68631 149.19871 \n      193       194       195       196       197       198       199       200 \n148.99240 123.27911 156.46977 111.37381 133.40676 105.70352 178.22203 162.98116 \n      201       202       203       204       205       206       207       208 \n 88.71579 142.38459 126.29081 195.54808 121.88463 175.31327 171.16320 128.80527 \n      209       210       211       212       213       214       215       216 \n157.38069 144.47601 145.67519 157.45049  90.83488 125.04501 117.56217 166.65784 \n      217       218       219       220       221       222       223       224 \n142.60365 151.28361 138.80714 152.33864 177.32217 135.09624 169.95920 161.33378 \n      225       226       227       228       229       230       231       232 \n150.99366 167.63206 102.71562 110.43832 159.52327 151.78922 158.52699 186.41680 \n      233       234       235       236       237       238       239       240 \n124.74060 118.60478 162.54510 198.55014 123.77119 209.05134 207.10887 110.39340 \n      241       242       243       244       245       246       247       248 \n139.38280 168.02915 166.40038 219.52660 144.11283 160.18067 182.99728 151.63440 \n      249       250       251       252       253       254       255       256 \n152.04702 175.37858 153.20573 193.75934 142.24807 204.76959 147.85201  91.68673 \n      257       258       259       260       261       262       263       264 \n178.87067 164.06869 167.01479 172.92703 123.77229 175.05272 117.60429 111.97751 \n      265       266       267       268       269       270       271       272 \n159.97603 166.63812 173.31198 207.68459 140.66875 135.57418 176.71423 170.00300 \n      273       274       275       276       277       278       279       280 \n170.04457 177.26620 133.02378 146.19662 166.87042 163.96474 156.38611 160.34511 \n      281       282       283       284       285       286       287       288 \n 71.51067 192.31939  89.27563 146.58132 139.44811 105.11565  79.53330 121.12299 \n      289       290       291       292       293       294       295       296 \n110.14355 178.41312 158.55532 181.01467 203.41524 207.36041 108.01030 173.06385 \n      297       298       299       300       301       302       303       304 \n146.09265 128.71230 116.85816 216.44539 169.53445 124.77337 164.95305 190.56794 \n      305       306       307       308       309       310       311       312 \n129.06273 142.52591  95.61497 171.09510 188.31374  93.51730 117.69757 121.30134 \n      313       314       315       316       317       318       319       320 \n178.57730 157.74528 170.68833 120.24799 116.37542 164.43192 175.61114 220.06158 \n      321       322       323       324       325       326       327       328 \n181.74555 160.98583  86.27842 195.97205 136.46895 102.86908 220.53640 125.83914 \n      329       330       331       332       333       334       335       336 \n186.27545 176.93132 178.40037 148.32790 181.84927 133.17072 167.12163 191.16061 \n      337       338       339       340       341       342       343       344 \n167.95000 211.35983 163.46813  96.61852 151.47610 139.82344 138.38826 176.47513 \n      345       346       347       348       349       350       351       352 \n106.40781 158.88878 179.08940 148.79682 143.06119  93.72813 134.88542 153.31118 \n      353       354       355       356       357       358       359       360 \n116.84713 186.85398 177.47107 142.76953 163.52582 167.98283 177.39367 213.53039 \n      361       362       363       364       365       366       367       368 \n161.02175 129.52203 179.97033 161.03589 136.94095 127.20454 159.24064 152.34627 \n      369       370       371       372       373       374       375       376 \n188.04240 190.15391 181.43039 132.62329 167.16688 157.13535 200.36373 123.08217 \n      377       378       379       380       381       382       383       384 \n 95.83962 182.55975 167.50830  76.89241 140.75411 116.13802 108.41218 171.20843 \n      385       386       387       388       389       390       391       392 \n108.95821 159.18156 148.49892 109.31069 139.47722 178.64745 165.07685 175.29912 \n      393       394       395       396       397       398       399       400 \n157.02854 160.68996 198.45546 120.21546 148.23662 146.56264 168.47318 172.63256 \n      401       402       403       404       405       406       407       408 \n199.19678 148.82619 111.56629 159.27455  77.41079 130.80692 104.47776 175.76773 \n      409       410       411       412       413       414       415       416 \n146.68981 136.07840 196.72408 116.78047 108.14538 140.87229 104.28901 220.51154 \n      417       418       419       420       421       422       423       424 \n124.22878 120.27876 150.89275 160.30190 125.30044 130.61268 165.22609 182.98173 \n      425       426       427       428       429       430       431       432 \n197.48268 127.15171 137.72440 138.36959 189.30688 131.46424 120.45398 126.79471 \n      433       434       435       436       437       438       439       440 \n159.92036 106.12099 145.96688 199.03398 200.80264  91.66356 207.26604 120.99236 \n      441       442       443       444       445       446       447       448 \n134.59685 186.49729 125.37228  91.33883 215.94567 113.12211 140.71538 183.96527 \n      449       450       451       452       453       454       455       456 \n109.96835 147.36813 152.39432 149.86401 106.21055 163.41078 168.27444 155.85423 \n      457       458       459       460       461       462       463       464 \n136.78099 204.50110 115.72090 166.82063 113.56614 123.34751 136.70975 143.39951 \n      465       466       467       468       469       470       471       472 \n135.34580  98.44458 164.99800 163.75184 158.33794 149.70401 146.75652 149.65797 \n      473       474       475       476       477       478       479       480 \n201.56434 160.68342 144.82817 147.41448 113.52576 191.18718 144.53477 145.80859 \n      481       482       483       484       485       486       487       488 \n145.64267 181.73417 141.61701 149.51494 132.31808 167.51278 128.26862 132.21064 \n      489       490       491       492       493       494       495       496 \n 94.26651 181.22267 139.62648 196.96431 131.21919  86.09835 159.41616 199.98189 \n      497       498       499       500       501       502       503       504 \n118.21873 127.61573 189.53638 160.71170 108.63548 137.19988 117.61193 115.58129 \n      505       506       507       508       509       510       511       512 \n181.74385 153.85581 111.85147 129.21202 127.75988 125.59242 156.39825 122.05988 \n      513       514       515       516       517       518       519       520 \n130.68281 168.16906 153.41066 145.22976  92.58597 162.04743 111.33337 108.24769 \n      521       522       523       524       525       526       527       528 \n167.66315 121.18656 149.18143 210.13095 183.02837 140.53427 146.56573 144.87310 \n      529       530       531       532       533       534       535       536 \n166.49509 122.59210 150.75937 125.61078  86.18543 102.22212 155.43198 148.15748 \n      537       538       539       540       541       542       543       544 \n149.35042 211.70608 166.89867 163.01990 165.98605 131.55750 139.05699 136.26600 \n      545       546       547       548       549       550       551       552 \n187.37512 209.16296 124.24118 217.15732 131.45206 200.44739 161.97900 211.75724 \n      553       554       555       556       557       558       559       560 \n196.60627 158.87603 146.42271 158.66215  94.57544 130.58982 219.83518 168.73653 \n      561       562       563       564       565       566       567       568 \n103.23676 138.61184 139.28506  90.38635 107.43178 145.56665 182.75359  84.66830 \n      569       570       571       572       573       574       575       576 \n 95.54172 110.40895 141.69131 149.70431 155.93306 147.90934  66.92172 147.40374 \n      577       578       579       580       581       582       583       584 \n164.29821 101.57863 140.66876 128.56197 180.70434 152.35734  98.03817 120.94437 \n      585       586       587       588       589       590       591       592 \n163.93671 103.51459 234.60747 183.02836 112.99979 126.88485 117.09249 197.43122 \n      593       594       595       596       597       598       599       600 \n189.30520 199.85646 204.25296 116.47294 133.09952 159.90792  94.78936 146.62481 \n      601       602       603       604       605       606       607       608 \n161.30100 206.37377 171.43030 102.88773 109.65036 109.00625 133.92054 119.10930 \n      609       610       611       612       613       614       615       616 \n133.56496 161.88262 215.55343 143.80112 137.94141 115.89952 171.28105 124.59056 \n      617       618       619       620       621       622       623       624 \n125.53644 117.37452 116.79146 120.18302 111.49305 145.67830 169.56072 142.96825 \n      625       626       627       628       629       630       631       632 \n199.42203 178.96055 179.89291 127.84212 168.28382 170.12228 174.47724 209.10902 \n      633       634       635       636       637       638       639       640 \n150.86026  98.97987 166.86275 142.15511 157.93122 133.83970 189.96146 181.17574 \n      641       642       643       644       645       646       647       648 \n117.20864 128.77141  88.50434 140.26888 113.20755 159.03742  93.20243 205.15286 \n      649       650       651       652       653       654       655       656 \n156.19785  93.57605 152.34148 167.76096 111.57876 156.65744 179.34831 116.69196 \n      657       658       659       660       661       662       663       664 \n137.27616 138.71104 106.39228 174.14174 136.91016 131.00698 157.96093 144.11312 \n      665       666       667       668       669       670       671       672 \n149.95415  97.68463  83.63646 140.30589 184.32985 101.12046 198.48657 141.59381 \n      673       674       675       676       677       678       679       680 \n122.32839 132.94777 189.39847 193.85573 140.69053 128.54644 123.63920 174.14799 \n      681       682       683       684       685       686       687       688 \n 76.07789 142.56740 142.26049 103.90995 153.78283 184.23997 169.68028 163.73493 \n      689       690       691       692       693       694       695       696 \n153.14078 214.91104 209.38485 187.78498 190.32461 203.45085 132.04024 147.59594 \n      697       698       699       700       701       702       703       704 \n209.54622 152.67528 149.49657 119.85993 158.63245 193.95325 157.32780  74.61815 \n      705       706       707       708       709       710       711       712 \n139.78303 148.86800 170.62168 177.30972 141.46807 165.29557  85.14906 195.68289 \n      713       714       715       716       717       718       719       720 \n 94.31627 174.03493  92.89663 124.01311 175.83164 101.39999 147.03294 142.49622 \n      721       722       723       724       725       726       727       728 \n102.47200 160.55174 153.39793 179.41667 161.80378 188.13850 170.92407 181.38065 \n      729       730       731       732       733       734       735       736 \n183.05153 109.29652 206.81269 107.39169 149.97108 115.57644 137.92279 112.17761 \n      737       738       739       740       741       742       743       744 \n161.60959 123.38311 102.45505 121.23915 148.07805 104.21889 101.91801 104.84263 \n      745       746       747       748       749       750       751       752 \n159.82396 145.34680 184.67126 149.98669 204.25467 127.65413 135.43458 188.38210 \n      753       754       755       756       757       758       759       760 \n111.04171 163.76741 106.66981 187.99267 170.43716 131.38677 196.23994 158.66072 \n      761       762       763       764       765       766       767       768 \n117.32960 116.56648  95.92667 204.91089 178.29951 170.92889 167.34383 117.10943 \n      769       770       771       772       773       774       775       776 \n166.87970 175.31637 151.84175 133.86458 147.68267 183.06057 139.74572 191.49750 \n      777       778       779       780       781       782       783       784 \n157.37761 206.26633 160.58114 188.55907 155.08604 158.00274 169.29532 130.42954 \n      785       786       787       788       789       790       791       792 \n117.71002 110.82296 115.95208 131.84435 160.53279 157.60878 137.71198 207.13405 \n      793       794       795       796       797       798       799       800 \n111.15026 134.09405 191.88358 143.70642 182.39076 123.30742 135.25593 159.66430 \n      801       802       803       804       805       806       807       808 \n137.93381 148.97340 126.11531  96.50831 122.65395 161.04010 182.92128 115.70395 \n      809       810       811       812       813       814       815       816 \n124.13266 130.35385 107.80088 109.31520 150.02567 165.23199 107.83846 208.98325 \n      817       818       819       820       821       822       823       824 \n191.25359 132.38617 132.99406 124.38528 190.38509 168.24510 144.47013 186.40262 \n      825       826       827       828       829       830       831       832 \n193.52193 199.05886 189.04803 165.37642 227.71931 181.57172 194.40629 114.47539 \n      833       834       835       836       837       838       839       840 \n126.94223 173.55531 173.64069  86.12804 158.79830 159.37572 114.80716 126.70284 \n      841       842       843       844       845       846       847       848 \n133.02065 131.38989  69.92034 133.43639 115.30965 174.83223 117.28155 215.16084 \n      849       850       851       852       853       854       855       856 \n115.09295 118.63449 149.71050  98.83716 137.78178 157.02373 181.20713 147.35541 \n      857       858       859       860       861       862       863       864 \n135.67820 162.41962 122.64183 166.25939 165.39337 173.02002 142.67348 118.10370 \n      865       866       867       868       869       870       871       872 \n121.27506 156.76180 102.17407 112.04873 161.54260 146.19527 151.90878  98.57488 \n      873       874       875       876       877       878       879       880 \n148.95197 141.54518 144.79879 135.99896 220.22154 166.94160 170.91508 134.34702 \n      881       882       883       884       885       886       887       888 \n140.27201 149.00314 181.41794 186.38880 112.23641 137.02320 150.14489 153.80293 \n      889       890       891       892       893       894       895       896 \n133.78237 177.00591  97.29345 177.82945 108.96474 143.77484 109.88781 137.99429 \n      897       898       899       900       901       902       903       904 \n195.43959 133.20663 165.10963  98.74387 102.76223 183.61170 249.30085 178.79915 \n      905       906       907       908       909       910       911       912 \n147.14736  82.47596 201.46996 151.11012 134.65252  92.85790 122.84505 139.48203 \n      913       914       915       916       917       918       919       920 \n179.77475 171.35085 173.99905 212.88938 125.60796 177.90413 165.16701 166.47500 \n      921       922       923       924       925       926       927       928 \n114.76052 135.98509 107.74073 168.20780 207.28439 128.85987 173.56322 125.16145 \n      929       930       931       932       933       934       935       936 \n 92.67335 162.16668 206.59530 187.35367 164.60405 159.80983 128.72787 169.05167 \n      937       938       939       940       941       942       943       944 \n129.30531 113.91941 116.44077 147.14597 116.33535 198.55777 135.72938 161.85915 \n      945       946       947       948       949       950       951       952 \n168.73342 165.43831 113.07268 173.07630  97.41609 163.47127 163.04448 178.74035 \n      953       954       955       956       957       958       959       960 \n161.54741 110.63222 185.37558 155.14032  81.43196 121.41604 168.27137 116.81465 \n      961       962       963       964       965       966       967       968 \n162.30799 128.27654 181.95155 106.08367 203.26144 124.03628 187.93702 158.60301 \n      969       970       971       972       973       974       975 \n 82.05091 167.86634 193.10652 144.21857 166.81020 154.91225 184.98919 \n\n\n\nplot(my.lm)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(car)\n\nLoading required package: carData\n\navPlots(my.lm)\n\n\n\n\n\nlibrary(gvlma)\ngvlma(my.lm)\n\n\nCall:\nlm(formula = Final ~ Yr1 + Yr2 + Yr3, data = ugtests)\n\nCoefficients:\n(Intercept)          Yr1          Yr2          Yr3  \n   14.14599      0.07603      0.43129      0.86568  \n\n\nASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS\nUSING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:\nLevel of Significance =  0.05 \n\nCall:\n gvlma(x = my.lm) \n\n                      Value   p-value                   Decision\nGlobal Stat        54.24704 4.672e-11 Assumptions NOT satisfied!\nSkewness            0.73224 3.922e-01    Assumptions acceptable.\nKurtosis            0.06324 8.015e-01    Assumptions acceptable.\nLink Function      53.43397 2.675e-13 Assumptions NOT satisfied!\nHeteroscedasticity  0.01759 8.945e-01    Assumptions acceptable.\n\n\n\nlibrary(visreg)\nvisreg(my.lm, xvar=\"Yr2\")"
  },
  {
    "objectID": "posts/week-3/index.html",
    "href": "posts/week-3/index.html",
    "title": "Week 3: Binomial Logistic Regression",
    "section": "",
    "text": "The slides are here.\nOur third class meeting will focus on Chapter 5 of Handbook of Regression Modeling in People Analytics. The video is on youtube."
  },
  {
    "objectID": "posts/week-3/index.html#overview-and-comments",
    "href": "posts/week-3/index.html#overview-and-comments",
    "title": "Week 3: Binomial Logistic Regression",
    "section": "Overview and Comments",
    "text": "Overview and Comments\nWhat we require is a regression type tool tuned to represent data drawn from a generic binomial distribution. There are actually a few such models that I will introduce you to. There are also some really interesting models that you can fit that build mixtures of the different approaches but we won’t go that far. I should also note that there is a whole class of models on binary classification using trees. If you remember regression trees, you can also build regression trees for binary problems. I will do a bit of this in the end. Before that, some initial observations:\n\nI am using stargazer to produce the tables; they are nice and easy to produce. They have raw html output so I can embed that directly using asis in the code chunks and typesetting to html.\n\nThis whole document makes use of fenced code chunks. You can copy and paste this into a new markdown or quarto to play along with the ticks built in.\n\nIf one wants to omit a chunk at the top, you would do it with the bracketed part adding option include=FALSE. I always suppress warnings and messages to read (surrounded by curly brackets) r setup, include=FALSE. If you use this option and load libraries, readers will find it hard to figure out how commands may have changed meaning by masking.\n\n\n```{r setup}\nknitr::opts_chunk$set(warning=FALSE, message=FALSE, tidy = TRUE)\nset.seed(9119)\n```"
  },
  {
    "objectID": "posts/week-3/index.html#a-probit-model",
    "href": "posts/week-3/index.html#a-probit-model",
    "title": "Week 3: Binomial Logistic Regression",
    "section": "A Probit Model",
    "text": "A Probit Model\nFirst, a little substitution and some notation. Let me label the normal probability up to \\(X_{i}\\beta\\) to be \\(\\Phi(X\\beta)\\) and the probability above \\(X\\beta\\) to be \\(1-\\Phi(X+{i}\\beta)\\). I could substitute this into the binomial and obtain the product for the entire sample – this is known as the likelihood.\n\\[\\prod^{n}_{i=1} \\Phi(X_{i}\\beta)^{1-y_{i}}(1-\\Phi(X_{i}\\beta))^{y_{i}}\\]\nTaking logs yields:\n\\[\\ln \\mathcal{L} =  \\sum^{n}_{i=1} (1-y_{i})\\ln \\Phi(X_{i}\\beta) + y_{i} \\ln (1-\\Phi(X_{i}\\beta))\\]\nSo the solution becomes\n\\[\\arg \\max_{\\beta} \\ln \\mathcal{L} =  \\arg \\max_{\\beta} \\sum^{n}_{i=1} (1-y_{i})\\ln \\Phi(X_{i}\\beta) + y_{i} \\ln (1-\\Phi(X_{i}\\beta))\\]\nIn English, we want to find the values of \\(\\beta\\) that maximize the log-likelihod of the entire sample.\n\nEstimation of a First GLM\nNow to another example with the same measure of Churn. The outcome of interest is Churn. The model specification will call glm, let me examine Churn as a function of InternetService, tenure, PhoneService, Contract and TotalCharges. There is one trick to deploying it, the outcome variable must be a factor type. To make the table nice, let me mutate the type to a factor and then we can model it.\n\n```{r, warning=FALSE, message=FALSE}\nChurn %<>%\n    mutate(ChurnF = as.factor(Churn))\nChurn %>%\n    select(Churn, ChurnF) %>%\n    mutate(as.numeric(ChurnF)) %>%\n    head()\n```\n\n  Churn ChurnF as.numeric(ChurnF)\n1    No     No                  1\n2    No     No                  1\n3   Yes    Yes                  2\n4    No     No                  1\n5   Yes    Yes                  2\n6   Yes    Yes                  2\n\n\nNow I want to estimate the model and have a look at the result. I will put this in a stargazer table.\n```{r, results='asis', warning=FALSE, message=FALSE}\nmy.probit <- glm(ChurnF ~ InternetService + tenure + PhoneService + Contract + TotalCharges,\n    family = binomial(link = \"probit\"), data = Churn)\nstargazer(my.probit, type = \"html\", style = \"apsr\")\n```\n\n\n\n\n\n\n\n\n\n\nChurnF\n\n\n\n\n\n\n\n\nInternetServiceFiber optic\n\n\n0.726***\n\n\n\n\n\n\n(0.053)\n\n\n\n\nInternetServiceNo\n\n\n-0.458***\n\n\n\n\n\n\n(0.070)\n\n\n\n\ntenure\n\n\n-0.028***\n\n\n\n\n\n\n(0.003)\n\n\n\n\nPhoneServiceYes\n\n\n-0.372***\n\n\n\n\n\n\n(0.073)\n\n\n\n\nContractOne year\n\n\n-0.489***\n\n\n\n\n\n\n(0.057)\n\n\n\n\nContractTwo year\n\n\n-0.866***\n\n\n\n\n\n\n(0.083)\n\n\n\n\nTotalCharges\n\n\n0.0001***\n\n\n\n\n\n\n(0.00003)\n\n\n\n\nConstant\n\n\n0.131*\n\n\n\n\n\n\n(0.069)\n\n\n\n\nN\n\n\n7,032\n\n\n\n\nLog Likelihood\n\n\n-3,019.439\n\n\n\n\nAIC\n\n\n6,054.878\n\n\n\n\n\n\n\n\np < .1; p < .05; p < .01\n\n\n\n\nWe can do astrology on the tables; read the stars. Fiber optic customers are more likely to Churn and those without internet service are less likely to Churn but both conditions are compared to a third category absorbed into the Constant. What is that category?\n\n```{r}\njanitor::tabyl(Churn$InternetService)\n```\n\n Churn$InternetService    n   percent\n                   DSL 2421 0.3437456\n           Fiber optic 3096 0.4395854\n                    No 1526 0.2166690\n\n\nDSL subscribers. It is the first in alphabetical order. That is the default option. That also means that the constant captures those on Month-to-month contracts and without phone service – the omitted category for each. So what do these coefficients show?\nThe slopes represent the effect of a one-unit change in \\(x\\) on the underlying distribution for the probabilities. Unless one has intuition for those distributions, they come across as nonsensical. In the table above, let me take the example of tenure. For each unit of tenure [another month having been a customer], the normal variable \\(Z \\sim N(0,1)\\) decreases by 0.028. But what that means depends on whether we are going from 0 to -0.028 or from -2 to -2.028. Remember the standard normal has about 95% of probability between -2 and 2 and has a modal/most common value at zero.\n\n```{r}\ndata.frame(Z = rnorm(10000)) %>%\n    ggplot(.) + aes(x = Z) + geom_density() + theme_minimal() + geom_vline(aes(xintercept = -2.028),\n    color = \"red\") + geom_vline(aes(xintercept = -2), color = \"red\") + geom_vline(aes(xintercept = -0.028),\n    color = \"blue\") + geom_vline(aes(xintercept = 0), color = \"blue\")\n```\n\n\n\n\nThe associated probabilities in that example would be either small or nearly trivial even over 1000s of customers.\n\n```{r}\npnorm(-2) - pnorm(-2.028)\npnorm(0) - pnorm(-0.028)\n```\n\n[1] 0.001470008\n[1] 0.01116892\n\n\nIt seems like most scholars I run across don’t actually know this; they tend to stick to stars (That’s why your book plays with odds and logistic regression but I want to start here because y’all have never seen the logistic distribution, except on my poster….) Before that, here’s another way of showing the actual estimated slopes even if their intuition is hard.\n\n```{r}\nlibrary(jtools)\nplot_summs(my.probit, inner_ci_level = 0.95)\n```\n\n\n\n\nI can make that plot better.\n\n\nA Scaled Coefficient Plot\nRemember scaling from last time and linear models? No rules against that. The two metric variables – tenure and TotalCharges – are now the change in Z for a one standard deviation change in the relevant variable. That’s 2267 dollars for TotalCharges and 24.6 months for tenure.\n\n```{r}\nChurn %>%\n    skim(TotalCharges, tenure)\n```\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n7043\n\n\nNumber of columns\n23\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nTotalCharges\n11\n1\n2283.30\n2266.77\n18.8\n401.45\n1397.47\n3794.74\n8684.8\n▇▂▂▂▁\n\n\ntenure\n0\n1\n32.37\n24.56\n0.0\n9.00\n29.00\n55.00\n72.0\n▇▃▃▃▆\n\n\n\n\n\n\n```{r}\nChurn %>%\n    mutate(tenure = scale(tenure), TotalCharges = scale(TotalCharges)) %>%\n    glm(ChurnF ~ InternetService + tenure + PhoneService + Contract + TotalCharges,\n        family = binomial(link = \"probit\"), data = .) %>%\n    plot_summs(., inner_ci_level = 0.95)\n```\n\n\n\n\nWhat the plot makes clear is that basically all of the predictors are deemed important in Churn decisions by conventional standards as all have a very low probability of no relationship/zero slope. But that’s as far as we can get with these unless our audience shares this intuition for probability distributions.\n\n\nThe Trouble with Non-linear Models\nI should be clear that the model does have lines; they are just lines inside of a nonlinear function – the F. The generalized linear part means that the interpretation of any one factor will depend on the values of the others. We will have to usually want to generate hypothetical data to understand what is really going on. After a presentation of the remaining models, I will return to my preferred method of interpretation."
  },
  {
    "objectID": "posts/week-3/index.html#logistic-regression",
    "href": "posts/week-3/index.html#logistic-regression",
    "title": "Week 3: Binomial Logistic Regression",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nThe logistic distribution is the focus of the textbook chapter. To respecify the model using that, the only change in syntax is the link, we need it to be link=\"logit\" which is the default.\nThe logistic function is given by:\n\\[\\Lambda = \\frac{e^{X\\beta}}{1+e^{X\\beta}}\\]\nBut the rest is the same; it takes the very general representation and provides a specific probability function for \\(F\\):\n\\[\\arg \\max_{\\beta} \\ln \\mathcal{L} =  \\arg \\max_{\\beta} \\sum^{n}_{i=1} (1-y_{i})\\ln \\Lambda(X_{i}\\beta) + y_{i} \\ln (1-\\Lambda(X_{i}\\beta))\\]\nOne of the advantages of using the logistic distribution is that you can analytically solve it with only categorical variables. The other is the interpretation of the estimates; the slope is an increment in the log-odds, e.g. \\(\\ln (\\frac{\\pi_{y=1}}{1-\\pi_{y=1}})\\).\n```{r, results='asis', warning=FALSE, message=FALSE}\nmy.logit <- glm(ChurnF ~ InternetService + tenure + PhoneService + Contract + TotalCharges,\n    family = binomial(link = \"logit\"), data = Churn)\nstargazer(my.logit, my.probit, type = \"html\", style = \"apsr\")\n```\n\n\n\n\n\n\n\n\n\n\nChurnF\n\n\n\n\n\n\nlogistic\n\n\nprobit\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nInternetServiceFiber optic\n\n\n1.172***\n\n\n0.726***\n\n\n\n\n\n\n(0.091)\n\n\n(0.053)\n\n\n\n\nInternetServiceNo\n\n\n-0.765***\n\n\n-0.458***\n\n\n\n\n\n\n(0.127)\n\n\n(0.070)\n\n\n\n\ntenure\n\n\n-0.063***\n\n\n-0.028***\n\n\n\n\n\n\n(0.006)\n\n\n(0.003)\n\n\n\n\nPhoneServiceYes\n\n\n-0.714***\n\n\n-0.372***\n\n\n\n\n\n\n(0.126)\n\n\n(0.073)\n\n\n\n\nContractOne year\n\n\n-0.874***\n\n\n-0.489***\n\n\n\n\n\n\n(0.103)\n\n\n(0.057)\n\n\n\n\nContractTwo year\n\n\n-1.781***\n\n\n-0.866***\n\n\n\n\n\n\n(0.172)\n\n\n(0.083)\n\n\n\n\nTotalCharges\n\n\n0.0004***\n\n\n0.0001***\n\n\n\n\n\n\n(0.0001)\n\n\n(0.00003)\n\n\n\n\nConstant\n\n\n0.373***\n\n\n0.131*\n\n\n\n\n\n\n(0.117)\n\n\n(0.069)\n\n\n\n\nN\n\n\n7,032\n\n\n7,032\n\n\n\n\nLog Likelihood\n\n\n-3,004.328\n\n\n-3,019.439\n\n\n\n\nAIC\n\n\n6,024.656\n\n\n6,054.878\n\n\n\n\n\n\n\n\np < .1; p < .05; p < .01\n\n\n\n\nIf we see these in a side by side comparison, it is obvious that the logistic version is bigger in absolute value across the board. So what do these mean in terms of actual odds of Churn or not?\n\n```{r}\nexp(my.logit$coefficients)\n```\n\n               (Intercept) InternetServiceFiber optic \n                 1.4521830                  3.2282235 \n         InternetServiceNo                     tenure \n                 0.4652907                  0.9393570 \n           PhoneServiceYes           ContractOne year \n                 0.4898382                  0.4173338 \n          ContractTwo year               TotalCharges \n                 0.1685345                  1.0003559 \n\n\nAll else equal,\n\nThe odds of Churning with Fiber optics, as opposed to DSL, increase by 223%.\nThe odds of Churning with No internet, as opposed to DSL, decrease by 53.5% .\nThe odds of Churning with No phone service, as opposed to Phone service, are 51% lower.\nThe odds of Churning decrease by 4% per unit tenure [month].\nThe odds of Churning increase by 0.04% per dollar of total charges.\nThe odds of Churning decrease under contracts. Compared to none, about 83% lower odds under a two-year contract and 58% lower odds under a one-year contract.\n\nIf you choose to work with odds, then the suggestion to exponentiate the confidence intervals for the odds-ratios is sound.\n\n```{r}\nexp(confint(my.logit))\n```\n\n                               2.5 %    97.5 %\n(Intercept)                1.1535576 1.8288933\nInternetServiceFiber optic 2.7035580 3.8611249\nInternetServiceNo          0.3619193 0.5951803\ntenure                     0.9284409 0.9500587\nPhoneServiceYes            0.3822153 0.6276984\nContractOne year           0.3400210 0.5101487\nContractTwo year           0.1190988 0.2338910\nTotalCharges               1.0002350 1.0004795\n\n\nThere are diagnostics that can be applied to these models. The various pseudo-\\(r^2\\) measures. This model fit is neither terrible nor good.\n\n```{r}\nlibrary(DescTools)\nDescTools::PseudoR2(my.logit, which = c(\"McFadden\", \"CoxSnell\", \"Nagelkerke\", \"Tjur\"))\n```\n\n  McFadden   CoxSnell Nagelkerke       Tjur \n 0.2621401  0.2618213  0.3817196  0.2798359 \n\n\nTaking advantage of the book’s example, I first need to clean up the data, there are a few missing values. Then let me estimate the regression and diagnose it.\n\n```{r}\nChurn.CC <- Churn %>%\n    select(ChurnF, InternetService, tenure, PhoneService, Contract, TotalCharges) %>%\n    filter(!is.na(TotalCharges))\nmy.logit.CC <- glm(ChurnF ~ InternetService + tenure + PhoneService + Contract +\n    TotalCharges, family = binomial(link = \"logit\"), data = Churn.CC)\nlibrary(LogisticDx)\n# get range of goodness-of-fit diagnostics\nmodel_diagnostics <- LogisticDx::gof(my.logit.CC, plotROC = TRUE)\n```\n\n\n\n\nOne very common plot for binary logistic regression is the ROC: the Receiver Operating Curve. It plots specificity against sensitivity. Specificity is the ability, in this case, to correctly identify non-Churners[few false positives is highly specific]; sensitivity is the ability of the test to correctly identify Churners [few false negatives is highly sensitive]. A useful mnemonic is that the presence of the letter f in specificity is a reminder that the False test results are False for the condition, while the t in sensitivity is True test results are True for the condition. Now, turning to the actual provided diagnostics, what all is in there? ?gof for example.\n\n```{r}\n# returns a list\nnames(model_diagnostics)\nmodel_diagnostics$gof\n```\n\n[1] \"ct\"    \"chiSq\" \"ctHL\"  \"gof\"   \"R2\"    \"auc\"  \n         test  stat       val df         pVal\n1:         HL chiSq 31.832686  8 9.979186e-05\n2:        mHL     F 16.436041  9 4.896337e-27\n3:       OsRo     Z  3.282456 NA 1.029070e-03\n4: SstPgeq0.5     Z  6.656484 NA 2.804560e-11\n5:   SstPl0.5     Z  5.983933 NA 2.178133e-09\n6:    SstBoth chiSq 80.116227  2 4.008504e-18\n7: SllPgeq0.5 chiSq 45.280618  1 1.707304e-11\n8:   SllPl0.5 chiSq 29.794958  1 4.802393e-08\n9:    SllBoth chiSq 54.741365  2 1.297369e-12\n\n\nThis is not a very good model. It fails all the tests. We need to add more predictors; I have those but let’s keep it simple for now. Let me look at two more."
  },
  {
    "objectID": "posts/week-3/index.html#other-binomial-glms",
    "href": "posts/week-3/index.html#other-binomial-glms",
    "title": "Week 3: Binomial Logistic Regression",
    "section": "Other Binomial GLMs",
    "text": "Other Binomial GLMs\nThe others\n```{r, results='asis', warning=FALSE, message=FALSE}\nmy.cauchit <- glm(ChurnF ~ InternetService + tenure + PhoneService + Contract + TotalCharges,\n    family = binomial(link = \"cauchit\"), data = Churn)\nmy.cloglogit <- glm(ChurnF ~ InternetService + tenure + PhoneService + Contract +\n    TotalCharges, family = binomial(link = \"cloglog\"), data = Churn)\nstargazer(my.cauchit, my.cloglogit, my.logit, my.probit, type = \"html\", style = \"apsr\")\n```\n\n\n\n\n\n\n\n\n\n\nChurnF\n\n\n\n\n\n\nglm: binomial\n\n\nglm: binomial\n\n\nlogistic\n\n\nprobit\n\n\n\n\n\n\nlink = cauchit\n\n\nlink = cloglog\n\n\n\n\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n(3)\n\n\n(4)\n\n\n\n\n\n\n\n\nInternetServiceFiber optic\n\n\n0.993***\n\n\n0.877***\n\n\n1.172***\n\n\n0.726***\n\n\n\n\n\n\n(0.094)\n\n\n(0.071)\n\n\n(0.091)\n\n\n(0.053)\n\n\n\n\nInternetServiceNo\n\n\n-0.790***\n\n\n-0.640***\n\n\n-0.765***\n\n\n-0.458***\n\n\n\n\n\n\n(0.174)\n\n\n(0.112)\n\n\n(0.127)\n\n\n(0.070)\n\n\n\n\ntenure\n\n\n-0.123***\n\n\n-0.059***\n\n\n-0.063***\n\n\n-0.028***\n\n\n\n\n\n\n(0.011)\n\n\n(0.005)\n\n\n(0.006)\n\n\n(0.003)\n\n\n\n\nPhoneServiceYes\n\n\n-0.705***\n\n\n-0.586***\n\n\n-0.714***\n\n\n-0.372***\n\n\n\n\n\n\n(0.136)\n\n\n(0.099)\n\n\n(0.126)\n\n\n(0.073)\n\n\n\n\nContractOne year\n\n\n-1.190***\n\n\n-0.767***\n\n\n-0.874***\n\n\n-0.489***\n\n\n\n\n\n\n(0.166)\n\n\n(0.092)\n\n\n(0.103)\n\n\n(0.057)\n\n\n\n\nContractTwo year\n\n\n-7.065***\n\n\n-1.696***\n\n\n-1.781***\n\n\n-0.866***\n\n\n\n\n\n\n(1.254)\n\n\n(0.161)\n\n\n(0.172)\n\n\n(0.083)\n\n\n\n\nTotalCharges\n\n\n0.001***\n\n\n0.0004***\n\n\n0.0004***\n\n\n0.0001***\n\n\n\n\n\n\n(0.0001)\n\n\n(0.0001)\n\n\n(0.0001)\n\n\n(0.00003)\n\n\n\n\nConstant\n\n\n0.590***\n\n\n-0.024\n\n\n0.373***\n\n\n0.131*\n\n\n\n\n\n\n(0.122)\n\n\n(0.088)\n\n\n(0.117)\n\n\n(0.069)\n\n\n\n\nN\n\n\n7,032\n\n\n7,032\n\n\n7,032\n\n\n7,032\n\n\n\n\nLog Likelihood\n\n\n-2,997.445\n\n\n-2,991.982\n\n\n-3,004.328\n\n\n-3,019.439\n\n\n\n\nAIC\n\n\n6,010.891\n\n\n5,999.964\n\n\n6,024.656\n\n\n6,054.878\n\n\n\n\n\n\n\n\np < .1; p < .05; p < .01\n\n\n\n\nThe best fit seems to be provided by the cloglog distribution which is asymmetric.\n\n```{r}\nlibrary(pROC)\npredicted <- predict(my.cloglogit, type = \"response\")\nauc(Churn.CC$ChurnF, predicted, plot = TRUE)\n```\n\nArea under the curve: 0.8386"
  },
  {
    "objectID": "posts/week-3/index.html#residuals",
    "href": "posts/week-3/index.html#residuals",
    "title": "Week 3: Binomial Logistic Regression",
    "section": "Residuals",
    "text": "Residuals\n\n```{r}\nd <- density(residuals(my.logit, \"pearson\"))\nplot(d, main = \"\")\n```\n\n\n\n\nThis is rather poor."
  },
  {
    "objectID": "posts/week-3/index.html#predicted-probability",
    "href": "posts/week-3/index.html#predicted-probability",
    "title": "Week 3: Binomial Logistic Regression",
    "section": "Predicted Probability",
    "text": "Predicted Probability\nI find that the most straightforward way to interpret them is with plots in the probability metric. Let me take the example of tenure.\nI will need to create data for interpretation. Let’s suppose we have a DSL user with phone service on a two year contract with average TotalCharges. The last thing I need to know is what values of tenure to show.\n\n```{r}\nlibrary(skimr)\nChurn %>%\n    filter(InternetService == \"DSL\", PhoneService == \"Yes\", Contract == \"Two year\") %>%\n    skim(tenure, TotalCharges)\n```\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n467\n\n\nNumber of columns\n23\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ntenure\n0\n1.00\n60.52\n14.79\n0.0\n55.00\n66.00\n71.00\n72.00\n▁▁▁▂▇\n\n\nTotalCharges\n3\n0.99\n4733.47\n1382.51\n130.5\n3867.24\n4913.88\n5879.86\n6859.05\n▁▂▅▇▇\n\n\n\n\n\nNow I can create the data and generate predictions in the probability metric of the response.\n\n```{r}\nTenure.Pred <- data.frame(InternetService = \"DSL\", PhoneService = \"Yes\", Contract = \"Two year\",\n    TotalCharges = 4733.5, tenure = seq(0, 72, by = 1))\nTenure.Pred$Prob.Churn <- predict(my.logit, newdata = Tenure.Pred, type = \"response\")\n```\n\nNow let me plot it.\n\n```{r}\nggplot(Tenure.Pred) + aes(x = tenure, y = Prob.Churn) + geom_line() + theme_minimal()\n```\n\n\n\n\nWe could get fancier, too.\n\n```{r}\nTenure.Pred.Three <- rbind(data.frame(InternetService = \"DSL\", PhoneService = \"Yes\",\n    Contract = \"Month-to-month\", TotalCharges = 4733.5, tenure = seq(0, 72, by = 1)),\n    data.frame(InternetService = \"DSL\", PhoneService = \"Yes\", Contract = \"One year\",\n        TotalCharges = 4733.5, tenure = seq(0, 72, by = 1)), data.frame(InternetService = \"DSL\",\n        PhoneService = \"Yes\", Contract = \"Two year\", TotalCharges = 4733.5, tenure = seq(0,\n            72, by = 1)))\nTenure.Pred.Three$Prob.Churn <- predict(my.logit, newdata = Tenure.Pred.Three, type = \"response\")\nggplot(Tenure.Pred.Three) + aes(x = tenure, y = Prob.Churn, color = Contract) + geom_line() +\n    theme_minimal() + labs(y = \"Pr(Churn)\")\n```"
  },
  {
    "objectID": "posts/week-3/index.html#quadratic-terms",
    "href": "posts/week-3/index.html#quadratic-terms",
    "title": "Week 3: Binomial Logistic Regression",
    "section": "Quadratic Terms?",
    "text": "Quadratic Terms?\nWhat would happen if I assume that the effect of tenure is not a line but instead has some curvature.\n```{r, results='asis', warning=FALSE, message=FALSE}\nmod.train.SQ <- train %>%\n    glm(ChurnF ~ InternetService + tenure + I(tenure^2) + PhoneService + Contract +\n        TotalCharges, family = binomial(link = \"probit\"), data = .)\nstargazer(mod.train, mod.train.SQ, type = \"html\", style = \"apsr\")\n```\n\n\n\n\n\n\n\n\n\n\nChurnF\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nInternetServiceFiber optic\n\n\n0.703***\n\n\n0.761***\n\n\n\n\n\n\n(0.061)\n\n\n(0.062)\n\n\n\n\nInternetServiceNo\n\n\n-0.520***\n\n\n-0.566***\n\n\n\n\n\n\n(0.082)\n\n\n(0.083)\n\n\n\n\ntenure\n\n\n-0.030***\n\n\n-0.043***\n\n\n\n\n\n\n(0.004)\n\n\n(0.004)\n\n\n\n\nI(tenure2)\n\n\n\n\n0.0003***\n\n\n\n\n\n\n\n\n(0.0001)\n\n\n\n\nPhoneServiceYes\n\n\n-0.296***\n\n\n-0.262***\n\n\n\n\n\n\n(0.085)\n\n\n(0.085)\n\n\n\n\nContractOne year\n\n\n-0.467***\n\n\n-0.465***\n\n\n\n\n\n\n(0.067)\n\n\n(0.067)\n\n\n\n\nContractTwo year\n\n\n-0.927***\n\n\n-1.067***\n\n\n\n\n\n\n(0.100)\n\n\n(0.105)\n\n\n\n\nTotalCharges\n\n\n0.0001***\n\n\n0.00005\n\n\n\n\n\n\n(0.00004)\n\n\n(0.00004)\n\n\n\n\nConstant\n\n\n0.086\n\n\n0.171**\n\n\n\n\n\n\n(0.081)\n\n\n(0.082)\n\n\n\n\nN\n\n\n5,275\n\n\n5,275\n\n\n\n\nLog Likelihood\n\n\n-2,235.094\n\n\n-2,220.230\n\n\n\n\nAIC\n\n\n4,486.189\n\n\n4,458.460\n\n\n\n\n\n\n\n\np < .1; p < .05; p < .01\n\n\n\n\nAs we can see from the table, the curvature appears to be different from zero though interpreting such a thing ceteris paribus is probably nonsense. Maybe better to see what happens in the metric of the predicted probability. Let me recycle the prediction data I used to draw this in the earlier section. What would the two predictions look like and how do they differ?\n\n```{r}\nTenure.Pred$Prob.Churn.2 <- predict(mod.train, newdata = Tenure.Pred, type = \"response\")\nTenure.Pred$Prob.Churn.Sq <- predict(mod.train.SQ, newdata = Tenure.Pred, type = \"response\")\nggplot(Tenure.Pred) + aes(x = tenure, y = Prob.Churn.Sq) + geom_line() + geom_line(aes(y = Prob.Churn.2),\n    color = \"purple\") + theme_minimal() + labs(y = \"Pr(Churn)\")\n```\n\n\n\n\nNow let’s predict for the test set, does it really do better?\n\n```{r}\ntest$Pred.Probs.Sq <- predict(mod.train, newdata = test, type = \"response\")\ntest %>%\n    mutate(Pred.Val.Sq = (Pred.Probs.Sq > 0.5)) %>%\n    janitor::tabyl(Churn, Pred.Val.Sq, show_na = FALSE) %>%\n    adorn_percentages(\"row\")\n```\n\n Churn     FALSE      TRUE\n    No 0.8961749 0.1038251\n   Yes 0.5042017 0.4957983\n\n\nNot usually. Such people are really unlikely to Churn no matter what; it only starts at about 0.25."
  },
  {
    "objectID": "posts/week-3/index.html#a-final-note-a-classification-tree",
    "href": "posts/week-3/index.html#a-final-note-a-classification-tree",
    "title": "Week 3: Binomial Logistic Regression",
    "section": "A final note: a classification tree",
    "text": "A final note: a classification tree\nFirst, I will start with a generic classification tree with everything set to the defaults. Then I will look at a report to refine it.\n\n```{r}\nlibrary(rpart)\nlibrary(rpart.plot)\nfit.BT <- rpart(ChurnF ~ InternetService + tenure + PhoneService + Contract + TotalCharges,\n    data = train, method = \"class\")\nrpart.plot(fit.BT)\n```\n\n\n\n\nHow well does it fit the test sample?\n\n```{r}\ntest$Churn.No <- predict(fit.BT, newdata = test)[, 1]\ntest$Churn.PredRT <- (test$Churn.No < 0.5)\ntest %>%\n    tabyl(Churn, Churn.PredRT)\n```\n\n Churn FALSE TRUE\n    No  1210   74\n   Yes   319  157\n\n\nNot very well. We can alter the tolerance for complexity using some diagnostics about the tree.\n\n```{r}\nprintcp(fit.BT)\n```\n\n\nClassification tree:\nrpart(formula = ChurnF ~ InternetService + tenure + PhoneService + \n    Contract + TotalCharges, data = train, method = \"class\")\n\nVariables actually used in tree construction:\n[1] Contract        InternetService tenure         \n\nRoot node error: 1393/5283 = 0.26368\n\nn= 5283 \n\n        CP nsplit rel error  xerror     xstd\n1 0.056712      0   1.00000 1.00000 0.022991\n2 0.010000      3   0.78177 0.78823 0.021172\n\n\nThe option cp controls a complexity parameter that keeps the tree from overfitting the tree. I want to show a fairly complex one so I will change from the default of 0.01 to 0.0025.\n\n```{r}\nfit.BT.2 <- rpart(ChurnF ~ InternetService + tenure + PhoneService + Contract + TotalCharges,\n    data = train, method = \"class\", cp = 0.0025)\nrpart.plot(fit.BT.2, extra = 106)\n```\n\n\n\n\n\n```{r}\ntest$Churn.No <- predict(fit.BT.2, newdata = test)[, 1]\ntest$Churn.PredRT2 <- (test$Churn.No < 0.5)\ntest %>%\n    tabyl(Churn, Churn.PredRT2)\n```\n\n Churn FALSE TRUE\n    No  1154  130\n   Yes   251  225\n\n\nIn this case, we have 1379 correct with the big tree and 1367 right with the smaller tree."
  },
  {
    "objectID": "posts/smart-Prediction/index.html",
    "href": "posts/smart-Prediction/index.html",
    "title": "A Small Thread on Smart Prediction",
    "section": "",
    "text": "A linear regression example. The data can be loaded from the web.\n\n# if needed, download ugtests data\nurl <- \"http://peopleanalytics-regression-book.org/data/ugtests.csv\"\nugtests <- read.csv(url)\nstr(ugtests)\n\n'data.frame':   975 obs. of  4 variables:\n $ Yr1  : int  27 70 27 26 46 86 40 60 49 80 ...\n $ Yr2  : int  50 104 36 75 77 122 100 92 98 127 ...\n $ Yr3  : int  52 126 148 115 75 119 125 78 119 67 ...\n $ Final: int  93 207 175 125 114 159 153 84 147 80 ...\n\n\nThere are 975 individuals graduating in the past three years from the biology department of a large academic institution. We have data on four examinations:\n\na first year exam ranging from 0 to 100 (Yr1)\na second year exam ranging from 0 to 200 (Yr2)\na third year exam ranging from 0 to 200 (Yr3)\na Final year exam ranging from 0 to 300 (Final)\n\n\nlibrary(skimr); library(kableExtra); library(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.8     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.1\n✔ readr   2.1.2     ✔ forcats 0.5.2\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter()     masks stats::filter()\n✖ dplyr::group_rows() masks kableExtra::group_rows()\n✖ dplyr::lag()        masks stats::lag()\n\nskim(ugtests) %>% dplyr::filter(skim_type==\"numeric\") %>% kable()\n\n\n\n \n  \n    skim_type \n    skim_variable \n    n_missing \n    complete_rate \n    numeric.mean \n    numeric.sd \n    numeric.p0 \n    numeric.p25 \n    numeric.p50 \n    numeric.p75 \n    numeric.p100 \n    numeric.hist \n  \n \n\n  \n    numeric \n    Yr1 \n    0 \n    1 \n    52.14564 \n    14.92408 \n    3 \n    42 \n    53 \n    62 \n    99 \n    ▁▃▇▅▁ \n  \n  \n    numeric \n    Yr2 \n    0 \n    1 \n    92.39897 \n    30.03847 \n    6 \n    73 \n    94 \n    112 \n    188 \n    ▁▅▇▃▁ \n  \n  \n    numeric \n    Yr3 \n    0 \n    1 \n    105.12103 \n    33.50705 \n    8 \n    81 \n    105 \n    130 \n    198 \n    ▁▅▇▅▁ \n  \n  \n    numeric \n    Final \n    0 \n    1 \n    148.96205 \n    44.33966 \n    8 \n    118 \n    147 \n    175 \n    295 \n    ▁▅▇▃▁ \n  \n\n\n\n\n\n\n\n\nlibrary(GGally)\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\n# display a pairplot of all four columns of data\nGGally::ggpairs(ugtests)\n\n\n\n\n\nmy.lm <- lm(Final ~ Yr1 + Yr2 + Yr3, data=ugtests)\nsummary(my.lm)\n\n\nCall:\nlm(formula = Final ~ Yr1 + Yr2 + Yr3, data = ugtests)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-92.638 -20.349   0.001  18.954  98.489 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 14.14599    5.48006   2.581  0.00999 ** \nYr1          0.07603    0.06538   1.163  0.24519    \nYr2          0.43129    0.03251  13.267  < 2e-16 ***\nYr3          0.86568    0.02914  29.710  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 30.43 on 971 degrees of freedom\nMultiple R-squared:  0.5303,    Adjusted R-squared:  0.5289 \nF-statistic: 365.5 on 3 and 971 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\nugtests %>% mutate(Fitted.Value = predict(my.lm)) %>% kable() %>% scroll_box(height=\"300px\")\n\n\n\n \n  \n    Yr1 \n    Yr2 \n    Yr3 \n    Final \n    Fitted.Value \n  \n \n\n  \n    27 \n    50 \n    52 \n    93 \n    82.77839 \n  \n  \n    70 \n    104 \n    126 \n    207 \n    173.39734 \n  \n  \n    27 \n    36 \n    148 \n    175 \n    159.84579 \n  \n  \n    26 \n    75 \n    115 \n    125 \n    148.02242 \n  \n  \n    46 \n    77 \n    75 \n    114 \n    115.77826 \n  \n  \n    86 \n    122 \n    119 \n    159 \n    176.31713 \n  \n  \n    40 \n    100 \n    125 \n    153 \n    168.52573 \n  \n  \n    60 \n    92 \n    78 \n    84 \n    125.90895 \n  \n  \n    49 \n    98 \n    119 \n    147 \n    163.15331 \n  \n  \n    80 \n    127 \n    67 \n    80 \n    133.00197 \n  \n  \n    43 \n    134 \n    61 \n    154 \n    128.01391 \n  \n  \n    26 \n    53 \n    150 \n    154 \n    168.83298 \n  \n  \n    64 \n    123 \n    110 \n    175 \n    167.28471 \n  \n  \n    62 \n    84 \n    142 \n    182 \n    178.01432 \n  \n  \n    61 \n    65 \n    134 \n    155 \n    162.81842 \n  \n  \n    60 \n    150 \n    116 \n    198 \n    183.81939 \n  \n  \n    58 \n    76 \n    107 \n    161 \n    143.96109 \n  \n  \n    28 \n    81 \n    143 \n    229 \n    175.00126 \n  \n  \n    64 \n    87 \n    106 \n    100 \n    148.29571 \n  \n  \n    55 \n    111 \n    135 \n    179 \n    183.06708 \n  \n  \n    77 \n    97 \n    111 \n    164 \n    157.92531 \n  \n  \n    50 \n    92 \n    72 \n    130 \n    119.95460 \n  \n  \n    47 \n    83 \n    129 \n    194 \n    165.18879 \n  \n  \n    65 \n    72 \n    148 \n    152 \n    178.26106 \n  \n  \n    57 \n    180 \n    112 \n    216 \n    193.06715 \n  \n  \n    41 \n    41 \n    152 \n    123 \n    166.52931 \n  \n  \n    49 \n    90 \n    166 \n    232 \n    200.39004 \n  \n  \n    93 \n    102 \n    136 \n    168 \n    182.94018 \n  \n  \n    32 \n    62 \n    160 \n    151 \n    181.82752 \n  \n  \n    53 \n    73 \n    63 \n    88 \n    104.19713 \n  \n  \n    28 \n    84 \n    109 \n    184 \n    146.86195 \n  \n  \n    75 \n    125 \n    98 \n    134 \n    158.59539 \n  \n  \n    44 \n    55 \n    111 \n    115 \n    137.30246 \n  \n  \n    41 \n    127 \n    96 \n    167 \n    155.14171 \n  \n  \n    55 \n    97 \n    101 \n    170 \n    147.59592 \n  \n  \n    68 \n    109 \n    141 \n    229 \n    188.38693 \n  \n  \n    73 \n    52 \n    124 \n    132 \n    149.46722 \n  \n  \n    32 \n    92 \n    154 \n    245 \n    189.57199 \n  \n  \n    49 \n    65 \n    138 \n    180 \n    165.36883 \n  \n  \n    63 \n    107 \n    165 \n    213 \n    207.92058 \n  \n  \n    43 \n    87 \n    120 \n    211 \n    158.81869 \n  \n  \n    40 \n    110 \n    93 \n    151 \n    145.13679 \n  \n  \n    46 \n    71 \n    91 \n    121 \n    127.04145 \n  \n  \n    28 \n    118 \n    136 \n    180 \n    184.89905 \n  \n  \n    46 \n    124 \n    176 \n    232 \n    223.48248 \n  \n  \n    41 \n    97 \n    152 \n    214 \n    190.68129 \n  \n  \n    76 \n    95 \n    52 \n    123 \n    105.91152 \n  \n  \n    52 \n    106 \n    144 \n    166 \n    188.47370 \n  \n  \n    47 \n    104 \n    124 \n    111 \n    169.91737 \n  \n  \n    54 \n    27 \n    111 \n    90 \n    125.98673 \n  \n  \n    36 \n    97 \n    140 \n    187 \n    179.91299 \n  \n  \n    73 \n    22 \n    146 \n    160 \n    155.57364 \n  \n  \n    60 \n    68 \n    54 \n    125 \n    94.78176 \n  \n  \n    67 \n    45 \n    80 \n    145 \n    107.90209 \n  \n  \n    50 \n    99 \n    94 \n    168 \n    142.01859 \n  \n  \n    32 \n    72 \n    92 \n    114 \n    127.27405 \n  \n  \n    75 \n    103 \n    91 \n    152 \n    143.04734 \n  \n  \n    60 \n    80 \n    43 \n    125 \n    90.43469 \n  \n  \n    54 \n    13 \n    106 \n    143 \n    115.62032 \n  \n  \n    69 \n    64 \n    161 \n    161 \n    186.36874 \n  \n  \n    54 \n    125 \n    107 \n    98 \n    164.78997 \n  \n  \n    52 \n    65 \n    157 \n    183 \n    182.04486 \n  \n  \n    36 \n    138 \n    72 \n    152 \n    138.72937 \n  \n  \n    36 \n    131 \n    116 \n    203 \n    173.80034 \n  \n  \n    39 \n    105 \n    116 \n    158 \n    162.81500 \n  \n  \n    50 \n    71 \n    72 \n    65 \n    110.89761 \n  \n  \n    55 \n    110 \n    106 \n    133 \n    157.53103 \n  \n  \n    33 \n    89 \n    134 \n    122 \n    171.04054 \n  \n  \n    52 \n    68 \n    135 \n    138 \n    164.29372 \n  \n  \n    59 \n    97 \n    105 \n    132 \n    151.36275 \n  \n  \n    42 \n    101 \n    165 \n    174 \n    203.73632 \n  \n  \n    39 \n    76 \n    97 \n    173 \n    133.85978 \n  \n  \n    45 \n    63 \n    131 \n    124 \n    158.14239 \n  \n  \n    55 \n    71 \n    111 \n    139 \n    145.03931 \n  \n  \n    69 \n    55 \n    82 \n    119 \n    114.09836 \n  \n  \n    72 \n    62 \n    104 \n    171 \n    136.39042 \n  \n  \n    61 \n    108 \n    98 \n    160 \n    150.19917 \n  \n  \n    55 \n    90 \n    49 \n    117 \n    99.56150 \n  \n  \n    35 \n    84 \n    107 \n    178 \n    145.66277 \n  \n  \n    59 \n    36 \n    95 \n    122 \n    116.39753 \n  \n  \n    44 \n    74 \n    87 \n    144 \n    124.72053 \n  \n  \n    43 \n    78 \n    101 \n    124 \n    138.48918 \n  \n  \n    66 \n    142 \n    101 \n    244 \n    167.84005 \n  \n  \n    38 \n    71 \n    63 \n    96 \n    102.19417 \n  \n  \n    62 \n    96 \n    147 \n    248 \n    187.51815 \n  \n  \n    46 \n    63 \n    120 \n    194 \n    148.69592 \n  \n  \n    59 \n    105 \n    157 \n    198 \n    199.82845 \n  \n  \n    68 \n    72 \n    111 \n    160 \n    146.45894 \n  \n  \n    54 \n    86 \n    79 \n    124 \n    123.73077 \n  \n  \n    44 \n    66 \n    102 \n    136 \n    134.25546 \n  \n  \n    41 \n    87 \n    91 \n    127 \n    133.56188 \n  \n  \n    15 \n    97 \n    132 \n    193 \n    171.39099 \n  \n  \n    52 \n    88 \n    107 \n    93 \n    148.68036 \n  \n  \n    53 \n    92 \n    118 \n    182 \n    160.00402 \n  \n  \n    54 \n    71 \n    107 \n    114 \n    141.50056 \n  \n  \n    75 \n    71 \n    108 \n    147 \n    143.96279 \n  \n  \n    52 \n    54 \n    98 \n    122 \n    126.22552 \n  \n  \n    41 \n    72 \n    113 \n    172 \n    146.13759 \n  \n  \n    52 \n    74 \n    128 \n    108 \n    160.82167 \n  \n  \n    64 \n    95 \n    125 \n    158 \n    168.19393 \n  \n  \n    64 \n    122 \n    78 \n    168 \n    139.15162 \n  \n  \n    29 \n    57 \n    99 \n    101 \n    126.63646 \n  \n  \n    50 \n    107 \n    91 \n    163 \n    142.87183 \n  \n  \n    62 \n    98 \n    171 \n    197 \n    209.15707 \n  \n  \n    45 \n    65 \n    134 \n    162 \n    161.60200 \n  \n  \n    49 \n    93 \n    129 \n    174 \n    169.65369 \n  \n  \n    70 \n    37 \n    76 \n    137 \n    101.21716 \n  \n  \n    40 \n    71 \n    98 \n    154 \n    132.64506 \n  \n  \n    50 \n    105 \n    81 \n    154 \n    133.35245 \n  \n  \n    53 \n    66 \n    60 \n    126 \n    98.58109 \n  \n  \n    70 \n    110 \n    44 \n    105 \n    104.99919 \n  \n  \n    65 \n    37 \n    101 \n    102 \n    122.47906 \n  \n  \n    79 \n    122 \n    69 \n    92 \n    132.50088 \n  \n  \n    60 \n    63 \n    103 \n    131 \n    135.04371 \n  \n  \n    50 \n    114 \n    118 \n    189 \n    169.26422 \n  \n  \n    66 \n    94 \n    112 \n    101 \n    156.66084 \n  \n  \n    46 \n    156 \n    135 \n    184 \n    201.79068 \n  \n  \n    31 \n    85 \n    57 \n    108 \n    102.50589 \n  \n  \n    82 \n    99 \n    184 \n    211 \n    222.36274 \n  \n  \n    63 \n    71 \n    102 \n    172 \n    137.85639 \n  \n  \n    33 \n    78 \n    78 \n    144 \n    117.81825 \n  \n  \n    48 \n    39 \n    67 \n    88 \n    92.61602 \n  \n  \n    31 \n    80 \n    133 \n    153 \n    166.14124 \n  \n  \n    59 \n    67 \n    50 \n    86 \n    90.81172 \n  \n  \n    60 \n    128 \n    94 \n    171 \n    155.28613 \n  \n  \n    60 \n    121 \n    174 \n    263 \n    221.52163 \n  \n  \n    26 \n    108 \n    63 \n    158 \n    117.23941 \n  \n  \n    36 \n    129 \n    61 \n    146 \n    125.32530 \n  \n  \n    63 \n    83 \n    115 \n    144 \n    154.28567 \n  \n  \n    48 \n    74 \n    87 \n    111 \n    125.02463 \n  \n  \n    26 \n    132 \n    88 \n    81 \n    149.23229 \n  \n  \n    39 \n    125 \n    136 \n    201 \n    188.75433 \n  \n  \n    46 \n    121 \n    113 \n    126 \n    167.65071 \n  \n  \n    54 \n    79 \n    103 \n    150 \n    141.48812 \n  \n  \n    70 \n    95 \n    102 \n    136 \n    148.73942 \n  \n  \n    50 \n    85 \n    85 \n    132 \n    128.18946 \n  \n  \n    34 \n    74 \n    144 \n    176 \n    173.30410 \n  \n  \n    62 \n    97 \n    61 \n    168 \n    113.50085 \n  \n  \n    49 \n    56 \n    121 \n    141 \n    146.77068 \n  \n  \n    10 \n    36 \n    132 \n    153 \n    144.70245 \n  \n  \n    50 \n    92 \n    99 \n    143 \n    143.32800 \n  \n  \n    23 \n    102 \n    130 \n    170 \n    172.42426 \n  \n  \n    74 \n    126 \n    84 \n    110 \n    146.83111 \n  \n  \n    31 \n    88 \n    124 \n    142 \n    161.80039 \n  \n  \n    38 \n    149 \n    86 \n    123 \n    155.74509 \n  \n  \n    70 \n    102 \n    67 \n    136 \n    121.45958 \n  \n  \n    42 \n    104 \n    67 \n    111 \n    120.19341 \n  \n  \n    50 \n    103 \n    155 \n    248 \n    196.55029 \n  \n  \n    56 \n    26 \n    90 \n    97 \n    107.52819 \n  \n  \n    43 \n    77 \n    110 \n    100 \n    145.84903 \n  \n  \n    38 \n    83 \n    89 \n    106 \n    129.87730 \n  \n  \n    70 \n    86 \n    32 \n    88 \n    84.26017 \n  \n  \n    50 \n    93 \n    103 \n    113 \n    147.22201 \n  \n  \n    45 \n    92 \n    133 \n    186 \n    172.38103 \n  \n  \n    50 \n    123 \n    90 \n    146 \n    148.90671 \n  \n  \n    73 \n    105 \n    146 \n    204 \n    191.37033 \n  \n  \n    59 \n    78 \n    47 \n    79 \n    92.95881 \n  \n  \n    44 \n    49 \n    87 \n    116 \n    113.93839 \n  \n  \n    34 \n    58 \n    85 \n    59 \n    115.32834 \n  \n  \n    48 \n    161 \n    151 \n    289 \n    217.95006 \n  \n  \n    53 \n    18 \n    96 \n    70 \n    109.04391 \n  \n  \n    47 \n    101 \n    140 \n    183 \n    182.47442 \n  \n  \n    46 \n    149 \n    106 \n    189 \n    173.66693 \n  \n  \n    25 \n    121 \n    87 \n    83 \n    143.54644 \n  \n  \n    71 \n    136 \n    156 \n    171 \n    213.24494 \n  \n  \n    27 \n    114 \n    103 \n    169 \n    154.53040 \n  \n  \n    61 \n    106 \n    108 \n    110 \n    157.99341 \n  \n  \n    36 \n    108 \n    121 \n    113 \n    168.20918 \n  \n  \n    66 \n    104 \n    91 \n    124 \n    142.79439 \n  \n  \n    42 \n    27 \n    95 \n    129 \n    111.22351 \n  \n  \n    58 \n    116 \n    116 \n    174 \n    169.00364 \n  \n  \n    48 \n    72 \n    54 \n    165 \n    95.59458 \n  \n  \n    80 \n    143 \n    137 \n    247 \n    200.50023 \n  \n  \n    52 \n    133 \n    87 \n    159 \n    150.77458 \n  \n  \n    50 \n    37 \n    80 \n    41 \n    103.15936 \n  \n  \n    27 \n    128 \n    75 \n    127 \n    136.32932 \n  \n  \n    44 \n    116 \n    98 \n    187 \n    152.35701 \n  \n  \n    60 \n    101 \n    68 \n    100 \n    121.13371 \n  \n  \n    64 \n    97 \n    64 \n    150 \n    116.24995 \n  \n  \n    35 \n    120 \n    120 \n    189 \n    172.44290 \n  \n  \n    34 \n    94 \n    56 \n    68 \n    105.74986 \n  \n  \n    57 \n    116 \n    88 \n    134 \n    144.68854 \n  \n  \n    76 \n    163 \n    60 \n    100 \n    142.16437 \n  \n  \n    48 \n    17 \n    90 \n    70 \n    103.03841 \n  \n  \n    74 \n    39 \n    55 \n    104 \n    84.20453 \n  \n  \n    54 \n    46 \n    100 \n    125 \n    124.65866 \n  \n  \n    47 \n    70 \n    147 \n    168 \n    175.16434 \n  \n  \n    57 \n    111 \n    133 \n    166 \n    181.48777 \n  \n  \n    56 \n    89 \n    146 \n    210 \n    183.17732 \n  \n  \n    25 \n    73 \n    137 \n    137 \n    166.12881 \n  \n  \n    73 \n    26 \n    91 \n    79 \n    109.68631 \n  \n  \n    64 \n    63 \n    119 \n    105 \n    149.19871 \n  \n  \n    67 \n    64 \n    118 \n    135 \n    148.99240 \n  \n  \n    43 \n    117 \n    64 \n    140 \n    123.27911 \n  \n  \n    47 \n    123 \n    99 \n    154 \n    156.46977 \n  \n  \n    57 \n    107 \n    54 \n    118 \n    111.37381 \n  \n  \n    21 \n    44 \n    114 \n    144 \n    133.40676 \n  \n  \n    50 \n    71 \n    66 \n    163 \n    105.70352 \n  \n  \n    54 \n    116 \n    127 \n    144 \n    178.22203 \n  \n  \n    76 \n    137 \n    97 \n    139 \n    162.98116 \n  \n  \n    48 \n    42 \n    61 \n    109 \n    88.71579 \n  \n  \n    32 \n    97 \n    97 \n    142 \n    142.38459 \n  \n  \n    70 \n    57 \n    95 \n    116 \n    126.29081 \n  \n  \n    60 \n    123 \n    143 \n    206 \n    195.54808 \n  \n  \n    76 \n    122 \n    57 \n    164 \n    121.88463 \n  \n  \n    55 \n    87 \n    138 \n    171 \n    175.31327 \n  \n  \n    46 \n    89 \n    133 \n    183 \n    171.16320 \n  \n  \n    59 \n    129 \n    63 \n    93 \n    128.80527 \n  \n  \n    64 \n    90 \n    115 \n    123 \n    157.38069 \n  \n  \n    54 \n    106 \n    93 \n    118 \n    144.47601 \n  \n  \n    47 \n    106 \n    95 \n    164 \n    145.67519 \n  \n  \n    71 \n    109 \n    105 \n    148 \n    157.45049 \n  \n  \n    54 \n    86 \n    41 \n    125 \n    90.83488 \n  \n  \n    59 \n    42 \n    102 \n    156 \n    125.04501 \n  \n  \n    36 \n    111 \n    61 \n    73 \n    117.56217 \n  \n  \n    50 \n    120 \n    112 \n    180 \n    166.65784 \n  \n  \n    57 \n    65 \n    111 \n    89 \n    142.60365 \n  \n  \n    58 \n    99 \n    104 \n    188 \n    151.28361 \n  \n  \n    30 \n    73 \n    105 \n    110 \n    138.80714 \n  \n  \n    78 \n    120 \n    93 \n    185 \n    152.33864 \n  \n  \n    36 \n    93 \n    139 \n    157 \n    177.32217 \n  \n  \n    56 \n    112 \n    79 \n    128 \n    135.09624 \n  \n  \n    42 \n    111 \n    121 \n    172 \n    169.95920 \n  \n  \n    76 \n    83 \n    122 \n    147 \n    161.33378 \n  \n  \n    65 \n    71 \n    117 \n    171 \n    150.99366 \n  \n  \n    40 \n    118 \n    115 \n    97 \n    167.63206 \n  \n  \n    56 \n    59 \n    68 \n    105 \n    102.71562 \n  \n  \n    56 \n    103 \n    55 \n    105 \n    110.43832 \n  \n  \n    81 \n    100 \n    111 \n    153 \n    159.52327 \n  \n  \n    42 \n    105 \n    103 \n    157 \n    151.78922 \n  \n  \n    57 \n    124 \n    100 \n    174 \n    158.52699 \n  \n  \n    58 \n    52 \n    168 \n    133 \n    186.41680 \n  \n  \n    27 \n    65 \n    93 \n    143 \n    124.74060 \n  \n  \n    38 \n    95 \n    70 \n    81 \n    118.60478 \n  \n  \n    41 \n    98 \n    119 \n    155 \n    162.54510 \n  \n  \n    65 \n    107 \n    154 \n    196 \n    198.55014 \n  \n  \n    60 \n    75 \n    84 \n    129 \n    123.77119 \n  \n  \n    50 \n    136 \n    153 \n    229 \n    209.05134 \n  \n  \n    24 \n    114 \n    164 \n    286 \n    207.10887 \n  \n  \n    55 \n    83 \n    65 \n    106 \n    110.39340 \n  \n  \n    73 \n    135 \n    71 \n    115 \n    139.38280 \n  \n  \n    28 \n    111 \n    120 \n    230 \n    168.02915 \n  \n  \n    52 \n    105 \n    119 \n    167 \n    166.40038 \n  \n  \n    56 \n    95 \n    185 \n    237 \n    219.52660 \n  \n  \n    38 \n    114 \n    90 \n    107 \n    144.11283 \n  \n  \n    21 \n    84 \n    125 \n    154 \n    160.18067 \n  \n  \n    48 \n    92 \n    145 \n    172 \n    182.99728 \n  \n  \n    50 \n    39 \n    135 \n    171 \n    151.63440 \n  \n  \n    50 \n    52 \n    129 \n    115 \n    152.04702 \n  \n  \n    61 \n    60 \n    151 \n    146 \n    175.37858 \n  \n  \n    61 \n    123 \n    94 \n    162 \n    153.20573 \n  \n  \n    65 \n    126 \n    139 \n    198 \n    193.75934 \n  \n  \n    42 \n    117 \n    86 \n    123 \n    142.24807 \n  \n  \n    50 \n    106 \n    163 \n    224 \n    204.76959 \n  \n  \n    46 \n    49 \n    126 \n    132 \n    147.85201 \n  \n  \n    59 \n    61 \n    54 \n    135 \n    91.68673 \n  \n  \n    62 \n    90 \n    140 \n    201 \n    178.87067 \n  \n  \n    85 \n    156 \n    88 \n    102 \n    164.06869 \n  \n  \n    72 \n    131 \n    105 \n    152 \n    167.01479 \n  \n  \n    53 \n    132 \n    113 \n    203 \n    172.92703 \n  \n  \n    21 \n    116 \n    67 \n    102 \n    123.77229 \n  \n  \n    45 \n    44 \n    160 \n    144 \n    175.05272 \n  \n  \n    59 \n    95 \n    67 \n    101 \n    117.60429 \n  \n  \n    76 \n    91 \n    61 \n    86 \n    111.97751 \n  \n  \n    53 \n    110 \n    109 \n    179 \n    159.97603 \n  \n  \n    71 \n    46 \n    147 \n    74 \n    166.63812 \n  \n  \n    69 \n    110 \n    123 \n    138 \n    173.31198 \n  \n  \n    37 \n    101 \n    170 \n    222 \n    207.68459 \n  \n  \n    50 \n    132 \n    76 \n    124 \n    140.66875 \n  \n  \n    56 \n    83 \n    94 \n    97 \n    135.57418 \n  \n  \n    68 \n    100 \n    132 \n    181 \n    176.71423 \n  \n  \n    94 \n    120 \n    112 \n    141 \n    170.00300 \n  \n  \n    37 \n    90 \n    132 \n    154 \n    170.04457 \n  \n  \n    24 \n    99 \n    137 \n    185 \n    177.26620 \n  \n  \n    62 \n    68 \n    98 \n    135 \n    133.02378 \n  \n  \n    77 \n    124 \n    84 \n    161 \n    146.19662 \n  \n  \n    29 \n    70 \n    139 \n    185 \n    166.87042 \n  \n  \n    48 \n    84 \n    127 \n    182 \n    163.96474 \n  \n  \n    63 \n    124 \n    97 \n    127 \n    156.38611 \n  \n  \n    47 \n    136 \n    97 \n    104 \n    160.34511 \n  \n  \n    50 \n    70 \n    27 \n    170 \n    71.51067 \n  \n  \n    17 \n    97 \n    156 \n    226 \n    192.31939 \n  \n  \n    45 \n    92 \n    37 \n    128 \n    89.27563 \n  \n  \n    41 \n    65 \n    117 \n    177 \n    146.58132 \n  \n  \n    79 \n    108 \n    84 \n    152 \n    139.44811 \n  \n  \n    71 \n    84 \n    57 \n    140 \n    105.11565 \n  \n  \n    75 \n    30 \n    54 \n    170 \n    79.53330 \n  \n  \n    65 \n    74 \n    81 \n    116 \n    121.12299 \n  \n  \n    52 \n    97 \n    58 \n    118 \n    110.14355 \n  \n  \n    67 \n    72 \n    148 \n    181 \n    178.41312 \n  \n  \n    67 \n    38 \n    142 \n    135 \n    158.55532 \n  \n  \n    56 \n    88 \n    144 \n    141 \n    181.01467 \n  \n  \n    61 \n    123 \n    152 \n    221 \n    203.41524 \n  \n  \n    50 \n    110 \n    164 \n    206 \n    207.36041 \n  \n  \n    35 \n    81 \n    65 \n    90 \n    108.01030 \n  \n  \n    71 \n    89 \n    133 \n    167 \n    173.06385 \n  \n  \n    52 \n    82 \n    107 \n    111 \n    146.09265 \n  \n  \n    63 \n    106 \n    74 \n    160 \n    128.71230 \n  \n  \n    72 \n    97 \n    64 \n    135 \n    116.85816 \n  \n  \n    56 \n    128 \n    165 \n    206 \n    216.44539 \n  \n  \n    64 \n    68 \n    140 \n    214 \n    169.53445 \n  \n  \n    62 \n    85 \n    80 \n    99 \n    124.77337 \n  \n  \n    73 \n    114 \n    111 \n    182 \n    164.95305 \n  \n  \n    40 \n    121 \n    140 \n    216 \n    190.56794 \n  \n  \n    57 \n    144 \n    56 \n    127 \n    129.06273 \n  \n  \n    45 \n    85 \n    102 \n    78 \n    142.52591 \n  \n  \n    53 \n    25 \n    77 \n    128 \n    95.61497 \n  \n  \n    56 \n    65 \n    144 \n    124 \n    171.09510 \n  \n  \n    27 \n    100 \n    149 \n    196 \n    188.31374 \n  \n  \n    43 \n    50 \n    63 \n    121 \n    93.51730 \n  \n  \n    77 \n    80 \n    73 \n    166 \n    117.69757 \n  \n  \n    50 \n    61 \n    89 \n    127 \n    121.30134 \n  \n  \n    47 \n    102 \n    135 \n    225 \n    178.57730 \n  \n  \n    63 \n    85 \n    118 \n    164 \n    157.74528 \n  \n  \n    64 \n    161 \n    95 \n    164 \n    170.68833 \n  \n  \n    59 \n    65 \n    85 \n    119 \n    120.24799 \n  \n  \n    49 \n    118 \n    55 \n    42 \n    116.37542 \n  \n  \n    71 \n    73 \n    131 \n    126 \n    164.43192 \n  \n  \n    65 \n    106 \n    128 \n    146 \n    175.61114 \n  \n  \n    41 \n    131 \n    169 \n    237 \n    220.06158 \n  \n  \n    71 \n    73 \n    151 \n    190 \n    181.74555 \n  \n  \n    44 \n    134 \n    99 \n    120 \n    160.98583 \n  \n  \n    39 \n    56 \n    52 \n    95 \n    86.27842 \n  \n  \n    99 \n    87 \n    158 \n    238 \n    195.97205 \n  \n  \n    17 \n    106 \n    87 \n    134 \n    136.46895 \n  \n  \n    41 \n    62 \n    68 \n    126 \n    102.86908 \n  \n  \n    47 \n    119 \n    175 \n    231 \n    220.53640 \n  \n  \n    65 \n    103 \n    72 \n    93 \n    125.83914 \n  \n  \n    57 \n    94 \n    147 \n    234 \n    186.27545 \n  \n  \n    36 \n    66 \n    152 \n    201 \n    176.93132 \n  \n  \n    39 \n    103 \n    135 \n    159 \n    178.40037 \n  \n  \n    31 \n    123 \n    91 \n    167 \n    148.32790 \n  \n  \n    44 \n    78 \n    151 \n    137 \n    181.84927 \n  \n  \n    19 \n    98 \n    87 \n    154 \n    133.17072 \n  \n  \n    39 \n    119 \n    114 \n    157 \n    167.12163 \n  \n  \n    42 \n    116 \n    143 \n    271 \n    191.16061 \n  \n  \n    33 \n    128 \n    111 \n    179 \n    167.95000 \n  \n  \n    46 \n    126 \n    161 \n    271 \n    211.35983 \n  \n  \n    48 \n    125 \n    106 \n    154 \n    163.46813 \n  \n  \n    56 \n    83 \n    49 \n    162 \n    96.61853 \n  \n  \n    60 \n    73 \n    117 \n    162 \n    151.47610 \n  \n  \n    61 \n    100 \n    90 \n    151 \n    139.82344 \n  \n  \n    48 \n    109 \n    85 \n    164 \n    138.38826 \n  \n  \n    42 \n    96 \n    136 \n    160 \n    176.47513 \n  \n  \n    54 \n    92 \n    56 \n    66 \n    106.40781 \n  \n  \n    84 \n    98 \n    111 \n    182 \n    158.88878 \n  \n  \n    77 \n    126 \n    121 \n    195 \n    179.08940 \n  \n  \n    53 \n    62 \n    120 \n    141 \n    148.79682 \n  \n  \n    58 \n    98 \n    95 \n    132 \n    143.06119 \n  \n  \n    29 \n    65 \n    57 \n    109 \n    93.72813 \n  \n  \n    64 \n    82 \n    93 \n    114 \n    134.88542 \n  \n  \n    33 \n    78 \n    119 \n    97 \n    153.31118 \n  \n  \n    55 \n    108 \n    60 \n    109 \n    116.84713 \n  \n  \n    42 \n    102 \n    145 \n    206 \n    186.85398 \n  \n  \n    56 \n    140 \n    114 \n    178 \n    177.47107 \n  \n  \n    54 \n    90 \n    99 \n    105 \n    142.76953 \n  \n  \n    65 \n    84 \n    125 \n    160 \n    163.52582 \n  \n  \n    38 \n    73 \n    138 \n    170 \n    167.98283 \n  \n  \n    54 \n    92 \n    138 \n    155 \n    177.39367 \n  \n  \n    69 \n    133 \n    158 \n    228 \n    213.53039 \n  \n  \n    55 \n    92 \n    119 \n    152 \n    161.02175 \n  \n  \n    45 \n    97 \n    81 \n    191 \n    129.52203 \n  \n  \n    49 \n    139 \n    118 \n    201 \n    179.97033 \n  \n  \n    72 \n    79 \n    124 \n    146 \n    161.03589 \n  \n  \n    57 \n    88 \n    93 \n    91 \n    136.94095 \n  \n  \n    59 \n    45 \n    103 \n    122 \n    127.20454 \n  \n  \n    49 \n    109 \n    109 \n    226 \n    159.24064 \n  \n  \n    61 \n    119 \n    95 \n    158 \n    152.34627 \n  \n  \n    58 \n    120 \n    136 \n    184 \n    188.04240 \n  \n  \n    57 \n    105 \n    146 \n    210 \n    190.15391 \n  \n  \n    62 \n    114 \n    131 \n    227 \n    181.43039 \n  \n  \n    40 \n    85 \n    91 \n    109 \n    132.62329 \n  \n  \n    56 \n    86 \n    129 \n    200 \n    167.16688 \n  \n  \n    50 \n    120 \n    101 \n    152 \n    157.13535 \n  \n  \n    78 \n    133 \n    142 \n    209 \n    200.36373 \n  \n  \n    22 \n    52 \n    98 \n    152 \n    123.08217 \n  \n  \n    34 \n    65 \n    59 \n    66 \n    95.83962 \n  \n  \n    60 \n    125 \n    127 \n    230 \n    182.55975 \n  \n  \n    66 \n    77 \n    133 \n    138 \n    167.50830 \n  \n  \n    47 \n    81 \n    28 \n    77 \n    76.89241 \n  \n  \n    45 \n    111 \n    87 \n    185 \n    140.75411 \n  \n  \n    40 \n    109 \n    60 \n    57 \n    116.13802 \n  \n  \n    52 \n    97 \n    56 \n    132 \n    108.41218 \n  \n  \n    69 \n    71 \n    140 \n    167 \n    171.20843 \n  \n  \n    48 \n    107 \n    52 \n    123 \n    108.95821 \n  \n  \n    37 \n    117 \n    106 \n    167 \n    159.18156 \n  \n  \n    55 \n    73 \n    114 \n    152 \n    148.49892 \n  \n  \n    57 \n    42 \n    84 \n    124 \n    109.31069 \n  \n  \n    28 \n    101 \n    92 \n    144 \n    139.47722 \n  \n  \n    52 \n    23 \n    174 \n    153 \n    178.64745 \n  \n  \n    29 \n    110 \n    117 \n    110 \n    165.07685 \n  \n  \n    44 \n    115 \n    125 \n    161 \n    175.29912 \n  \n  \n    65 \n    87 \n    116 \n    148 \n    157.02854 \n  \n  \n    79 \n    87 \n    119 \n    138 \n    160.68996 \n  \n  \n    58 \n    104 \n    156 \n    159 \n    198.45546 \n  \n  \n    82 \n    97 \n    67 \n    111 \n    120.21546 \n  \n  \n    58 \n    110 \n    95 \n    123 \n    148.23662 \n  \n  \n    47 \n    92 \n    103 \n    190 \n    146.56264 \n  \n  \n    56 \n    81 \n    133 \n    182 \n    168.47318 \n  \n  \n    77 \n    103 \n    125 \n    179 \n    172.63256 \n  \n  \n    28 \n    109 \n    157 \n    239 \n    199.19678 \n  \n  \n    54 \n    92 \n    105 \n    139 \n    148.82619 \n  \n  \n    59 \n    81 \n    67 \n    114 \n    111.56629 \n  \n  \n    21 \n    110 \n    111 \n    132 \n    159.27455 \n  \n  \n    47 \n    26 \n    56 \n    129 \n    77.41079 \n  \n  \n    43 \n    8 \n    127 \n    82 \n    130.80692 \n  \n  \n    40 \n    92 \n    55 \n    111 \n    104.47776 \n  \n  \n    44 \n    92 \n    137 \n    105 \n    175.76773 \n  \n  \n    49 \n    108 \n    95 \n    129 \n    146.68981 \n  \n  \n    45 \n    56 \n    109 \n    132 \n    136.07840 \n  \n  \n    70 \n    134 \n    138 \n    191 \n    196.72408 \n  \n  \n    36 \n    57 \n    87 \n    105 \n    116.78047 \n  \n  \n    60 \n    103 \n    52 \n    94 \n    108.14538 \n  \n  \n    57 \n    65 \n    109 \n    175 \n    140.87229 \n  \n  \n    82 \n    40 \n    77 \n    85 \n    104.28901 \n  \n  \n    35 \n    105 \n    183 \n    229 \n    220.51154 \n  \n  \n    37 \n    48 \n    100 \n    107 \n    124.22878 \n  \n  \n    49 \n    113 \n    62 \n    59 \n    120.27876 \n  \n  \n    64 \n    87 \n    109 \n    162 \n    150.89275 \n  \n  \n    57 \n    96 \n    116 \n    114 \n    160.30190 \n  \n  \n    24 \n    115 \n    69 \n    62 \n    125.30044 \n  \n  \n    48 \n    99 \n    81 \n    163 \n    130.61268 \n  \n  \n    59 \n    89 \n    125 \n    174 \n    165.22609 \n  \n  \n    48 \n    102 \n    140 \n    145 \n    182.98173 \n  \n  \n    39 \n    79 \n    169 \n    173 \n    197.48268 \n  \n  \n    35 \n    19 \n    118 \n    96 \n    127.15171 \n  \n  \n    50 \n    77 \n    100 \n    108 \n    137.72440 \n  \n  \n    54 \n    136 \n    71 \n    167 \n    138.36959 \n  \n  \n    63 \n    108 \n    143 \n    187 \n    189.30688 \n  \n  \n    19 \n    82 \n    93 \n    150 \n    131.46424 \n  \n  \n    40 \n    117 \n    61 \n    134 \n    120.45398 \n  \n  \n    43 \n    83 \n    85 \n    128 \n    126.79471 \n  \n  \n    69 \n    93 \n    116 \n    172 \n    159.92036 \n  \n  \n    49 \n    32 \n    86 \n    149 \n    106.12099 \n  \n  \n    39 \n    84 \n    107 \n    94 \n    145.96688 \n  \n  \n    49 \n    127 \n    146 \n    238 \n    199.03398 \n  \n  \n    61 \n    133 \n    144 \n    279 \n    200.80264 \n  \n  \n    70 \n    57 \n    55 \n    85 \n    91.66356 \n  \n  \n    71 \n    84 \n    175 \n    219 \n    207.26604 \n  \n  \n    59 \n    143 \n    47 \n    81 \n    120.99236 \n  \n  \n    72 \n    102 \n    82 \n    143 \n    134.59685 \n  \n  \n    72 \n    128 \n    129 \n    180 \n    186.49729 \n  \n  \n    52 \n    46 \n    101 \n    147 \n    125.37228 \n  \n  \n    3 \n    52 \n    63 \n    128 \n    91.33883 \n  \n  \n    56 \n    171 \n    143 \n    220 \n    215.94567 \n  \n  \n    52 \n    130 \n    45 \n    48 \n    113.12211 \n  \n  \n    62 \n    132 \n    75 \n    109 \n    140.71538 \n  \n  \n    38 \n    94 \n    146 \n    146 \n    183.96527 \n  \n  \n    49 \n    63 \n    75 \n    118 \n    109.96835 \n  \n  \n    86 \n    89 \n    102 \n    171 \n    147.36813 \n  \n  \n    50 \n    107 \n    102 \n    167 \n    152.39432 \n  \n  \n    33 \n    68 \n    120 \n    126 \n    149.86401 \n  \n  \n    17 \n    80 \n    65 \n    79 \n    106.21055 \n  \n  \n    41 \n    98 \n    120 \n    147 \n    163.41078 \n  \n  \n    66 \n    141 \n    102 \n    178 \n    168.27444 \n  \n  \n    84 \n    101 \n    106 \n    110 \n    155.85423 \n  \n  \n    32 \n    82 \n    98 \n    134 \n    136.78099 \n  \n  \n    35 \n    102 \n    166 \n    220 \n    204.50110 \n  \n  \n    45 \n    65 \n    81 \n    137 \n    115.72090 \n  \n  \n    41 \n    132 \n    107 \n    188 \n    166.82063 \n  \n  \n    80 \n    100 \n    58 \n    128 \n    113.56614 \n  \n  \n    55 \n    103 \n    70 \n    91 \n    123.34751 \n  \n  \n    60 \n    105 \n    84 \n    151 \n    136.70975 \n  \n  \n    62 \n    76 \n    106 \n    123 \n    143.39951 \n  \n  \n    25 \n    106 \n    85 \n    100 \n    135.34580 \n  \n  \n    57 \n    71 \n    57 \n    83 \n    98.44458 \n  \n  \n    62 \n    104 \n    117 \n    212 \n    164.99800 \n  \n  \n    53 \n    187 \n    75 \n    108 \n    163.75184 \n  \n  \n    71 \n    95 \n    113 \n    151 \n    158.33794 \n  \n  \n    26 \n    107 \n    101 \n    184 \n    149.70401 \n  \n  \n    44 \n    99 \n    100 \n    106 \n    146.75652 \n  \n  \n    76 \n    76 \n    112 \n    193 \n    149.65797 \n  \n  \n    42 \n    106 \n    160 \n    177 \n    201.56434 \n  \n  \n    57 \n    129 \n    100 \n    172 \n    160.68342 \n  \n  \n    47 \n    94 \n    100 \n    153 \n    144.82817 \n  \n  \n    58 \n    82 \n    108 \n    106 \n    147.41448 \n  \n  \n    50 \n    51 \n    85 \n    173 \n    113.52576 \n  \n  \n    65 \n    110 \n    144 \n    164 \n    191.18718 \n  \n  \n    44 \n    136 \n    79 \n    145 \n    144.53477 \n  \n  \n    43 \n    103 \n    97 \n    132 \n    145.80859 \n  \n  \n    64 \n    123 \n    85 \n    126 \n    145.64267 \n  \n  \n    50 \n    167 \n    106 \n    177 \n    181.73417 \n  \n  \n    61 \n    60 \n    112 \n    106 \n    141.61701 \n  \n  \n    52 \n    108 \n    98 \n    174 \n    149.51494 \n  \n  \n    75 \n    44 \n    108 \n    130 \n    132.31808 \n  \n  \n    67 \n    123 \n    110 \n    226 \n    167.51278 \n  \n  \n    39 \n    53 \n    102 \n    129 \n    128.26862 \n  \n  \n    52 \n    102 \n    81 \n    156 \n    132.21064 \n  \n  \n    48 \n    91 \n    43 \n    86 \n    94.26651 \n  \n  \n    76 \n    97 \n    138 \n    154 \n    181.22267 \n  \n  \n    52 \n    65 \n    108 \n    169 \n    139.62648 \n  \n  \n    39 \n    134 \n    141 \n    195 \n    196.96431 \n  \n  \n    39 \n    104 \n    80 \n    106 \n    131.21919 \n  \n  \n    49 \n    104 \n    27 \n    53 \n    86.09835 \n  \n  \n    68 \n    90 \n    117 \n    135 \n    159.41616 \n  \n  \n    56 \n    138 \n    141 \n    256 \n    199.98189 \n  \n  \n    61 \n    76 \n    77 \n    110 \n    118.21873 \n  \n  \n    88 \n    85 \n    81 \n    123 \n    127.61573 \n  \n  \n    43 \n    96 \n    151 \n    198 \n    189.53638 \n  \n  \n    91 \n    103 \n    110 \n    159 \n    160.71170 \n  \n  \n    26 \n    74 \n    70 \n    124 \n    108.63548 \n  \n  \n    8 \n    31 \n    126 \n    110 \n    137.19988 \n  \n  \n    36 \n    79 \n    77 \n    116 \n    117.61193 \n  \n  \n    43 \n    57 \n    85 \n    126 \n    115.58129 \n  \n  \n    54 \n    78 \n    150 \n    220 \n    181.74385 \n  \n  \n    40 \n    70 \n    123 \n    171 \n    153.85581 \n  \n  \n    17 \n    71 \n    76 \n    110 \n    111.85147 \n  \n  \n    63 \n    63 \n    96 \n    89 \n    129.21202 \n  \n  \n    67 \n    79 \n    86 \n    182 \n    127.75988 \n  \n  \n    50 \n    85 \n    82 \n    169 \n    125.59242 \n  \n  \n    35 \n    139 \n    92 \n    183 \n    156.39825 \n  \n  \n    55 \n    96 \n    72 \n    157 \n    122.05988 \n  \n  \n    65 \n    50 \n    104 \n    129 \n    130.68281 \n  \n  \n    58 \n    96 \n    125 \n    171 \n    168.16906 \n  \n  \n    63 \n    89 \n    111 \n    198 \n    153.41066 \n  \n  \n    30 \n    118 \n    90 \n    125 \n    145.22976 \n  \n  \n    33 \n    160 \n    8 \n    8 \n    92.58597 \n  \n  \n    62 \n    53 \n    139 \n    163 \n    162.04743 \n  \n  \n    57 \n    133 \n    41 \n    76 \n    111.33337 \n  \n  \n    50 \n    105 \n    52 \n    107 \n    108.24769 \n  \n  \n    46 \n    113 \n    117 \n    154 \n    167.66315 \n  \n  \n    72 \n    97 \n    69 \n    114 \n    121.18656 \n  \n  \n    65 \n    123 \n    89 \n    207 \n    149.18143 \n  \n  \n    42 \n    164 \n    141 \n    295 \n    210.13095 \n  \n  \n    54 \n    87 \n    147 \n    192 \n    183.02837 \n  \n  \n    81 \n    64 \n    107 \n    159 \n    140.53427 \n  \n  \n    59 \n    120 \n    88 \n    149 \n    146.56573 \n  \n  \n    42 \n    99 \n    98 \n    156 \n    144.87310 \n  \n  \n    47 \n    78 \n    133 \n    163 \n    166.49509 \n  \n  \n    44 \n    51 \n    96 \n    135 \n    122.59210 \n  \n  \n    56 \n    60 \n    123 \n    153 \n    150.75937 \n  \n  \n    22 \n    96 \n    79 \n    76 \n    125.61078 \n  \n  \n    55 \n    63 \n    47 \n    106 \n    86.18543 \n  \n  \n    56 \n    98 \n    48 \n    115 \n    102.22212 \n  \n  \n    50 \n    102 \n    108 \n    135 \n    155.43198 \n  \n  \n    57 \n    112 \n    94 \n    142 \n    148.15748 \n  \n  \n    62 \n    146 \n    78 \n    153 \n    149.35042 \n  \n  \n    61 \n    80 \n    183 \n    189 \n    211.70608 \n  \n  \n    81 \n    89 \n    125 \n    157 \n    166.89867 \n  \n  \n    59 \n    116 \n    109 \n    165 \n    163.01990 \n  \n  \n    41 \n    112 \n    116 \n    147 \n    165.98605 \n  \n  \n    49 \n    97 \n    83 \n    122 \n    131.55750 \n  \n  \n    33 \n    59 \n    112 \n    111 \n    139.05699 \n  \n  \n    72 \n    142 \n    64 \n    96 \n    136.26600 \n  \n  \n    38 \n    128 \n    133 \n    167 \n    187.37512 \n  \n  \n    52 \n    162 \n    140 \n    269 \n    209.16296 \n  \n  \n    61 \n    100 \n    72 \n    134 \n    124.24118 \n  \n  \n    37 \n    133 \n    165 \n    267 \n    217.15732 \n  \n  \n    71 \n    127 \n    66 \n    85 \n    131.45206 \n  \n  \n    56 \n    117 \n    152 \n    265 \n    200.44739 \n  \n  \n    62 \n    97 \n    117 \n    157 \n    161.97900 \n  \n  \n    56 \n    81 \n    183 \n    194 \n    211.75724 \n  \n  \n    50 \n    67 \n    173 \n    154 \n    196.60627 \n  \n  \n    56 \n    129 \n    98 \n    105 \n    158.87603 \n  \n  \n    29 \n    137 \n    82 \n    146 \n    146.42271 \n  \n  \n    40 \n    41 \n    143 \n    151 \n    158.66215 \n  \n  \n    63 \n    69 \n    53 \n    109 \n    94.57544 \n  \n  \n    81 \n    57 \n    99 \n    120 \n    130.58982 \n  \n  \n    67 \n    156 \n    154 \n    232 \n    219.83518 \n  \n  \n    38 \n    145 \n    103 \n    148 \n    168.73653 \n  \n  \n    52 \n    85 \n    56 \n    173 \n    103.23676 \n  \n  \n    56 \n    78 \n    100 \n    124 \n    138.61184 \n  \n  \n    42 \n    74 \n    104 \n    164 \n    139.28506 \n  \n  \n    31 \n    85 \n    43 \n    88 \n    90.38635 \n  \n  \n    50 \n    73 \n    67 \n    102 \n    107.43178 \n  \n  \n    63 \n    123 \n    85 \n    164 \n    145.56665 \n  \n  \n    75 \n    177 \n    100 \n    151 \n    182.75359 \n  \n  \n    69 \n    53 \n    49 \n    130 \n    84.66830 \n  \n  \n    42 \n    91 \n    45 \n    90 \n    95.54172 \n  \n  \n    55 \n    73 \n    70 \n    172 \n    110.40895 \n  \n  \n    57 \n    95 \n    95 \n    151 \n    141.69131 \n  \n  \n    54 \n    84 \n    110 \n    125 \n    149.70431 \n  \n  \n    57 \n    122 \n    98 \n    202 \n    155.93306 \n  \n  \n    65 \n    106 \n    96 \n    95 \n    147.90934 \n  \n  \n    40 \n    27 \n    44 \n    80 \n    66.92172 \n  \n  \n    75 \n    85 \n    105 \n    113 \n    147.40374 \n  \n  \n    53 \n    114 \n    112 \n    176 \n    164.29821 \n  \n  \n    75 \n    49 \n    70 \n    139 \n    101.57863 \n  \n  \n    44 \n    117 \n    84 \n    144 \n    140.66876 \n  \n  \n    66 \n    71 \n    91 \n    135 \n    128.56197 \n  \n  \n    52 \n    92 \n    142 \n    188 \n    180.70434 \n  \n  \n    54 \n    48 \n    131 \n    165 \n    152.35734 \n  \n  \n    57 \n    54 \n    65 \n    86 \n    98.03817 \n  \n  \n    40 \n    80 \n    80 \n    162 \n    120.94437 \n  \n  \n    66 \n    147 \n    94 \n    120 \n    163.93671 \n  \n  \n    61 \n    68 \n    64 \n    159 \n    103.51459 \n  \n  \n    27 \n    113 \n    196 \n    193 \n    234.60747 \n  \n  \n    60 \n    102 \n    139 \n    114 \n    183.02836 \n  \n  \n    49 \n    62 \n    79 \n    63 \n    112.99979 \n  \n  \n    73 \n    100 \n    74 \n    140 \n    126.88485 \n  \n  \n    57 \n    48 \n    90 \n    175 \n    117.09249 \n  \n  \n    22 \n    116 \n    152 \n    235 \n    197.43122 \n  \n  \n    34 \n    83 \n    158 \n    230 \n    189.30520 \n  \n  \n    53 \n    72 \n    174 \n    202 \n    199.85646 \n  \n  \n    43 \n    96 \n    168 \n    201 \n    204.25296 \n  \n  \n    10 \n    97 \n    69 \n    94 \n    116.47294 \n  \n  \n    29 \n    76 \n    97 \n    126 \n    133.09952 \n  \n  \n    69 \n    101 \n    112 \n    145 \n    159.90792 \n  \n  \n    55 \n    97 \n    40 \n    24 \n    94.78936 \n  \n  \n    71 \n    112 \n    91 \n    150 \n    146.62481 \n  \n  \n    47 \n    78 \n    127 \n    210 \n    161.30100 \n  \n  \n    54 \n    105 \n    165 \n    245 \n    206.37377 \n  \n  \n    66 \n    60 \n    146 \n    208 \n    171.43030 \n  \n  \n    47 \n    65 \n    66 \n    151 \n    102.88773 \n  \n  \n    74 \n    98 \n    55 \n    155 \n    109.65036 \n  \n  \n    43 \n    110 \n    51 \n    79 \n    109.00625 \n  \n  \n    74 \n    78 \n    93 \n    170 \n    133.92054 \n  \n  \n    61 \n    60 \n    86 \n    142 \n    119.10930 \n  \n  \n    59 \n    130 \n    68 \n    124 \n    133.56496 \n  \n  \n    38 \n    99 \n    118 \n    155 \n    161.88262 \n  \n  \n    67 \n    126 \n    164 \n    287 \n    215.55343 \n  \n  \n    39 \n    85 \n    104 \n    67 \n    143.80112 \n  \n  \n    54 \n    133 \n    72 \n    114 \n    137.94141 \n  \n  \n    70 \n    59 \n    82 \n    137 \n    115.89952 \n  \n  \n    42 \n    96 \n    130 \n    154 \n    171.28105 \n  \n  \n    64 \n    22 \n    111 \n    110 \n    124.59056 \n  \n  \n    44 \n    106 \n    72 \n    116 \n    125.53644 \n  \n  \n    33 \n    85 \n    74 \n    108 \n    117.37452 \n  \n  \n    71 \n    91 \n    67 \n    96 \n    116.79146 \n  \n  \n    63 \n    24 \n    105 \n    152 \n    120.18302 \n  \n  \n    42 \n    132 \n    43 \n    113 \n    111.49305 \n  \n  \n    47 \n    104 \n    96 \n    151 \n    145.67830 \n  \n  \n    59 \n    85 \n    132 \n    191 \n    169.56072 \n  \n  \n    50 \n    45 \n    122 \n    110 \n    142.96825 \n  \n  \n    71 \n    118 \n    149 \n    235 \n    199.42203 \n  \n  \n    46 \n    85 \n    144 \n    193 \n    178.96055 \n  \n  \n    53 \n    106 \n    134 \n    140 \n    179.89291 \n  \n  \n    68 \n    75 \n    88 \n    150 \n    127.84212 \n  \n  \n    42 \n    75 \n    137 \n    141 \n    168.28382 \n  \n  \n    61 \n    100 \n    125 \n    176 \n    170.12228 \n  \n  \n    96 \n    124 \n    115 \n    182 \n    174.47724 \n  \n  \n    73 \n    110 \n    164 \n    247 \n    209.10902 \n  \n  \n    69 \n    74 \n    115 \n    144 \n    150.86026 \n  \n  \n    64 \n    69 \n    58 \n    102 \n    98.97987 \n  \n  \n    64 \n    116 \n    113 \n    116 \n    166.86275 \n  \n  \n    40 \n    79 \n    105 \n    112 \n    142.15511 \n  \n  \n    55 \n    131 \n    96 \n    146 \n    157.93122 \n  \n  \n    62 \n    100 \n    83 \n    137 \n    133.83970 \n  \n  \n    37 \n    86 \n    157 \n    192 \n    189.96146 \n  \n  \n    36 \n    120 \n    130 \n    161 \n    181.17574 \n  \n  \n    36 \n    60 \n    86 \n    104 \n    117.20864 \n  \n  \n    57 \n    53 \n    101 \n    147 \n    128.77141 \n  \n  \n    18 \n    103 \n    33 \n    128 \n    88.50434 \n  \n  \n    72 \n    73 \n    103 \n    133 \n    140.26888 \n  \n  \n    11 \n    19 \n    104 \n    79 \n    113.20755 \n  \n  \n    58 \n    123 \n    101 \n    174 \n    159.03742 \n  \n  \n    68 \n    83 \n    44 \n    108 \n    93.20243 \n  \n  \n    43 \n    74 \n    180 \n    193 \n    205.15286 \n  \n  \n    10 \n    159 \n    84 \n    179 \n    156.19785 \n  \n  \n    39 \n    95 \n    41 \n    59 \n    93.57605 \n  \n  \n    32 \n    96 \n    109 \n    108 \n    152.34148 \n  \n  \n    41 \n    84 \n    132 \n    138 \n    167.76096 \n  \n  \n    47 \n    43 \n    87 \n    158 \n    111.57876 \n  \n  \n    32 \n    104 \n    110 \n    147 \n    156.65744 \n  \n  \n    34 \n    84 \n    146 \n    133 \n    179.34831 \n  \n  \n    53 \n    110 \n    59 \n    65 \n    116.69196 \n  \n  \n    61 \n    68 \n    103 \n    149 \n    137.27616 \n  \n  \n    46 \n    82 \n    99 \n    156 \n    138.71104 \n  \n  \n    42 \n    72 \n    67 \n    89 \n    106.39228 \n  \n  \n    58 \n    152 \n    104 \n    199 \n    174.14174 \n  \n  \n    79 \n    70 \n    100 \n    144 \n    136.91016 \n  \n  \n    58 \n    56 \n    102 \n    146 \n    131.00698 \n  \n  \n    60 \n    78 \n    122 \n    137 \n    157.96093 \n  \n  \n    72 \n    106 \n    91 \n    134 \n    144.11312 \n  \n  \n    63 \n    85 \n    109 \n    170 \n    149.95415 \n  \n  \n    69 \n    33 \n    74 \n    147 \n    97.68463 \n  \n  \n    32 \n    21 \n    67 \n    142 \n    83.63646 \n  \n  \n    50 \n    87 \n    98 \n    199 \n    140.30589 \n  \n  \n    43 \n    104 \n    141 \n    230 \n    184.32985 \n  \n  \n    30 \n    92 \n    52 \n    87 \n    101.12046 \n  \n  \n    58 \n    84 \n    166 \n    186 \n    198.48657 \n  \n  \n    84 \n    86 \n    97 \n    171 \n    141.59381 \n  \n  \n    58 \n    70 \n    85 \n    114 \n    122.32839 \n  \n  \n    55 \n    53 \n    106 \n    136 \n    132.94777 \n  \n  \n    58 \n    83 \n    156 \n    155 \n    189.39847 \n  \n  \n    83 \n    109 \n    146 \n    172 \n    193.85573 \n  \n  \n    44 \n    103 \n    91 \n    167 \n    140.69053 \n  \n  \n    54 \n    51 \n    102 \n    117 \n    128.54644 \n  \n  \n    47 \n    81 \n    82 \n    123 \n    123.63920 \n  \n  \n    40 \n    103 \n    130 \n    189 \n    174.14799 \n  \n  \n    42 \n    82 \n    27 \n    64 \n    76.07789 \n  \n  \n    30 \n    160 \n    66 \n    155 \n    142.56740 \n  \n  \n    54 \n    139 \n    74 \n    110 \n    142.26049 \n  \n  \n    50 \n    111 \n    44 \n    35 \n    103.90995 \n  \n  \n    69 \n    143 \n    84 \n    178 \n    153.78283 \n  \n  \n    59 \n    109 \n    137 \n    184 \n    184.23997 \n  \n  \n    66 \n    72 \n    138 \n    140 \n    169.68028 \n  \n  \n    40 \n    119 \n    110 \n    178 \n    163.73493 \n  \n  \n    53 \n    52 \n    130 \n    146 \n    153.14078 \n  \n  \n    41 \n    103 \n    177 \n    248 \n    214.91104 \n  \n  \n    37 \n    121 \n    162 \n    222 \n    209.38485 \n  \n  \n    42 \n    60 \n    167 \n    178 \n    187.78498 \n  \n  \n    65 \n    108 \n    144 \n    224 \n    190.32461 \n  \n  \n    50 \n    119 \n    155 \n    204 \n    203.45085 \n  \n  \n    72 \n    76 \n    92 \n    151 \n    132.04024 \n  \n  \n    43 \n    67 \n    117 \n    118 \n    147.59594 \n  \n  \n    45 \n    130 \n    157 \n    276 \n    209.54622 \n  \n  \n    53 \n    73 \n    119 \n    132 \n    152.67528 \n  \n  \n    80 \n    97 \n    101 \n    73 \n    149.49657 \n  \n  \n    43 \n    89 \n    74 \n    113 \n    119.85993 \n  \n  \n    29 \n    79 \n    125 \n    135 \n    158.63245 \n  \n  \n    44 \n    88 \n    160 \n    170 \n    193.95325 \n  \n  \n    70 \n    139 \n    90 \n    141 \n    157.32780 \n  \n  \n    45 \n    54 \n    39 \n    127 \n    74.61815 \n  \n  \n    49 \n    96 \n    93 \n    143 \n    139.78303 \n  \n  \n    55 \n    114 \n    94 \n    140 \n    148.86800 \n  \n  \n    39 \n    95 \n    130 \n    232 \n    170.62168 \n  \n  \n    36 \n    101 \n    135 \n    183 \n    177.30972 \n  \n  \n    59 \n    58 \n    113 \n    154 \n    141.46807 \n  \n  \n    44 \n    146 \n    98 \n    137 \n    165.29557 \n  \n  \n    41 \n    45 \n    56 \n    102 \n    85.14906 \n  \n  \n    39 \n    123 \n    145 \n    180 \n    195.68289 \n  \n  \n    54 \n    74 \n    51 \n    103 \n    94.31627 \n  \n  \n    73 \n    119 \n    119 \n    224 \n    174.03493 \n  \n  \n    47 \n    88 \n    43 \n    114 \n    92.89663 \n  \n  \n    52 \n    85 \n    80 \n    113 \n    124.01311 \n  \n  \n    61 \n    47 \n    158 \n    137 \n    175.83164 \n  \n  \n    56 \n    70 \n    61 \n    127 \n    101.39999 \n  \n  \n    70 \n    79 \n    108 \n    106 \n    147.03294 \n  \n  \n    28 \n    108 \n    92 \n    170 \n    142.49622 \n  \n  \n    47 \n    54 \n    71 \n    90 \n    102.47200 \n  \n  \n    66 \n    97 \n    115 \n    145 \n    160.55174 \n  \n  \n    29 \n    105 \n    106 \n    153 \n    153.39793 \n  \n  \n    70 \n    130 \n    120 \n    186 \n    179.41667 \n  \n  \n    71 \n    93 \n    118 \n    111 \n    161.80378 \n  \n  \n    42 \n    111 \n    142 \n    202 \n    188.13850 \n  \n  \n    38 \n    130 \n    113 \n    158 \n    170.92407 \n  \n  \n    50 \n    116 \n    131 \n    183 \n    181.38065 \n  \n  \n    49 \n    106 \n    138 \n    202 \n    183.05153 \n  \n  \n    58 \n    100 \n    55 \n    117 \n    109.29652 \n  \n  \n    37 \n    105 \n    167 \n    206 \n    206.81269 \n  \n  \n    54 \n    16 \n    95 \n    138 \n    107.39169 \n  \n  \n    64 \n    123 \n    90 \n    198 \n    149.97108 \n  \n  \n    44 \n    109 \n    59 \n    68 \n    115.57644 \n  \n  \n    30 \n    85 \n    98 \n    155 \n    137.92279 \n  \n  \n    67 \n    79 \n    68 \n    134 \n    112.17762 \n  \n  \n    52 \n    124 \n    104 \n    193 \n    161.60959 \n  \n  \n    50 \n    114 \n    65 \n    102 \n    123.38311 \n  \n  \n    58 \n    46 \n    74 \n    121 \n    102.45505 \n  \n  \n    38 \n    71 \n    85 \n    192 \n    121.23915 \n  \n  \n    28 \n    137 \n    84 \n    215 \n    148.07805 \n  \n  \n    59 \n    74 \n    62 \n    162 \n    104.21889 \n  \n  \n    58 \n    113 \n    40 \n    89 \n    101.91801 \n  \n  \n    79 \n    94 \n    51 \n    166 \n    104.84263 \n  \n  \n    57 \n    125 \n    101 \n    156 \n    159.82396 \n  \n  \n    99 \n    76 \n    105 \n    110 \n    145.34680 \n  \n  \n    59 \n    110 \n    137 \n    218 \n    184.67126 \n  \n  \n    34 \n    38 \n    135 \n    118 \n    149.98669 \n  \n  \n    54 \n    76 \n    177 \n    205 \n    204.25467 \n  \n  \n    61 \n    132 \n    60 \n    140 \n    127.65413 \n  \n  \n    48 \n    60 \n    106 \n    152 \n    135.43458 \n  \n  \n    63 \n    146 \n    123 \n    186 \n    188.38210 \n  \n  \n    53 \n    125 \n    45 \n    96 \n    111.04171 \n  \n  \n    41 \n    147 \n    96 \n    98 \n    163.76741 \n  \n  \n    23 \n    78 \n    66 \n    124 \n    106.66981 \n  \n  \n    40 \n    107 \n    144 \n    155 \n    187.99267 \n  \n  \n    30 \n    52 \n    152 \n    161 \n    170.43716 \n  \n  \n    53 \n    124 \n    69 \n    110 \n    131.38677 \n  \n  \n    58 \n    137 \n    137 \n    211 \n    196.23994 \n  \n  \n    69 \n    68 \n    127 \n    184 \n    158.66072 \n  \n  \n    32 \n    65 \n    84 \n    147 \n    117.32960 \n  \n  \n    74 \n    104 \n    60 \n    104 \n    116.56648 \n  \n  \n    58 \n    69 \n    55 \n    131 \n    95.92667 \n  \n  \n    75 \n    124 \n    152 \n    208 \n    204.91089 \n  \n  \n    14 \n    59 \n    159 \n    174 \n    178.29951 \n  \n  \n    49 \n    108 \n    123 \n    178 \n    170.92889 \n  \n  \n    52 \n    55 \n    145 \n    149 \n    167.34383 \n  \n  \n    52 \n    71 \n    79 \n    106 \n    117.10943 \n  \n  \n    53 \n    124 \n    110 \n    150 \n    166.87970 \n  \n  \n    61 \n    100 \n    131 \n    170 \n    175.31637 \n  \n  \n    38 \n    154 \n    79 \n    115 \n    151.84175 \n  \n  \n    62 \n    84 \n    91 \n    116 \n    133.86458 \n  \n  \n    45 \n    109 \n    96 \n    151 \n    147.68267 \n  \n  \n    21 \n    123 \n    132 \n    192 \n    183.06057 \n  \n  \n    37 \n    90 \n    97 \n    110 \n    139.74572 \n  \n  \n    75 \n    121 \n    138 \n    171 \n    191.49750 \n  \n  \n    52 \n    62 \n    130 \n    130 \n    157.37761 \n  \n  \n    31 \n    163 \n    138 \n    235 \n    206.26633 \n  \n  \n    55 \n    97 \n    116 \n    101 \n    160.58114 \n  \n  \n    47 \n    85 \n    155 \n    201 \n    188.55907 \n  \n  \n    57 \n    110 \n    103 \n    149 \n    155.08604 \n  \n  \n    61 \n    100 \n    111 \n    187 \n    158.00274 \n  \n  \n    50 \n    94 \n    128 \n    120 \n    169.29532 \n  \n  \n    40 \n    104 \n    79 \n    110 \n    130.42954 \n  \n  \n    71 \n    57 \n    85 \n    111 \n    117.71002 \n  \n  \n    50 \n    119 \n    48 \n    82 \n    110.82296 \n  \n  \n    54 \n    78 \n    74 \n    99 \n    115.95208 \n  \n  \n    36 \n    112 \n    77 \n    118 \n    131.84435 \n  \n  \n    32 \n    117 \n    108 \n    191 \n    160.53279 \n  \n  \n    61 \n    75 \n    123 \n    145 \n    157.60878 \n  \n  \n    38 \n    55 \n    112 \n    136 \n    137.71198 \n  \n  \n    58 \n    90 \n    173 \n    243 \n    207.13405 \n  \n  \n    31 \n    93 \n    63 \n    152 \n    111.15026 \n  \n  \n    60 \n    117 \n    75 \n    120 \n    134.09404 \n  \n  \n    34 \n    95 \n    155 \n    190 \n    191.88358 \n  \n  \n    44 \n    112 \n    90 \n    206 \n    143.70642 \n  \n  \n    63 \n    102 \n    138 \n    198 \n    182.39076 \n  \n  \n    59 \n    46 \n    98 \n    139 \n    123.30742 \n  \n  \n    35 \n    96 \n    89 \n    113 \n    135.25593 \n  \n  \n    60 \n    96 \n    115 \n    114 \n    159.66430 \n  \n  \n    53 \n    89 \n    94 \n    108 \n    137.93381 \n  \n  \n    57 \n    144 \n    79 \n    163 \n    148.97340 \n  \n  \n    39 \n    46 \n    103 \n    145 \n    126.11531 \n  \n  \n    43 \n    75 \n    54 \n    109 \n    96.50831 \n  \n  \n    46 \n    109 \n    67 \n    121 \n    122.65395 \n  \n  \n    33 \n    118 \n    108 \n    114 \n    161.04010 \n  \n  \n    35 \n    62 \n    161 \n    173 \n    182.92128 \n  \n  \n    56 \n    57 \n    84 \n    186 \n    115.70395 \n  \n  \n    65 \n    87 \n    78 \n    134 \n    124.13266 \n  \n  \n    49 \n    36 \n    112 \n    143 \n    130.35385 \n  \n  \n    38 \n    84 \n    63 \n    141 \n    107.80088 \n  \n  \n    46 \n    58 \n    77 \n    99 \n    109.31520 \n  \n  \n    75 \n    69 \n    116 \n    141 \n    150.02567 \n  \n  \n    43 \n    138 \n    102 \n    167 \n    165.23199 \n  \n  \n    90 \n    97 \n    52 \n    122 \n    107.83846 \n  \n  \n    60 \n    112 \n    164 \n    243 \n    208.98325 \n  \n  \n    32 \n    124 \n    140 \n    213 \n    191.25359 \n  \n  \n    71 \n    83 \n    89 \n    162 \n    132.38617 \n  \n  \n    57 \n    121 \n    72 \n    124 \n    132.99406 \n  \n  \n    58 \n    139 \n    53 \n    86 \n    124.38528 \n  \n  \n    66 \n    118 \n    139 \n    174 \n    190.38509 \n  \n  \n    53 \n    81 \n    133 \n    151 \n    168.24510 \n  \n  \n    64 \n    42 \n    124 \n    111 \n    144.47013 \n  \n  \n    59 \n    110 \n    139 \n    179 \n    186.40262 \n  \n  \n    62 \n    132 \n    136 \n    242 \n    193.52193 \n  \n  \n    49 \n    111 \n    154 \n    218 \n    199.05886 \n  \n  \n    70 \n    60 \n    166 \n    165 \n    189.04803 \n  \n  \n    56 \n    124 \n    108 \n    148 \n    165.37642 \n  \n  \n    45 \n    134 \n    176 \n    287 \n    227.71931 \n  \n  \n    69 \n    87 \n    144 \n    182 \n    181.57172 \n  \n  \n    56 \n    105 \n    151 \n    164 \n    194.40629 \n  \n  \n    68 \n    42 \n    89 \n    128 \n    114.47539 \n  \n  \n    68 \n    97 \n    76 \n    111 \n    126.94223 \n  \n  \n    44 \n    123 \n    119 \n    212 \n    173.55531 \n  \n  \n    33 \n    87 \n    138 \n    208 \n    173.64069 \n  \n  \n    66 \n    81 \n    37 \n    119 \n    86.12804 \n  \n  \n    44 \n    149 \n    89 \n    154 \n    158.79830 \n  \n  \n    68 \n    116 \n    104 \n    131 \n    159.37572 \n  \n  \n    56 \n    77 \n    73 \n    89 \n    114.80716 \n  \n  \n    8 \n    101 \n    79 \n    138 \n    126.70284 \n  \n  \n    74 \n    100 \n    81 \n    126 \n    133.02065 \n  \n  \n    47 \n    107 \n    78 \n    129 \n    131.38989 \n  \n  \n    29 \n    66 \n    29 \n    83 \n    69.92034 \n  \n  \n    68 \n    96 \n    84 \n    108 \n    133.43639 \n  \n  \n    46 \n    100 \n    63 \n    100 \n    115.30965 \n  \n  \n    49 \n    103 \n    130 \n    150 \n    174.83223 \n  \n  \n    43 \n    77 \n    77 \n    106 \n    117.28155 \n  \n  \n    68 \n    149 \n    152 \n    190 \n    215.16084 \n  \n  \n    64 \n    6 \n    108 \n    67 \n    115.09295 \n  \n  \n    49 \n    57 \n    88 \n    140 \n    118.63449 \n  \n  \n    72 \n    125 \n    88 \n    155 \n    149.71050 \n  \n  \n    56 \n    48 \n    69 \n    119 \n    98.83716 \n  \n  \n    39 \n    59 \n    110 \n    173 \n    137.78178 \n  \n  \n    48 \n    94 \n    114 \n    189 \n    157.02373 \n  \n  \n    70 \n    92 \n    141 \n    203 \n    181.20713 \n  \n  \n    46 \n    90 \n    105 \n    170 \n    147.35541 \n  \n  \n    57 \n    65 \n    103 \n    151 \n    135.67820 \n  \n  \n    62 \n    92 \n    120 \n    169 \n    162.41962 \n  \n  \n    62 \n    64 \n    88 \n    129 \n    122.64183 \n  \n  \n    55 \n    64 \n    139 \n    94 \n    166.25939 \n  \n  \n    45 \n    132 \n    105 \n    205 \n    165.39337 \n  \n  \n    37 \n    125 \n    118 \n    165 \n    173.02002 \n  \n  \n    46 \n    39 \n    125 \n    132 \n    142.67348 \n  \n  \n    31 \n    75 \n    80 \n    88 \n    118.10370 \n  \n  \n    61 \n    59 \n    89 \n    142 \n    121.27506 \n  \n  \n    43 \n    18 \n    152 \n    98 \n    156.76180 \n  \n  \n    67 \n    110 \n    41 \n    98 \n    102.17407 \n  \n  \n    54 \n    83 \n    67 \n    69 \n    112.04873 \n  \n  \n    17 \n    126 \n    106 \n    112 \n    161.54260 \n  \n  \n    64 \n    46 \n    124 \n    162 \n    146.19527 \n  \n  \n    49 \n    92 \n    109 \n    154 \n    151.90878 \n  \n  \n    47 \n    55 \n    66 \n    153 \n    98.57488 \n  \n  \n    61 \n    75 \n    113 \n    159 \n    148.95197 \n  \n  \n    27 \n    114 \n    88 \n    116 \n    141.54518 \n  \n  \n    52 \n    79 \n    107 \n    118 \n    144.79879 \n  \n  \n    22 \n    96 \n    91 \n    138 \n    135.99896 \n  \n  \n    66 \n    137 \n    164 \n    257 \n    220.22154 \n  \n  \n    31 \n    122 \n    113 \n    170 \n    166.94160 \n  \n  \n    42 \n    53 \n    151 \n    135 \n    170.91508 \n  \n  \n    57 \n    86 \n    91 \n    128 \n    134.34702 \n  \n  \n    60 \n    41 \n    120 \n    128 \n    140.27201 \n  \n  \n    50 \n    61 \n    121 \n    192 \n    149.00314 \n  \n  \n    68 \n    137 \n    119 \n    175 \n    181.41794 \n  \n  \n    58 \n    70 \n    159 \n    159 \n    186.38880 \n  \n  \n    39 \n    64 \n    78 \n    134 \n    112.23641 \n  \n  \n    58 \n    84 \n    95 \n    142 \n    137.02320 \n  \n  \n    72 \n    124 \n    89 \n    136 \n    150.14489 \n  \n  \n    40 \n    104 \n    106 \n    225 \n    153.80293 \n  \n  \n    43 \n    43 \n    113 \n    148 \n    133.78237 \n  \n  \n    66 \n    93 \n    136 \n    178 \n    177.00591 \n  \n  \n    53 \n    59 \n    62 \n    87 \n    97.29345 \n  \n  \n    49 \n    124 \n    123 \n    161 \n    177.82945 \n  \n  \n    70 \n    65 \n    71 \n    101 \n    108.96474 \n  \n  \n    50 \n    83 \n    104 \n    141 \n    143.77484 \n  \n  \n    59 \n    47 \n    82 \n    141 \n    109.88781 \n  \n  \n    54 \n    99 \n    89 \n    125 \n    137.99429 \n  \n  \n    52 \n    80 \n    165 \n    136 \n    195.43959 \n  \n  \n    42 \n    86 \n    91 \n    148 \n    133.20663 \n  \n  \n    58 \n    115 \n    112 \n    147 \n    165.10963 \n  \n  \n    44 \n    78 \n    55 \n    119 \n    98.74387 \n  \n  \n    80 \n    89 \n    51 \n    85 \n    102.76223 \n  \n  \n    62 \n    103 \n    139 \n    220 \n    183.61170 \n  \n  \n    50 \n    139 \n    198 \n    286 \n    249.30085 \n  \n  \n    50 \n    106 \n    133 \n    192 \n    178.79915 \n  \n  \n    50 \n    141 \n    79 \n    127 \n    147.14736 \n  \n  \n    52 \n    75 \n    37 \n    56 \n    82.47596 \n  \n  \n    63 \n    80 \n    171 \n    176 \n    201.46996 \n  \n  \n    66 \n    45 \n    130 \n    139 \n    151.11012 \n  \n  \n    50 \n    104 \n    83 \n    141 \n    134.65252 \n  \n  \n    58 \n    94 \n    39 \n    50 \n    92.85790 \n  \n  \n    53 \n    50 \n    96 \n    169 \n    122.84505 \n  \n  \n    45 \n    94 \n    94 \n    115 \n    139.48203 \n  \n  \n    35 \n    137 \n    120 \n    183 \n    179.77475 \n  \n  \n    43 \n    100 \n    128 \n    174 \n    171.35085 \n  \n  \n    38 \n    101 \n    131 \n    238 \n    173.99905 \n  \n  \n    44 \n    158 \n    147 \n    193 \n    212.88938 \n  \n  \n    56 \n    90 \n    79 \n    81 \n    125.60796 \n  \n  \n    37 \n    46 \n    163 \n    130 \n    177.90413 \n  \n  \n    47 \n    97 \n    122 \n    167 \n    165.16701 \n  \n  \n    70 \n    102 \n    119 \n    150 \n    166.47500 \n  \n  \n    50 \n    92 \n    66 \n    59 \n    114.76052 \n  \n  \n    45 \n    116 \n    79 \n    130 \n    135.98509 \n  \n  \n    53 \n    21 \n    93 \n    130 \n    107.74073 \n  \n  \n    41 \n    75 \n    137 \n    203 \n    168.20780 \n  \n  \n    49 \n    110 \n    164 \n    182 \n    207.28439 \n  \n  \n    64 \n    60 \n    97 \n    111 \n    128.85987 \n  \n  \n    67 \n    129 \n    114 \n    183 \n    173.56322 \n  \n  \n    72 \n    46 \n    99 \n    178 \n    125.16145 \n  \n  \n    67 \n    96 \n    37 \n    86 \n    92.67335 \n  \n  \n    47 \n    78 \n    128 \n    158 \n    162.16668 \n  \n  \n    47 \n    177 \n    130 \n    244 \n    206.59530 \n  \n  \n    54 \n    89 \n    151 \n    172 \n    187.35367 \n  \n  \n    56 \n    64 \n    137 \n    125 \n    164.60405 \n  \n  \n    34 \n    123 \n    104 \n    134 \n    159.80983 \n  \n  \n    57 \n    81 \n    87 \n    98 \n    128.72787 \n  \n  \n    59 \n    134 \n    107 \n    169 \n    169.05167 \n  \n  \n    75 \n    33 \n    110 \n    118 \n    129.30531 \n  \n  \n    28 \n    114 \n    56 \n    83 \n    113.91941 \n  \n  \n    31 \n    31 \n    100 \n    122 \n    116.44077 \n  \n  \n    55 \n    108 \n    95 \n    181 \n    147.14597 \n  \n  \n    41 \n    31 \n    99 \n    124 \n    116.33535 \n  \n  \n    48 \n    106 \n    156 \n    215 \n    198.55777 \n  \n  \n    40 \n    36 \n    119 \n    109 \n    135.72938 \n  \n  \n    21 \n    118 \n    110 \n    175 \n    161.85915 \n  \n  \n    38 \n    147 \n    102 \n    136 \n    168.73342 \n  \n  \n    34 \n    122 \n    111 \n    183 \n    165.43831 \n  \n  \n    62 \n    94 \n    62 \n    150 \n    113.07268 \n  \n  \n    71 \n    81 \n    137 \n    213 \n    173.07630 \n  \n  \n    78 \n    89 \n    45 \n    28 \n    97.41609 \n  \n  \n    36 \n    93 \n    123 \n    207 \n    163.47127 \n  \n  \n    31 \n    123 \n    108 \n    176 \n    163.04448 \n  \n  \n    84 \n    136 \n    115 \n    199 \n    178.74035 \n  \n  \n    34 \n    119 \n    108 \n    166 \n    161.54741 \n  \n  \n    41 \n    80 \n    68 \n    142 \n    110.63222 \n  \n  \n    45 \n    86 \n    151 \n    125 \n    185.37558 \n  \n  \n    46 \n    94 \n    112 \n    171 \n    155.14032 \n  \n  \n    55 \n    58 \n    44 \n    92 \n    81.43196 \n  \n  \n    64 \n    115 \n    61 \n    97 \n    121.41604 \n  \n  \n    48 \n    98 \n    125 \n    154 \n    168.27137 \n  \n  \n    54 \n    80 \n    74 \n    136 \n    116.81465 \n  \n  \n    66 \n    81 \n    125 \n    99 \n    162.30799 \n  \n  \n    56 \n    44 \n    105 \n    110 \n    128.27654 \n  \n  \n    46 \n    110 \n    135 \n    223 \n    181.95155 \n  \n  \n    49 \n    56 \n    74 \n    72 \n    106.08367 \n  \n  \n    66 \n    188 \n    119 \n    260 \n    203.26144 \n  \n  \n    41 \n    89 \n    79 \n    123 \n    124.03628 \n  \n  \n    50 \n    75 \n    159 \n    154 \n    187.93702 \n  \n  \n    64 \n    139 \n    92 \n    165 \n    158.60301 \n  \n  \n    46 \n    55 \n    47 \n    150 \n    82.05091 \n  \n  \n    49 \n    129 \n    109 \n    166 \n    167.86634 \n  \n  \n    78 \n    68 \n    166 \n    176 \n    193.10652 \n  \n  \n    44 \n    61 \n    116 \n    155 \n    144.21857 \n  \n  \n    80 \n    97 \n    121 \n    148 \n    166.81020 \n  \n  \n    37 \n    79 \n    120 \n    178 \n    154.91225 \n  \n  \n    58 \n    135 \n    125 \n    172 \n    184.98919 \n  \n\n\n\n\n\n\n\n\nSuppose we have a student with a 55 on Yr1, a 95 on Yr2 and a 110 on Yr3.\nFirst, what would their data look like?\n\nHypothetical.Student <- data.frame(Yr1=55, Yr2=95, Yr3=110)\nHypothetical.Student\n\n  Yr1 Yr2 Yr3\n1  55  95 110\n\n\nLet’s predict, first the single best guess – the fitted value.\nThe equation we sought estimates for was:\n\nlibrary(equatiomatic)\nequatiomatic::extract_eq(my.lm)\n\n\\[\n\\operatorname{Final} = \\alpha + \\beta_{1}(\\operatorname{Yr1}) + \\beta_{2}(\\operatorname{Yr2}) + \\beta_{3}(\\operatorname{Yr3}) + \\epsilon\n\\]\n\n\nAnd it was estimated to be\n\nequatiomatic::extract_eq(my.lm, use_coefs = TRUE, coef_digits = 4)\n\n\\[\n\\operatorname{\\widehat{Final}} = 14.146 + 0.076(\\operatorname{Yr1}) + 0.4313(\\operatorname{Yr2}) + 0.8657(\\operatorname{Yr3})\n\\]\n\n\nOur best guess for Hypothetical.Student must then be:\n\n14.145 + 0.076*55 + 0.4313*95 + 0.8657*110\n\n[1] 154.5255\n\n\nWhat does predict produce?\n\npredict(my.lm, newdata = Hypothetical.Student)\n\n       1 \n154.5245 \n\n\nThe same thing except that mine by hand was restricted to four digits.\nIn exact form, we could produce, using matrix multiplication,\n\nc(summary(my.lm)$coefficients[,1])\n\n(Intercept)         Yr1         Yr2         Yr3 \n14.14598945  0.07602621  0.43128539  0.86568123 \n\nc(1,Hypothetical.Student)\n\n[[1]]\n[1] 1\n\n$Yr1\n[1] 55\n\n$Yr2\n[1] 95\n\n$Yr3\n[1] 110\n\nc(1,55,95,110)%*%c(summary(my.lm)$coefficients[,1])\n\n         [,1]\n[1,] 154.5245\n\n\nBecause that is all predict does.\n\n\nTo produce confidence intervals, we can add the interval option. For an interval of the predicted average, we have:\n\npredict(my.lm, newdata = Hypothetical.Student, interval=\"confidence\")\n\n       fit     lwr     upr\n1 154.5245 152.551 156.498\n\n\n\n\n\nAn interval of the predicted range of data, we have\n\npredict(my.lm, newdata = Hypothetical.Student, interval=\"prediction\")\n\n       fit      lwr      upr\n1 154.5245 94.76778 214.2812\n\n\n\n\n\n\nOne great thing about R is smart prediction. So what does it do? Let’s try out the centering operations that I used.\n\nmy.lm <- lm(Final ~ scale(Yr1, scale=FALSE) + scale(Yr2, scale=FALSE) + scale(Yr3, scale=FALSE), data=ugtests)\nsummary(my.lm)\n\n\nCall:\nlm(formula = Final ~ scale(Yr1, scale = FALSE) + scale(Yr2, scale = FALSE) + \n    scale(Yr3, scale = FALSE), data = ugtests)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-92.638 -20.349   0.001  18.954  98.489 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(>|t|)    \n(Intercept)               148.96205    0.97467 152.833   <2e-16 ***\nscale(Yr1, scale = FALSE)   0.07603    0.06538   1.163    0.245    \nscale(Yr2, scale = FALSE)   0.43129    0.03251  13.267   <2e-16 ***\nscale(Yr3, scale = FALSE)   0.86568    0.02914  29.710   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 30.43 on 971 degrees of freedom\nMultiple R-squared:  0.5303,    Adjusted R-squared:  0.5289 \nF-statistic: 365.5 on 3 and 971 DF,  p-value: < 2.2e-16\n\n\n\nequatiomatic::extract_eq(my.lm, use_coefs = TRUE, coef_digits = 4)\n\n\\[\n\\operatorname{\\widehat{Final}} = 148.9621 + 0.076(\\operatorname{scale(Yr1,\\ scale\\ =\\ FALSE)}) + 0.4313(\\operatorname{scale(Yr2,\\ scale\\ =\\ FALSE)}) + 0.8657(\\operatorname{scale(Yr3,\\ scale\\ =\\ FALSE)})\n\\]\n\n\nOnly the intercept is impacted; the original intercept was the expected Final for a student that had all zeroes [possible but a very poor performance] and they’d have expected a 14.15 [the original intercept]. After the centering operation for each predictor, the average student [mean scores on Yr1, Yr2, and Yr3] could expect a score of 148.96205 – the intercept from the regression on centered data.\nNow let’s predict our Hypothetical.Student.\n\npredict(my.lm, newdata=Hypothetical.Student)\n\n       1 \n154.5245 \n\n\nThe result is the same.\n\npredict(my.lm, newdata=Hypothetical.Student, interval=\"confidence\")\n\n       fit     lwr     upr\n1 154.5245 152.551 156.498\n\n\n\npredict(my.lm, newdata=Hypothetical.Student, interval=\"prediction\")\n\n       fit      lwr      upr\n1 154.5245 94.76778 214.2812\n\n\nThis works because R knows that each variable was centered, the mean was subtracted because that is what scale does when the [unfortunately named] argument scale inside the function scale is set to FALSE => we are not creating z-scores [the default is \\(\\frac{x_{i} - \\overline{x}}{sd(x)}\\)] but are just taking \\(x_{i} - \\overline{x}\\) or centering the data.\n\n\n\nLet me try a regression where Yr1, Yr2, and Yr3 are allowed to have effects with curvature using each term and its square.\n\nmy.lm.Sq <- lm(Final ~ Yr1 + Yr1^2 + Yr2 + Yr2^2 + Yr3 + Yr3^2, data=ugtests)\nsummary(my.lm.Sq)\n\n\nCall:\nlm(formula = Final ~ Yr1 + Yr1^2 + Yr2 + Yr2^2 + Yr3 + Yr3^2, \n    data = ugtests)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-92.638 -20.349   0.001  18.954  98.489 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 14.14599    5.48006   2.581  0.00999 ** \nYr1          0.07603    0.06538   1.163  0.24519    \nYr2          0.43129    0.03251  13.267  < 2e-16 ***\nYr3          0.86568    0.02914  29.710  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 30.43 on 971 degrees of freedom\nMultiple R-squared:  0.5303,    Adjusted R-squared:  0.5289 \nF-statistic: 365.5 on 3 and 971 DF,  p-value: < 2.2e-16\n\n\nUnfortunately, that did not actually include the squared terms, we still only have three lines. We could use this:\n\nmy.lm.Sq <- lm(Final ~ Yr1 + Yr1*Yr1 + Yr2 + Yr2*Yr2 + Yr3 + Yr3*Yr3, data=ugtests)\nsummary(my.lm.Sq)\n\n\nCall:\nlm(formula = Final ~ Yr1 + Yr1 * Yr1 + Yr2 + Yr2 * Yr2 + Yr3 + \n    Yr3 * Yr3, data = ugtests)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-92.638 -20.349   0.001  18.954  98.489 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 14.14599    5.48006   2.581  0.00999 ** \nYr1          0.07603    0.06538   1.163  0.24519    \nYr2          0.43129    0.03251  13.267  < 2e-16 ***\nYr3          0.86568    0.02914  29.710  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 30.43 on 971 degrees of freedom\nMultiple R-squared:  0.5303,    Adjusted R-squared:  0.5289 \nF-statistic: 365.5 on 3 and 971 DF,  p-value: < 2.2e-16\n\n\nBut that also does not work. The key is to give \\(R\\) an object for the squared term that treats Yr1, Yr2, and Yr3 as base terms to be squared; the function for this is I.\n\nmy.lm.Sq <- lm(Final ~ Yr1 + I(Yr1^2) + Yr2 + I(Yr2^2) + Yr3 + I(Yr3^2), data=ugtests)\nsummary(my.lm.Sq)\n\n\nCall:\nlm(formula = Final ~ Yr1 + I(Yr1^2) + Yr2 + I(Yr2^2) + Yr3 + \n    I(Yr3^2), data = ugtests)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-94.292 -19.764  -0.006  18.961  93.503 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 24.4243935 13.4033492   1.822   0.0687 .  \nYr1          0.2023539  0.3209042   0.631   0.5285    \nI(Yr1^2)    -0.0011955  0.0030483  -0.392   0.6950    \nYr2          0.3434989  0.1446076   2.375   0.0177 *  \nI(Yr2^2)     0.0004756  0.0007687   0.619   0.5362    \nYr3          0.6617121  0.1549276   4.271 2.14e-05 ***\nI(Yr3^2)     0.0009624  0.0007183   1.340   0.1806    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 30.45 on 968 degrees of freedom\nMultiple R-squared:  0.5314,    Adjusted R-squared:  0.5285 \nF-statistic:   183 on 6 and 968 DF,  p-value: < 2.2e-16\n\n\nDoes the curvature improve fit? We can use an F test.\n\nanova(my.lm, my.lm.Sq)\n\nAnalysis of Variance Table\n\nModel 1: Final ~ scale(Yr1, scale = FALSE) + scale(Yr2, scale = FALSE) + \n    scale(Yr3, scale = FALSE)\nModel 2: Final ~ Yr1 + I(Yr1^2) + Yr2 + I(Yr2^2) + Yr3 + I(Yr3^2)\n  Res.Df    RSS Df Sum of Sq      F Pr(>F)\n1    971 899371                           \n2    968 897272  3    2098.6 0.7547 0.5197\n\n\nWe cannot tell the two models apart so the squared terms do not improve the model. That wasn’t really the goal, it was to illustrate Smart Prediction.\nWhat would we expect, given this new model, for our hypothetical student?\n\npredict(my.lm.Sq, newdata=Hypothetical.Student)\n\n       1 \n153.2958 \n\n\nI will forego the intervals but it just works because R knows to square each of Yr1, Yr2, and Yr3.\n\n\n\nSuppose that Yr1 and Yr2 have an interactive effect. We could use the I() construct but we do not have to. The regression would be:\n\nmy.lm.Int <- lm(Final ~ Yr1 + Yr2 + Yr1*Yr2 + Yr3, data=ugtests)\nsummary(my.lm.Int)\n\n\nCall:\nlm(formula = Final ~ Yr1 + Yr2 + Yr1 * Yr2 + Yr3, data = ugtests)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-94.745 -20.774  -0.013  19.112  98.618 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  2.961723  11.930732   0.248    0.804    \nYr1          0.290161   0.213180   1.361    0.174    \nYr2          0.550808   0.117829   4.675 3.36e-06 ***\nYr3          0.866264   0.029141  29.727  < 2e-16 ***\nYr1:Yr2     -0.002295   0.002175  -1.055    0.292    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 30.43 on 970 degrees of freedom\nMultiple R-squared:  0.5309,    Adjusted R-squared:  0.5289 \nF-statistic: 274.4 on 4 and 970 DF,  p-value: < 2.2e-16\n\nmy.lm.I <- lm(Final ~ Yr1 + Yr2 + I(Yr1*Yr2) + Yr3, data=ugtests)\nsummary(my.lm.I)\n\n\nCall:\nlm(formula = Final ~ Yr1 + Yr2 + I(Yr1 * Yr2) + Yr3, data = ugtests)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-94.745 -20.774  -0.013  19.112  98.618 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   2.961723  11.930732   0.248    0.804    \nYr1           0.290161   0.213180   1.361    0.174    \nYr2           0.550808   0.117829   4.675 3.36e-06 ***\nI(Yr1 * Yr2) -0.002295   0.002175  -1.055    0.292    \nYr3           0.866264   0.029141  29.727  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 30.43 on 970 degrees of freedom\nMultiple R-squared:  0.5309,    Adjusted R-squared:  0.5289 \nF-statistic: 274.4 on 4 and 970 DF,  p-value: < 2.2e-16\n\n\nIt is worth noting that this does not improve the fit of the model.\n\nanova(my.lm, my.lm.I)\n\nAnalysis of Variance Table\n\nModel 1: Final ~ scale(Yr1, scale = FALSE) + scale(Yr2, scale = FALSE) + \n    scale(Yr3, scale = FALSE)\nModel 2: Final ~ Yr1 + Yr2 + I(Yr1 * Yr2) + Yr3\n  Res.Df    RSS Df Sum of Sq      F Pr(>F)\n1    971 899371                           \n2    970 898340  1    1031.5 1.1137 0.2915\n\nanova(my.lm, my.lm.Int)\n\nAnalysis of Variance Table\n\nModel 1: Final ~ scale(Yr1, scale = FALSE) + scale(Yr2, scale = FALSE) + \n    scale(Yr3, scale = FALSE)\nModel 2: Final ~ Yr1 + Yr2 + Yr1 * Yr2 + Yr3\n  Res.Df    RSS Df Sum of Sq      F Pr(>F)\n1    971 899371                           \n2    970 898340  1    1031.5 1.1137 0.2915\n\n\nThe same holds for both:\n\npredict(my.lm.I, newdata=Hypothetical.Student)\n\n      1 \n154.544 \n\npredict(my.lm.Int, newdata=Hypothetical.Student)\n\n      1 \n154.544 \n\n\n\n\n\nLet me generate some fake data to showcase use cases for quadratic functions of x as determinants of y.\n\n\nI will generate \\(y\\) according to the following equation.\n\\[y = x + 2*x^2 + \\epsilon\\]\nwhere x is a sequence from -5 to 5 and \\(\\epsilon\\) is Normal(0,1).\n\nlibrary(magrittr)\n\n\nAttaching package: 'magrittr'\n\n\nThe following object is masked from 'package:purrr':\n\n    set_names\n\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n\nfake.df <- data.frame(x=seq(-5,5, by=0.05))\nfake.df$y <- fake.df$x + 2*fake.df$x^2 + rnorm(201)\n\n\n\n\nLet me plot x and y and include the estimated regression line [that does not include the squared term].\n\nfake.df %>% ggplot() + aes(x=x, y=y) + geom_point() + geom_smooth(method=\"lm\")\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nTo be transparent, here is the regression.\n\nsummary(lm(y~x, data=fake.df))\n\n\nCall:\nlm(formula = y ~ x, data = fake.df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.934 -13.695  -4.537  11.878  34.081 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  16.8327     1.0757  15.649  < 2e-16 ***\nx             0.9946     0.3708   2.682  0.00792 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.25 on 199 degrees of freedom\nMultiple R-squared:  0.0349,    Adjusted R-squared:  0.03005 \nF-statistic: 7.195 on 1 and 199 DF,  p-value: 0.007923\n\n\nNote just looking at the table would suggest we conclude that \\(y\\) is a linear function of \\(x\\) and this is, at best, partially true. It is actually a quadratic function of \\(x\\). The fit is not very good, though.\nWhat do the residuals look like?\n\nfake.df %<>% mutate(resid = lm(y~x, fake.df)$residuals)\nfake.df %>% ggplot() + aes(x=x, y=resid) + geom_point() + theme_minimal() + labs(y=\"Residuals from linear regression with only x\")\n\n\n\n\nThis is a characteristic pattern of a quadratic, a because the inflection point is at zero and \\(x\\) can be both positive or negative. Real world data is almost always a bit messier. Nevertheless, I digress. Let’s look at the regression estimates including a squared term.\n\nsummary(lm(y~x+I(x^2), fake.df))\n\n\nCall:\nlm(formula = y ~ x + I(x^2), data = fake.df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.48563 -0.65214 -0.01644  0.72777  2.29603 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.093122   0.110725  -0.841    0.401    \nx            0.994560   0.025444  39.089   <2e-16 ***\nI(x^2)       2.010986   0.009806 205.084   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.047 on 198 degrees of freedom\nMultiple R-squared:  0.9955,    Adjusted R-squared:  0.9954 \nF-statistic: 2.179e+04 on 2 and 198 DF,  p-value: < 2.2e-16\n\n\nNotice both the linear and the squared term are statistically different from zero and that the linear term has a much smaller standard error because it is far more precisely estimated. What do the residuals now look like?\n\nfake.df %<>% mutate(resid.sq = lm(y~x+I(x^2), fake.df)$residuals)\nfake.df %>% ggplot() + aes(x=x, y=resid.sq) + geom_point()\n\n\n\n\nThese are basically random with respect to \\(x\\).\nI should also point out that the residuals are well behaved now; they were not in the previous case.\n\nlibrary(gvlma)\ngvlma(lm(y~x, data=fake.df))\n\n\nCall:\nlm(formula = y ~ x, data = fake.df)\n\nCoefficients:\n(Intercept)            x  \n    16.8327       0.9946  \n\n\nASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS\nUSING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:\nLevel of Significance =  0.05 \n\nCall:\n gvlma(x = lm(y ~ x, data = fake.df)) \n\n                       Value   p-value                   Decision\nGlobal Stat        2.195e+02 0.0000000 Assumptions NOT satisfied!\nSkewness           1.300e+01 0.0003115 Assumptions NOT satisfied!\nKurtosis           6.447e+00 0.0111111 Assumptions NOT satisfied!\nLink Function      2.001e+02 0.0000000 Assumptions NOT satisfied!\nHeteroscedasticity 8.725e-07 0.9992547    Assumptions acceptable.\n\n\nVersus\n\ngvlma(lm(y~x+I(x^2), data=fake.df))\n\n\nCall:\nlm(formula = y ~ x + I(x^2), data = fake.df)\n\nCoefficients:\n(Intercept)            x       I(x^2)  \n   -0.09312      0.99456      2.01099  \n\n\nASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS\nUSING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:\nLevel of Significance =  0.05 \n\nCall:\n gvlma(x = lm(y ~ x + I(x^2), data = fake.df)) \n\n                     Value p-value                Decision\nGlobal Stat        2.63452  0.6207 Assumptions acceptable.\nSkewness           0.49801  0.4804 Assumptions acceptable.\nKurtosis           1.39940  0.2368 Assumptions acceptable.\nLink Function      0.70078  0.4025 Assumptions acceptable.\nHeteroscedasticity 0.03634  0.8488 Assumptions acceptable."
  }
]