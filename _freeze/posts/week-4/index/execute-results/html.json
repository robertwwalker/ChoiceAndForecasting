{
  "hash": "76bcaae9ed4b9b1d8f6b02b04651472e",
  "result": {
    "markdown": "---\ntitle: \"Week 4: Ordered and Multinomial Logistic Regression\"\nauthor: \"Robert W. Walker\"\ndate: \"2022-09-19\"\ncategories: [R]\nimage: \"image.png\"\ntoc: true\nexecute: \n  echo: fenced\n---\n\n\nThe slides [are here.](https://robertwwalker.github.io/xaringan/CMF-Week-4/).\n\nOur fourth class meeting will focus on [Chapter 6](https://peopleanalytics-regression-book.org/multinomial-logistic-regression-for-nominal-category-outcomes.html) and [Chapter 7](https://peopleanalytics-regression-book.org/ord-reg.html)  of __Handbook of Regression Modeling in People Analytics__.  The video will be on youtube.\n\n## The Skinny\n\nWith qualitative outcomes that are nominal (Chapter 6) or ordered (Chapter 7); linear regressions and/or binary GLMs are insufficiently flexible and/or rich to confront the problem.  In the nominal case, how one chose to turn the categories to values is unknown and, in the ordered case, the idea that unit distance separates the categories is an arbitrary assumption that is unlikely to be true.\n\n## Multinomial Models\n\n## A Bit on Random Utility\n\nThere's a great [blog post](https://khakieconomics.github.io/2019/03/17/The-logit-choice-model.html) that I found that details this.  In a classic paper for which [among many others], Daniel L. McFadden was awarded the Nobel Prize in Economics, he develops a multinomial/conditional logistic regression model for the study of nominal outcomes.  The core statistics demonstration is that, if random utility for options is described by a Gumbel/Type I extreme value distribution, then the difference in utility has a logistic distribution.  From this observation, one can develop random utility models for unordered choices that follows from utility maximization.  In short, we can use microeconomic foundations to motivate the models that follow.\n\n## A Bit On Model Specification\n\nThere are two ways to think about such models.  They can be motivated by choice-specific covariates or by chooser specific covariates [or a combination of both].  In general, if the covariates are chooser-specific, we call it a multinomial logit model while, if the covariates are choice specific, we call it conditional logit or conditional logistic regression.  McFadden's paper is built around transportation choices.\n\n## One Key Assumption to McFadden's Approach {.smaller}\n\nThe **independence of irrelevant alternatives** contends that, when people choose among a set of alternatives, the odds of choosing option A over option B should not depend on the presence or absence of unchosen alternative C.  Paul Allison, a famous emeritus sociologist at the University of Pennsylvania, has [a nice blog post on this](https://statisticalhorizons.com/iia/) that is well worth the time.\n\nImagine the following scenario; you are out to dinner and you are given menus.  One of your companions is excited to choose a steak from the menu.  The server arrives and announces the specials of the day; your companion then decides that a pork chop option from the menu is preferable.  Perhaps they were originally indifferent between the steak and the pork chop.  Nevertheless, it would appear as though the presentation of irrelevant alternatives -- the specials -- had a nontrivial effect on your companion's choices.\n\n## Multinomial [Unordered] Outcomes\n\nAn example: exchange rate regimes and the vanishing middle.\n\nThere is a sizable literature on how countries structure markets for currency exchange.  There are two polar approaches: \n\n1. Fixed: the price is fixed and central banks offer the needed quantities to maintain a given price.\n2. Flexible: the quantities are fixed and the price adjusts.\n\nbut there is also a third: **intermediate regimes**; things like floating pegs, the ERM snake, and others that are mixtures of the two.  The literature in international monetary economics highlights these as prone to instability.\n\n## The data\n\n![The Data](./img/Screen Shot 2022-09-19 at 1.46.14 PM.png)\n\n## Loading the Data\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r, message=FALSE, warning=FALSE}}\nlibrary(foreign); library(tidyverse)\nEXRT.data <- read.dta(\"./data/rr_dal_try1.dta\")\ntable(EXRT.data$regime2a)\nEXRT.data$regime2aF <- as.factor(EXRT.data$regime2a)\nEXRT.data$regime2aF <- relevel(EXRT.data$regime2aF, ref = \"1\")\n```\n````\n\n::: {.cell-output .cell-output-stdout}\n```\n\n   0    1    2 \n1016  240  206 \n```\n:::\n:::\n\n- 0 is fixed.\n- 1 is intermediate\n- 2 is flexible/floating\n\n## A Model\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r, results=\"hide\", message=FALSE, warning=FALSE}}\nlibrary(nnet); library(stargazer)\nmulti_model <- multinom(\n  formula = regime2aF ~ probirch + dumirch +  fix_l +float_l, maxit=500, \n  data = EXRT.data\n)\n```\n````\n:::\n\n\n## Result {.smaller}\n\n\n\n````{.cell-code}\n```{{r, results=\"asis\", message=FALSE, warning=FALSE}}\nstargazer(multi_model, type = \"html\")\n```\n````\n\n\n<table style=\"text-align:center\"><tr><td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\"></td><td colspan=\"2\"><em>Dependent variable:</em></td></tr>\n<tr><td></td><td colspan=\"2\" style=\"border-bottom: 1px solid black\"></td></tr>\n<tr><td style=\"text-align:left\"></td><td>0</td><td>2</td></tr>\n<tr><td style=\"text-align:left\"></td><td>(1)</td><td>(2)</td></tr>\n<tr><td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\">probirch</td><td>18.003<sup>***</sup></td><td>15.319<sup>**</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(5.632)</td><td>(5.982)</td></tr>\n<tr><td style=\"text-align:left\"></td><td></td><td></td></tr>\n<tr><td style=\"text-align:left\">dumirch</td><td>-23.831<sup>***</sup></td><td>-9.226</td></tr>\n<tr><td style=\"text-align:left\"></td><td>(7.078)</td><td>(7.483)</td></tr>\n<tr><td style=\"text-align:left\"></td><td></td><td></td></tr>\n<tr><td style=\"text-align:left\">fix_l</td><td>6.076<sup>***</sup></td><td>3.347<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.368)</td><td>(0.431)</td></tr>\n<tr><td style=\"text-align:left\"></td><td></td><td></td></tr>\n<tr><td style=\"text-align:left\">float_l</td><td>2.763<sup>***</sup></td><td>4.992<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.443)</td><td>(0.424)</td></tr>\n<tr><td style=\"text-align:left\"></td><td></td><td></td></tr>\n<tr><td style=\"text-align:left\">Constant</td><td>-2.945<sup>***</sup></td><td>-3.277<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.343)</td><td>(0.385)</td></tr>\n<tr><td style=\"text-align:left\"></td><td></td><td></td></tr>\n<tr><td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\">Akaike Inf. Crit.</td><td>967.448</td><td>967.448</td></tr>\n<tr><td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\"><em>Note:</em></td><td colspan=\"2\" style=\"text-align:right\"><sup>*</sup>p<0.1; <sup>**</sup>p<0.05; <sup>***</sup>p<0.01</td></tr>\n</table>\n\n\n## A Bit of Interpretation\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r, message=FALSE, warning=FALSE}}\nEXRT.data <- EXRT.data %>% select(regime2aF,probirch, dumirch,fix_l,float_l)\nsummary(EXRT.data)\n```\n````\n\n::: {.cell-output .cell-output-stdout}\n```\n regime2aF    probirch          dumirch            fix_l       \n 1: 240    Min.   :0.00000   Min.   :0.00000   Min.   :0.0000  \n 0:1016    1st Qu.:0.01000   1st Qu.:0.00000   1st Qu.:0.0000  \n 2: 206    Median :0.04000   Median :0.00000   Median :1.0000  \n           Mean   :0.04034   Mean   :0.00754   Mean   :0.7175  \n           3rd Qu.:0.05000   3rd Qu.:0.01000   3rd Qu.:1.0000  \n           Max.   :0.34000   Max.   :0.16000   Max.   :1.0000  \n           NA's   :150       NA's   :150                       \n    float_l      \n Min.   :0.0000  \n 1st Qu.:0.0000  \n Median :0.0000  \n Mean   :0.1327  \n 3rd Qu.:0.0000  \n Max.   :1.0000  \n                 \n```\n:::\n:::\n\n\n## A Plot\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\npred.data.I <- data.frame(fix_l = 0, float_l = 0, probirch = seq(0,0.34, by=0.01), dumirch = 0)\nPreds.1 <- data.frame(predict(multi_model, newdata=pred.data.I, type=\"p\"))\nPreds.1$Irregular <- pred.data.I$probirch\nPreds.1.df <- data.frame(Preds.1)\nGraph.1 <- Preds.1.df %>% pivot_longer(., cols=c(X1,X2,X0)) %>%\nggplot(.) + aes(x=Irregular, y=value, color=name) + geom_point() + theme_minimal()\npred.data.I <- data.frame(fix_l = 0, float_l = 0, probirch = seq(0,0.34, by=0.01), dumirch = seq(0,0.34, by=0.01))\nPreds.1 <- data.frame(predict(multi_model, newdata=pred.data.I, type=\"p\"))\nPreds.1.df <- data.frame(Preds.1)\nPreds.1.df$Irregular <- pred.data.I$probirch\nGraph.2 <- Preds.1.df %>% pivot_longer(., cols=c(X1,X2,X0)) %>%\nggplot(.) + aes(x=Irregular, y=value, color=name) + geom_point() + theme_minimal()\nlibrary(patchwork)\n```\n````\n:::\n\n\n## A Picture\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\nGraph.1 + Graph.2 + plot_annotation(\n  title = 'Intermediates')\n```\n````\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n## A Plot\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\npred.data.I <- data.frame(fix_l = 1, float_l = 0, probirch = seq(0,0.34, by=0.01), dumirch = 0)\nPreds.1 <- data.frame(predict(multi_model, newdata=pred.data.I, type=\"p\"))\nPreds.1$Irregular <- pred.data.I$probirch\nPreds.1.df <- data.frame(Preds.1)\nGraph.1 <- Preds.1.df %>% pivot_longer(., cols=c(X1,X2,X0)) %>%\nggplot(.) + aes(x=Irregular, y=value, color=name) + geom_point() + theme_minimal()\npred.data.I <- data.frame(fix_l = 1, float_l = 0, probirch = seq(0,0.34, by=0.01), dumirch = seq(0,0.34, by=0.01))\nPreds.1 <- data.frame(predict(multi_model, newdata=pred.data.I, type=\"p\"))\nPreds.1.df <- data.frame(Preds.1)\nPreds.1.df$Irregular <- pred.data.I$probirch\nGraph.2 <- Preds.1.df %>% pivot_longer(., cols=c(X1,X2,X0)) %>%\nggplot(.) + aes(x=Irregular, y=value, color=name) + geom_point() + theme_minimal()\nlibrary(patchwork)\n```\n````\n:::\n\n\n## A Picture\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\nGraph.1 + Graph.2 + plot_annotation(\n  title = 'Fixes')\n```\n````\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n## A Plot\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\npred.data.I <- data.frame(fix_l = 0, float_l = 1, probirch = seq(0,0.34, by=0.01), dumirch = 0)\nPreds.1 <- data.frame(predict(multi_model, newdata=pred.data.I, type=\"p\"))\nPreds.1$Irregular <- pred.data.I$probirch\nPreds.1.df <- data.frame(Preds.1)\nGraph.1 <- Preds.1.df %>% pivot_longer(., cols=c(X1,X2,X0)) %>%\nggplot(.) + aes(x=Irregular, y=value, color=name) + geom_point() + theme_minimal()\npred.data.I <- data.frame(fix_l = 0, float_l = 1, probirch = seq(0,0.34, by=0.01), dumirch = seq(0,0.34, by=0.01))\nPreds.1 <- data.frame(predict(multi_model, newdata=pred.data.I, type=\"p\"))\nPreds.1.df <- data.frame(Preds.1)\nPreds.1.df$Irregular <- pred.data.I$probirch\nGraph.2 <- Preds.1.df %>% pivot_longer(., cols=c(X1,X2,X0)) %>%\nggplot(.) + aes(x=Irregular, y=value, color=name) + geom_point() + theme_minimal()\nlibrary(patchwork)\n```\n````\n:::\n\n\n## A Picture\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\nGraph.1 + Graph.2 + plot_annotation(\n  title = 'Floats')\n```\n````\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n## Goodness of Fit\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\nDescTools::PseudoR2(multi_model, \n                    which = c(\"McFadden\", \"CoxSnell\", \"Nagelkerke\",\"AIC\"))\n```\n````\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in DescTools::PseudoR2(multi_model, which = c(\"McFadden\", \"CoxSnell\", :\nCould not find model or data element of multinom object for evaluating PseudoR2\nnull model. Will fit null model with new evaluation of 'EXRT.data'. Ensure\nobject has not changed since initial call, or try running multinom with 'model =\nTRUE'\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n   McFadden    CoxSnell  Nagelkerke         AIC \n  0.5676473   0.6125299   0.7545319 967.4475199 \n```\n:::\n:::\n\n\n## Simpler\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r, message=FALSE, warning=FALSE}}\nmulti_model.2 <- multinom(\n  formula = regime2aF ~ fix_l +float_l, maxit=500, \n  data = EXRT.data\n)\nmulti_model.3 <- multinom(\n  formula = regime2aF ~ probirch +  fix_l +float_l, maxit=500, \n  data = EXRT.data\n)\n```\n````\n\n::: {.cell-output .cell-output-stdout}\n```\n# weights:  12 (6 variable)\ninitial  value 1606.171166 \niter  10 value 532.176055\niter  20 value 531.979184\niter  20 value 531.979183\niter  20 value 531.979183\nfinal  value 531.979183 \nconverged\n# weights:  15 (8 variable)\ninitial  value 1441.379323 \niter  10 value 482.863574\niter  20 value 480.237337\niter  30 value 480.208852\nfinal  value 480.208838 \nconverged\n```\n:::\n:::\n\n\n## Comparisons\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\nDescTools::PseudoR2(multi_model.2, \n                    which = c(\"McFadden\", \"CoxSnell\", \"Nagelkerke\",\"AIC\"))\n```\n````\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in DescTools::PseudoR2(multi_model.2, which = c(\"McFadden\",\n\"CoxSnell\", : Could not find model or data element of multinom object for\nevaluating PseudoR2 null model. Will fit null model with new evaluation of\n'EXRT.data'. Ensure object has not changed since initial call, or try running\nmultinom with 'model = TRUE'\n```\n:::\n\n````{.cell-code}\n```{{r}}\nDescTools::PseudoR2(multi_model.3, \n                    which = c(\"McFadden\", \"CoxSnell\", \"Nagelkerke\", \"AIC\"))\n```\n````\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in DescTools::PseudoR2(multi_model.3, which = c(\"McFadden\",\n\"CoxSnell\", : Could not find model or data element of multinom object for\nevaluating PseudoR2 null model. Will fit null model with new evaluation of\n'EXRT.data'. Ensure object has not changed since initial call, or try running\nmultinom with 'model = TRUE'\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n    McFadden     CoxSnell   Nagelkerke          AIC \n   0.5592956    0.6029024    0.7459794 1075.9583656 \n   McFadden    CoxSnell  Nagelkerke         AIC \n  0.5617286   0.6086804   0.7497900 976.4176765 \n```\n:::\n:::\n\n## The Best Model is the One Presented\n\nIt minimizes AIC.\n\n## Ordered Models\n\nMy preferred method of thinking about ordered regression involves latent variables.  So what is a latent variable?  It is something that is unobservable, hence latent, and we only observe coarse realizations in the form of qualitative categories.  Consider the example from [Li in the *Journal of Politics*](https://www.journals.uchicago.edu/doi/abs/10.1111/j.1468-2508.2006.00370.x).\n\n![Li Abstract](./data/Screen Shot 2022-09-19 at 9.59.09 AM.png)\n\n\n\n## The Outcome\n\nThe outcome is summed from six individual types of incentives.  They are explained here.\n\n![Tax Incentives to FDI](./data/Screen Shot 2022-09-19 at 10.09.08 AM.png)\n\nand\n\n![Tax Incentives Part 2](./data/Screen Shot 2022-09-19 at 10.10.30 AM.png)\n\n## Inputs\n\nThere are two parts to the data description for the inputs.\n\n![Part 1](./data/Screen Shot 2022-09-19 at 10.01.12 AM.png)\n\nand ![Part 2](./data/Screen Shot 2022-09-19 at 10.01.32 AM.png)\n\nand there is a further description of other variables that are deployed.\n\n![Controls: Part 1](./data/Screen Shot 2022-09-19 at 10.17.41 AM.png)\n\nand\n\n![Controls: Part 2](./data/Screen Shot 2022-09-19 at 10.18.53 AM.png)\n\n## The Data\n\nThis should give us an idea of what is going on.  The data come in Stata format; we can read these via the `foreign` or `haven` libraries in R.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\nlibrary(MASS); library(foreign)\n```\n````\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'MASS'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:patchwork':\n\n    area\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:dplyr':\n\n    select\n```\n:::\n\n````{.cell-code}\n```{{r}}\nLi.Data <- read.dta(\"./data/li-replication.dta\")\ntable(Li.Data$generosityg)\nLi.Data$generositygF <- as.factor(Li.Data$generosityg)\n```\n````\n\n::: {.cell-output .cell-output-stdout}\n```\n\n 0  1  2  3  4  5  6 \n13 10 19  7  2  1  1 \n```\n:::\n:::\n\nIt is worthwhile to notice that the top of the scale is rather sparse.\n\nThere is also a concern about FDI and economies of scale.  The following is a plot of the relationship between FDI and size of the economy in the sample.\n\n![FD-Size](./data/size-fdi.png)\n\nWithout careful attention to normalization, China is a clear `x-y` outlier.\n\n## Motivating the Model\n\nSuppose there is some unobserved continuous variable, call it $y^{*}$ that measures the willingness/utility to be derived from tax incentives to FDI.  Unfortunately, this latent quantity is unobservable; we instead observe how many incentives are offered and posit that the number of incentives is a manifestation of increasing utility with unknown points of separation -- cutpoints -- that separate these latent utilities into a mutually exclusive and exhaustive partition.  In a simplified example, consider this.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\nplot(density(rlogis(1000)))\nabline(v=c(-3,-2,-1,0,2,4))\n```\n````\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\nSo anything below -3 is zero incentives; anything between -3 and -2 is one incentive, ... , and anything above 4 should be all six incentives.  What we have is a regression problem but the outcome is unobserved and takes the form of a logistic random variable.  Indeed, one could write the equation as:\n\n\n$$y^{*} = X\\beta + \\epsilon$$\n\nwhere $\\epsilon$ is assumed to have a logistic distribution but this is otherwise just a linear regression.  Indeed, the direct interpretation of the slopes is the effect of a one-unit change in X on that logistic random variable.\n\n## What to Replicate\n\nThe table of estimates is presented in the paper; I will copy it here.\n\n![Results Table](./data/Screen Shot 2022-09-19 at 10.24.19 AM.png)\n\nI will choose two of a few models estimated in the paper.  First, let us have a look at Model 1.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\nli.mod1 <- polr(generositygF ~ law00log + transition, data=Li.Data)\nsummary(li.mod1)\n```\n````\n\n::: {.cell-output .cell-output-stderr}\n```\n\nRe-fitting to get Hessian\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nCall:\npolr(formula = generositygF ~ law00log + transition, data = Li.Data)\n\nCoefficients:\n             Value Std. Error t value\nlaw00log   -0.6192     0.4756 -1.3019\ntransition -0.5161     0.7126 -0.7243\n\nIntercepts:\n    Value   Std. Error t value\n0|1 -1.3617  0.3768    -3.6144\n1|2 -0.4888  0.3323    -1.4707\n2|3  1.1785  0.3668     3.2133\n3|4  2.3771  0.5361     4.4339\n4|5  3.1160  0.7325     4.2541\n5|6  3.8252  1.0183     3.7565\n\nResidual Deviance: 164.3701 \nAIC: 180.3701 \n```\n:::\n:::\n\n\nWe can read these by stars.  There is nothing that is clearly different from zero as a slope or 1 as an odds-ratio.  The authors deploy a common strategy for adjusting standard errors that, in this case, is necessary to find a relationship with statistical confidence.  That's a diversion.  To the story.  In general, the sign of the rule of law indicator is negative, so as rule of law increases, incentives decrease though we cannot rule out no effect.  Transitions also have a negative sign; regime changes have no clear influence on incentives.  There is additional information that is commonly given short-shrift.  What do the cutpoints separating the categories imply?  Let's think this through recongizing that the estimates have an underlying t/normal distribution.  `4|5` is within one standard error of both `3|4` and `5|6`.  The model cannot really tell these values apart.  Things do improve in the lower part of the scale but we should note that this is where the vast majority of the data are actually observed.\n\n### Odds Ratios\n\nNext, I will turn the estimates into odds-ratios by exponentiating the estimates.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\nexp(li.mod1$coefficients)\n```\n````\n\n::: {.cell-output .cell-output-stdout}\n```\n  law00log transition \n 0.5384016  0.5968309 \n```\n:::\n:::\n\n\nFor Kawika, this is one of the many cases that I am familiar with where `robust` is necessary to find something.  Note neither effect can be differentiated from zero with much confidence at all.  To further examine the claims, I will also replicate the right-most column.\n\n## Column 4 Estimates\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\nli.mod4 <- polr(generositygF ~ law00log + transition + fdiinf + democfdi + democ + autocfdi2 + autocfdir + reggengl + reggengl2 + gdppclog + gdplog, data=Li.Data)\nsummary(li.mod4)\n```\n````\n\n::: {.cell-output .cell-output-stderr}\n```\n\nRe-fitting to get Hessian\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nCall:\npolr(formula = generositygF ~ law00log + transition + fdiinf + \n    democfdi + democ + autocfdi2 + autocfdir + reggengl + reggengl2 + \n    gdppclog + gdplog, data = Li.Data)\n\nCoefficients:\n              Value Std. Error t value\nlaw00log   -0.89148    0.66806 -1.3344\ntransition -0.57123    0.94945 -0.6016\nfdiinf      0.37605    0.18055  2.0828\ndemocfdi   -0.39969    0.18228 -2.1927\ndemoc      -1.23307    0.77661 -1.5878\nautocfdi2   1.24932    1.94198  0.6433\nautocfdir  -3.17796    1.95351 -1.6268\nreggengl    1.81476    0.44754  4.0550\nreggengl2  -0.05777    0.01444 -4.0007\ngdppclog    0.20891    0.43867  0.4762\ngdplog      0.15754    0.18138  0.8686\n\nIntercepts:\n    Value    Std. Error t value \n0|1  13.7773   0.1020   135.0542\n1|2  14.7314   0.3048    48.3349\n2|3  16.9740   0.5230    32.4561\n3|4  18.7911   0.8027    23.4107\n4|5  20.0387   1.1590    17.2900\n5|6  23.2947   4.7216     4.9336\n\nResidual Deviance: 134.2212 \nAIC: 168.2212 \n(2 observations deleted due to missingness)\n```\n:::\n:::\n\n\nMeasured via odds-ratios, we can obtain those:\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\nexp(li.mod4$coefficients)\n```\n````\n\n::: {.cell-output .cell-output-stdout}\n```\n  law00log transition     fdiinf   democfdi      democ  autocfdi2  autocfdir \n0.41004648 0.56483013 1.45651991 0.67052543 0.29139805 3.48796970 0.04167039 \n  reggengl  reggengl2   gdppclog     gdplog \n6.13958891 0.94386864 1.23232943 1.17063051 \n```\n:::\n:::\n\n\n## Diagnostics and Commentary\n\nGoodness of Fit:\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\nDescTools::PseudoR2(\n  li.mod1, \n  which = c(\"McFadden\", \"CoxSnell\", \"Nagelkerke\", \"AIC\")\n)\nDescTools::PseudoR2(\n  li.mod4, \n  which = c(\"McFadden\", \"CoxSnell\", \"Nagelkerke\", \"AIC\")\n)\n```\n````\n\n::: {.cell-output .cell-output-stdout}\n```\n    McFadden     CoxSnell   Nagelkerke          AIC \n  0.01104919   0.03405653   0.03560378 180.37009764 \n   McFadden    CoxSnell  Nagelkerke         AIC \n  0.1649728   0.4054507   0.4235700 168.2212440 \n```\n:::\n:::\n\nThe last model is clearly better than the first by any of these measures.  That said, there are a lot of additional predictors that add much complexity to the model and the difference in AIC is not very large.\n\nWhat about the others?\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n# lipsitz test \n# generalhoslem::lipsitz.test(li.mod1)\n# generalhoslem::lipsitz.test(li.mod4)\n```\n````\n:::\n\n\nThey fail to work because of sparseness.\n\n### Testing Proportional-Odds\n\nThe text cites a test that owes to Brant on examining proportionality.  It turns out that I know a bit about this; I published a purely theoretical stats paper showing that it is not at all clear what the alternative hypothesis embodied in this test actually means because the only model with a proper probability distribution for $y^{*}$ is this proportional-odds model.\n\nI will follow the text with this caveat in mind:\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\nbrant::brant(li.mod1)\n```\n````\n\n::: {.cell-output .cell-output-stdout}\n```\n-------------------------------------------- \nTest for\tX2\tdf\tprobability \n-------------------------------------------- \nOmnibus\t\t13.51\t10\t0.2\nlaw00log\t8.64\t5\t0.12\ntransition\t4.28\t5\t0.51\n-------------------------------------------- \n\nH0: Parallel Regression Assumption holds\n```\n:::\n:::\n\n\n### How Well Does the Model Predict?\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\nMat.Fit <- data.frame(fitted(li.mod4))\nlibrary(tidyverse)\nMat.Fit$pred.val <- rep(-999, 51)\nMat.Fit$pred.val[Mat.Fit$X0 > Mat.Fit$X1 & Mat.Fit$X0 > Mat.Fit$X2 & Mat.Fit$X0 > Mat.Fit$X3 & Mat.Fit$X0 > Mat.Fit$X4 & Mat.Fit$X0 > Mat.Fit$X5 & Mat.Fit$X0 > Mat.Fit$X6] <- 0\nMat.Fit$pred.val[Mat.Fit$X1 > Mat.Fit$X0 & Mat.Fit$X1 > Mat.Fit$X2 & Mat.Fit$X1 > Mat.Fit$X3 & Mat.Fit$X1 > Mat.Fit$X4 & Mat.Fit$X1 > Mat.Fit$X5 & Mat.Fit$X1 > Mat.Fit$X6] <- 1\nMat.Fit$pred.val[Mat.Fit$X2 > Mat.Fit$X0 & Mat.Fit$X2 > Mat.Fit$X1 & Mat.Fit$X2 > Mat.Fit$X3 & Mat.Fit$X2 > Mat.Fit$X4 & Mat.Fit$X2 > Mat.Fit$X5 & Mat.Fit$X2 > Mat.Fit$X6] <- 2\nMat.Fit$pred.val[Mat.Fit$X3 > Mat.Fit$X0 & Mat.Fit$X3 > Mat.Fit$X1 & Mat.Fit$X3 > Mat.Fit$X2 & Mat.Fit$X3 > Mat.Fit$X4 & Mat.Fit$X3 > Mat.Fit$X5 & Mat.Fit$X3 > Mat.Fit$X6] <- 3\nMat.Fit$pred.val[Mat.Fit$X5 > Mat.Fit$X0 & Mat.Fit$X5 > Mat.Fit$X1 & Mat.Fit$X5 > Mat.Fit$X2 & Mat.Fit$X5 > Mat.Fit$X3 & Mat.Fit$X5 > Mat.Fit$X4 & Mat.Fit$X5 > Mat.Fit$X6] <- 5\nMat.Fit$pred.val[Mat.Fit$X6 > Mat.Fit$X0 & Mat.Fit$X6 > Mat.Fit$X1 & Mat.Fit$X6 > Mat.Fit$X2 & Mat.Fit$X6 > Mat.Fit$X3 & Mat.Fit$X6 > Mat.Fit$X4 & Mat.Fit$X6 > Mat.Fit$X5] <- 6\nPred.Data <- Li.Data[c(1:28,30:41,43:53),]\ntable(Pred.Data$generosityg,Mat.Fit$pred.val)\n```\n````\n\n::: {.cell-output .cell-output-stdout}\n```\n   \n     0  2  3  6\n  0  6  7  0  0\n  1  5  4  0  0\n  2  5 12  1  0\n  3  1  2  4  0\n  4  0  1  1  0\n  5  0  1  0  0\n  6  0  0  0  1\n```\n:::\n:::\n\n\nSo `6+12+4+1` or 23 of 51 are correctly predicted with a rather big and complicated model.\n\n### On AIC\n\nThe AIC [and BIC] are built around the idea of likelihood presented last time.  The formal definition, [which is correct on Wikipedia](https://en.wikipedia.org/wiki/Akaike_information_criterion) explains the following:\n\n![AIC](./data/Screen Shot 2022-09-19 at 12.06.24 PM.png)",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}