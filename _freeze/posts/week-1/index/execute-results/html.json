{
  "hash": "ac2f60e036e6151564dc1de971e249fe",
  "result": {
    "markdown": "---\ntitle: \"Week 1: R, Inference, and Regression: A Review\"\nauthor: \"Robert W. Walker\"\ndate: \"2022-08-29\"\ncategories: [R]\nimage: \"image.png\"\n---\n\n\nThe slides [are here.](https://robertwwalker.github.io/xaringan/CMF-Week-1/)  The first class video is available from `youtube` [here](https://youtu.be/RUerRvBu9qw)\n\nOur first class meeting will focus on Chapters [1](https://peopleanalytics-regression-book.org/gitbook/inf-model.html), [2](https://peopleanalytics-regression-book.org/gitbook/the-basics-of-the-r-programming-language.html), [3](https://peopleanalytics-regression-book.org/gitbook/found-stats.html); I suspect we will leave Chapter 4  of __Handbook of Regression Modeling in People Analytics__ for next time.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# url of data set \nurl <- \"http://peopleanalytics-regression-book.org/data/salespeople.csv\"\n# load the data set and store it as a dataframe called salespeople\nsalespeople <- read.csv(url)\nlibrary(GGally)\n# convert performance and promotion to categorical\nsalespeople$promoted <- as.factor(salespeople$promoted)\nsalespeople$performance <- as.factor(salespeople$performance)\n# pairplot of salespeople\nggpairs(salespeople)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n## Hypothesis Tests and Confidence Intervals\n\n# Single means with the cars data\n\nI will work with R's internal dataset on cars: `cars`.  There are two variables in the dataset, this is what they look like.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(cars)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/cars-1.png){width=672}\n:::\n:::\n\n\n\n## An Hypothesis Test\n\nI will work with the speed variable.  The hypothesis to advance is that 17 or greater is the true average speed.  The alternative must then be that the average speed is less than 17.  Knowing only the sample size, I can figure out what $t$ must be to reject 17 or greater and conclude that the true average must be less with 90% probability.  The sample mean would have to be at least `qt(0.1, 49)` standard errors below 17 to rule out a mean of 17 or greater.  Now let's see what we have.  Let me skim the data for the relevant information.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(skimr)\nskim(cars)\n```\n\n::: {.cell-output-display}\nTable: Data summary\n\n|                         |     |\n|:------------------------|:----|\n|Name                     |cars |\n|Number of rows           |50   |\n|Number of columns        |2    |\n|_______________________  |     |\n|Column type frequency:   |     |\n|numeric                  |2    |\n|________________________ |     |\n|Group variables          |None |\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate|  mean|    sd| p0| p25| p50| p75| p100|hist  |\n|:-------------|---------:|-------------:|-----:|-----:|--:|---:|---:|---:|----:|:-----|\n|speed         |         0|             1| 15.40|  5.29|  4|  12|  15|  19|   25|▂▅▇▇▃ |\n|dist          |         0|             1| 42.98| 25.77|  2|  26|  36|  56|  120|▅▇▅▂▁ |\n:::\n:::\n\n\n\nDoing the math by hand, I get:\n\n\n$$ t = \\frac{\\overline{x} - \\mu}{\\frac{s}{\\sqrt{n}}} = \\frac{15.4 - 17}{\\frac{5.29}{\\sqrt{50}}} = -2.14 $$\n\n\nInterpreting the result, the sample mean is 2.14 standard errors below the hypothetical mean of 17.  The probability of a sample mean of 15.4 [or smaller] given a true average of 17, this standard deviation and sample size is `pt(-2.14, 49)` = 0.0186798.  Notice that probability is less than 0.1; thus with at least 90% confidence, the true mean is not 17 or greater and thus must be smaller.  Assuming the hypothetical mean [17 or greater] is true, the likelihood of generating a sample mean of 15.4 is only 0.0187 and this is far less than the 10% permissible outside of 90% confidence.  Indeed, any sample mean more than 1.299 standard errors below 17 would be too small to sustain the belief that the true mean is 17 or greater because `qt(0.1, 49)` is -1.299.  Put in the original metric, any sample mean below 16.0285747 would require a rejection of the claim that the true mean is 17 or greater with 90% confidence.\n\n## The Confidence Interval\n\nThe confidence interval is always centered on the sample mean.  Rearranging the equation above and solving for $\\mu$ given the $t$ above, we get\n\n\n$$ \\mu = \\overline{x} - t(\\frac{s}{\\sqrt{n}}) = 15.4 - (-1.299*\\frac{5.29}{\\sqrt{50}}) = 16.37143 $$\n\n\nWith 90% confidence, given this sample mean, the true value should be less than 16.37143.\n\n## The native `t.test`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(cars$speed, conf.level = 0.9, alternative = \"less\", mu=17)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tOne Sample t-test\n\ndata:  cars$speed\nt = -2.1397, df = 49, p-value = 0.01869\nalternative hypothesis: true mean is less than 17\n90 percent confidence interval:\n     -Inf 16.37143\nsample estimates:\nmean of x \n     15.4 \n```\n:::\n:::\n\n\n## Simplifying?\n\n\n$$ t(\\frac{s}{\\sqrt{n}}) = \\overline{x} - \\mu $$ can lead to either:\n\n\n$$  \\overline{x} - t(\\frac{s}{\\sqrt{n}}) = \\mu $$\n\n\nor\n\n\n$$ \\overline{x} = \\mu + t(\\frac{s}{\\sqrt{n}}) $$\n\n\nSo a minus $t$ will be below $\\mu$ but above $\\overline{x}$ and a positive $t$ will be above $\\mu$ but below $\\overline{x}$.  \n1. An hypothesis test given $\\mu$ with an alternative that is less must then render an upper bound given $\\overline{x}$.  \n2. An hypothesis test given $\\mu$ with an alternative that is greater must then render a lower bound given $\\overline{x}$.\n\n## A graphical representation\n\nGiven a sample size $n$, some unknown constant $\\mu$ and satisfaction of Lindeberg's condition, the sampling distribution of the sample mean follows a $t$ distribution with degrees of freedom $n-1$.  To render a graphical representation, let's arbitrarily set n to 50, as in the above example.  Here is a plot.\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(seq(-5,5, by=0.01), dt(seq(-5,5, by=0.01), df=49), xlab=expression(paste(\"x-bar -\",mu,\" (measured in std. errors of the mean)\", sep=\"\")), ylab=\"Density\", type=\"l\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n### Inverting the `scale` transformation\n\nWe can now reverse the scale by the standard error of the mean.  In the above example, it is 0.7478.  Measured in miles per hour, we obtain:\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(seq(-5,5, by=0.01)*0.7478, dt(seq(-5,5, by=0.01), df=49), xlab=expression(paste(\"x-bar -\",mu,\" (measured in mph)\", sep=\"\")), ylab=\"Density\", type=\"l\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\nNow we will take the concrete example above.  \n\n### The Hypothesis Test\n\nWe claim that the true mean is 17 or greater.  Now we need center the distribution above as though the claim is true.\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(x=17+seq(-5,5, by=0.01)*0.7478, dt(seq(-5,5, by=0.01), df=49), xlab=expression(paste(mu,\" (measured in mph)\", sep=\"\")), ylab=\"Density\", type=\"l\")\nabline(v=17, col=\"red\")\npolygon(x = c(17+seq(0,5, by=0.01)*0.7478, 21), y = c(0, dt(seq(0,5, by=0.01), df=49)), col = \"red\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\nThe sample mean is estimated to be 15.4.  How likely is that?\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(x=17+seq(-5,5, by=0.01)*0.7478, dt(seq(-5,5, by=0.01), df=49), xlab=expression(paste(\"x-bar -\",mu,\" (measured in mph)\", sep=\"\")), ylab=\"Density\", type=\"l\")\nabline(v=17, col=\"red\")\nabline(v=15.4, col=\"blue\")\npolygon(x = c(17+seq(0,5, by=0.01)*0.7478, 21), y = c(0, dt(seq(0,5, by=0.01), df=49)), col = \"red\")\npolygon(x = c(12, 17+seq(-5,-2.14, by=0.01)*0.7478), y = c(dt(seq(-5,-2.14, by=0.01), df=49), 0), col = \"blue\")\nabline(h=0, col=\"black\")\nabline(v=17 + qt(0.1, df=49)*0.7874, col=\"black\", lty=3)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\nThe probability of seeing such a small sample mean if the true average is 17 is only 0.01869.  The probability above the dotted black line is 0.9 with 0.1 below.  WIth 90% confidence, anything below this would be sufficient evidence to reject the claim that the true average is 17 or above.\n\n\n## The Confidence Interval\n\nLet's take the sample mean as the center and work out a confidence interval at 90%.  It's exactly the 16.37143 gives above.\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(x=15.4+seq(-5,5, by=0.01)*0.7478, dt(seq(-5,5, by=0.01), df=49), xlab=expression(paste(mu,\" | x-bar (measured in mph)\", sep=\"\")), ylab=\"Density\", type=\"l\")\nabline(v=15.4, col=\"blue\")\nabline(v=15.4 - qt(0.1, df=49)*0.7478, col=\"black\", lty=3)\npolygon(x = c(11, 15.4+seq(-5,1.3, by=0.01)*0.7478), y = c(dt(seq(-5,1.3, by=0.01), df=49), 0), col = \"blue\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\nAs an aside, 17 has exactly 0.01869 probability above it shown in orange.\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(x=15.4+seq(-5,5, by=0.01)*0.7478, dt(seq(-5,5, by=0.01), df=49), xlab=expression(paste(mu,\" | x-bar (measured in mph)\", sep=\"\")), ylab=\"Density\", type=\"l\")\nabline(v=15.4, col=\"blue\")\nabline(v=15.4 - qt(0.1, df=49)*0.7478, col=\"black\", lty=3)\npolygon(x = c(11, 15.4+seq(-5,1.3, by=0.01)*0.7478), y = c(dt(seq(-5,1.3, by=0.01), df=49), 0), col = \"blue\")\npolygon(x = c(15.4+seq(2.14,5, by=0.01)*0.7478, 17), y = c(dt(seq(2.14,5, by=0.01), df=49), 0), col = \"orange\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}