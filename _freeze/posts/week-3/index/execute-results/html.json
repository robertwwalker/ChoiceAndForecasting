{
  "hash": "0f1157b46010f0c8d2e3b9c2f3d2f2fc",
  "result": {
    "markdown": "---\ntitle: \"Week 3: Binomial Logistic Regression\"\nauthor: \"Robert W. Walker\"\ndate: \"2022-09-12\"\ncategories: [R]\nimage: \"image.png\"\ntoc: true\nexecute: \n  echo: fenced\n---\n\n\nThe slides [are here.](https://robertwwalker.github.io/xaringan/CMF-Week-3/)\n\nOur third class meeting will focus on [Chapter 5](https://peopleanalytics-regression-book.org/bin-log-reg.html)  of __Handbook of Regression Modeling in People Analytics__.\n\n# The Skinny\n\nWhy not just use a linear model for a binary outcome?  It turns out that you can but many people know just enough to think you don't know what you are talking about if you do.  In a very, very dense paper, Jamie Robins (1986, iirc) proves that, as long as the predicted probabilities tend to remain in the region of about 0.2 to 0.8, that is, **as long as the model is not all that good**, then it really doesn't matter except that the standard errors you would estimate are likely incorrect.  If the model is good for one or the other of the levels, then the behavior in the extremes matters; you could end up with predictions that are less than zero or greater than one and those are invalid as probabilities.  Let's just peak at this example.  I will load some data on Churn.  For details, [look on Kaggle.com -- a great source of data](https://www.kaggle.com/datasets/blastchar/telco-customer-churn).\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\nChurn <- read.csv(\"https://github.com/robertwwalker/DADMStuff/raw/master/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\nnames(Churn)\ntable(Churn$Churn)\n```\n````\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"customerID\"       \"gender\"           \"SeniorCitizen\"    \"Partner\"         \n [5] \"Dependents\"       \"tenure\"           \"PhoneService\"     \"MultipleLines\"   \n [9] \"InternetService\"  \"OnlineSecurity\"   \"OnlineBackup\"     \"DeviceProtection\"\n[13] \"TechSupport\"      \"StreamingTV\"      \"StreamingMovies\"  \"Contract\"        \n[17] \"PaperlessBilling\" \"PaymentMethod\"    \"MonthlyCharges\"   \"TotalCharges\"    \n[21] \"Churn\"           \n\n  No  Yes \n5174 1869 \n```\n:::\n:::\n\n\n\nNow to a regression model.  We will need a variable type change to pull this off.  Let's have a look at the necessary transformation.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\nstr(Churn$Churn)\nstr(as.factor(Churn$Churn))\nstr(as.numeric(as.factor(Churn$Churn)))\nChurn$Churn.Numeric <- as.numeric(as.factor(Churn$Churn))-1\nstr(Churn$Churn.Numeric)\n```\n````\n\n::: {.cell-output .cell-output-stdout}\n```\n chr [1:7043] \"No\" \"No\" \"Yes\" \"No\" \"Yes\" \"Yes\" \"No\" \"No\" \"Yes\" \"No\" \"No\" ...\n Factor w/ 2 levels \"No\",\"Yes\": 1 1 2 1 2 2 1 1 2 1 ...\n num [1:7043] 1 1 2 1 2 2 1 1 2 1 ...\n num [1:7043] 0 0 1 0 1 1 0 0 1 0 ...\n```\n:::\n:::\n\n\n`Churn` is a character variable.  To turn it to a quantity taking values of zero and one, `as.factor()` turns the character variable `Churn` into a factor with two levels, No and Yes.  `as.numeric()` turns this into a number, one or two.  I then subtract one to get a variable that takes values zero or one and store it as `Churn.Numeric`.  I will make Churn a function of a few chosen variables; there is more in the dataset that I could work with.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r, message=FALSE, warning=FALSE}}\nlibrary(stargazer); library(magrittr); library(tidyverse); library(skimr)\nmy.lm <- lm(Churn.Numeric~InternetService+tenure+PhoneService+Contract+TotalCharges, data=Churn)\nsummary(my.lm)\n```\n````\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Churn.Numeric ~ InternetService + tenure + PhoneService + \n    Contract + TotalCharges, data = Churn)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.64458 -0.26256 -0.07269  0.35565  1.12180 \n\nCoefficients:\n                             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                 4.274e-01  1.651e-02  25.892  < 2e-16 ***\nInternetServiceFiber optic  2.601e-01  1.268e-02  20.512  < 2e-16 ***\nInternetServiceNo          -1.476e-01  1.588e-02  -9.290  < 2e-16 ***\ntenure                     -1.909e-03  4.642e-04  -4.112 3.97e-05 ***\nPhoneServiceYes            -3.823e-02  1.769e-02  -2.161   0.0307 *  \nContractOne year           -1.385e-01  1.383e-02 -10.018  < 2e-16 ***\nContractTwo year           -1.184e-01  1.641e-02  -7.218 5.81e-13 ***\nTotalCharges               -3.961e-05  5.244e-06  -7.555 4.73e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3812 on 7024 degrees of freedom\n  (11 observations deleted due to missingness)\nMultiple R-squared:  0.2562,\tAdjusted R-squared:  0.2554 \nF-statistic: 345.6 on 7 and 7024 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nScientific notation can render tables hard to read.  We can adjust R's internal options to require more leading zeroes before scientific notation is used with `scipen` in options, e.g.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\noptions(scipen=6)\nsummary(my.lm)\n```\n````\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Churn.Numeric ~ InternetService + tenure + PhoneService + \n    Contract + TotalCharges, data = Churn)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.64458 -0.26256 -0.07269  0.35565  1.12180 \n\nCoefficients:\n                               Estimate   Std. Error t value Pr(>|t|)    \n(Intercept)                 0.427421784  0.016507683  25.892  < 2e-16 ***\nInternetServiceFiber optic  0.260067246  0.012678756  20.512  < 2e-16 ***\nInternetServiceNo          -0.147560579  0.015884435  -9.290  < 2e-16 ***\ntenure                     -0.001908716  0.000464227  -4.112 3.97e-05 ***\nPhoneServiceYes            -0.038230897  0.017690283  -2.161   0.0307 *  \nContractOne year           -0.138502281  0.013825977 -10.018  < 2e-16 ***\nContractTwo year           -0.118438811  0.016407760  -7.218 5.81e-13 ***\nTotalCharges               -0.000039614  0.000005244  -7.555 4.73e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3812 on 7024 degrees of freedom\n  (11 observations deleted due to missingness)\nMultiple R-squared:  0.2562,\tAdjusted R-squared:  0.2554 \nF-statistic: 345.6 on 7 and 7024 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nThat is much easier to read.  The first thing to note is that there are lots of stars.  The model seems to `explain variance` in the outcome with all the caveats that go with that idea for a 0/1 variable.  For example, about 25% of the variance in `Churn` can be accounted for by these predictors.  The model F-statistic assessing the joint hypothesis that all predictors have zero slopes yields an absolutely enormous statistic; the observed F-value is 345.6; 99% of F values with 7 numerator and 7024 denominator degrees of freedom are less than 2.64; a statistic this large is quite unlikely by chance.  Moreover, each of the individual t-statistics are greater than 2 in absolute value.  Those are the encouraging parts.  The residual standard error should give us pause; on average, we are 0.38 away from the observed outcome in the probability metric.  That is not great though to make it smaller, the model would have to predict in the extremes.  Now, let me put this into a nice table.\n\n\n\n````{.cell-code}\n```{{r, results='asis', warning=FALSE, message=FALSE}}\nstargazer(my.lm, type=\"html\", style=\"apsr\")\n```\n````\n\n\n<table style=\"text-align:center\"><tr><td colspan=\"2\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\"></td><td>Churn.Numeric</td></tr>\n<tr><td colspan=\"2\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\">InternetServiceFiber optic</td><td>0.260<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.013)</td></tr>\n<tr><td style=\"text-align:left\">InternetServiceNo</td><td>-0.148<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.016)</td></tr>\n<tr><td style=\"text-align:left\">tenure</td><td>-0.002<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.0005)</td></tr>\n<tr><td style=\"text-align:left\">PhoneServiceYes</td><td>-0.038<sup>**</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.018)</td></tr>\n<tr><td style=\"text-align:left\">ContractOne year</td><td>-0.139<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.014)</td></tr>\n<tr><td style=\"text-align:left\">ContractTwo year</td><td>-0.118<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.016)</td></tr>\n<tr><td style=\"text-align:left\">TotalCharges</td><td>-0.00004<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.00001)</td></tr>\n<tr><td style=\"text-align:left\">Constant</td><td>0.427<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.017)</td></tr>\n<tr><td style=\"text-align:left\">N</td><td>7,032</td></tr>\n<tr><td style=\"text-align:left\">R<sup>2</sup></td><td>0.256</td></tr>\n<tr><td style=\"text-align:left\">Adjusted R<sup>2</sup></td><td>0.255</td></tr>\n<tr><td style=\"text-align:left\">Residual Std. Error</td><td>0.381 (df = 7024)</td></tr>\n<tr><td style=\"text-align:left\">F Statistic</td><td>345.603<sup>***</sup> (df = 7; 7024)</td></tr>\n<tr><td colspan=\"2\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td colspan=\"2\" style=\"text-align:left\"><sup>*</sup>p < .1; <sup>**</sup>p < .05; <sup>***</sup>p < .01</td></tr>\n</table>\n\n\nLet's return to my original criticism of using this particular model and assuming that the outcome is a quantity when it only takes two values.  Are all of the predictions well-behaved?\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\nmy.lm$fitted.values %>% skim()\n```\n````\n\n::: {.cell-output-display}\nTable: Data summary\n\n|                         |           |\n|:------------------------|:----------|\n|Name                     |Piped data |\n|Number of rows           |7032       |\n|Number of columns        |1          |\n|_______________________  |           |\n|Column type frequency:   |           |\n|numeric                  |1          |\n|________________________ |           |\n|Group variables          |None       |\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate| mean|   sd|    p0|  p25|  p50|  p75| p100|hist  |\n|:-------------|---------:|-------------:|----:|----:|-----:|----:|----:|----:|----:|:-----|\n|data          |         0|             1| 0.27| 0.22| -0.14| 0.08| 0.24| 0.42| 0.64|▆▇▇▇▇ |\n:::\n:::\n\n\nNo, there are negative values.  To prevent that, we need a different tool; this is the subject of Chapter 5.\n\nWe should also examine residuals.  Using a variety of tests of linear model assumptions, we find the model lacking in every one but constant variance [homo/heteroscedasticity].\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\nlibrary(gvlma)\ngvlma(my.lm)\n```\n````\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Churn.Numeric ~ InternetService + tenure + PhoneService + \n    Contract + TotalCharges, data = Churn)\n\nCoefficients:\n               (Intercept)  InternetServiceFiber optic  \n                0.42742178                  0.26006725  \n         InternetServiceNo                      tenure  \n               -0.14756058                 -0.00190872  \n           PhoneServiceYes            ContractOne year  \n               -0.03823090                 -0.13850228  \n          ContractTwo year                TotalCharges  \n               -0.11843881                 -0.00003961  \n\n\nASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS\nUSING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:\nLevel of Significance =  0.05 \n\nCall:\n gvlma(x = my.lm) \n\n                     Value   p-value                   Decision\nGlobal Stat        630.599 0.000e+00 Assumptions NOT satisfied!\nSkewness           377.348 0.000e+00 Assumptions NOT satisfied!\nKurtosis            64.207 1.110e-15 Assumptions NOT satisfied!\nLink Function      187.439 0.000e+00 Assumptions NOT satisfied!\nHeteroscedasticity   1.605 2.051e-01    Assumptions acceptable.\n```\n:::\n:::\n\n\nA linear model seems not to work well for these data.  Models designed for this task will occupy our attention after a few notes and an overview.\n\n## Overview and Comments\n\nWhat we require is a regression type tool tuned to represent data drawn from a generic binomial distribution.  There are actually a few such models that I will introduce you to.  There are also some really interesting models that you can fit that build mixtures of the different approaches but we won't go that far.  I should also note that there is a whole class of models on binary classification using trees.  If you remember regression trees, you can also build regression trees for binary problems.  I will do a bit of this in the end.  Before that, some initial observations: \n\n- I am using `stargazer` to produce the tables; they are nice and easy to produce.  They have raw html output so I can embed that directly using `asis` in the code chunks and typesetting to html.  \n- This whole document makes use of `fenced` code chunks.  You can copy and paste this into a new markdown or quarto to play along with the ticks built in.  \n- If one wants to omit a chunk at the top, you would do it with the bracketed part adding option `include=FALSE`.  I always suppress warnings and messages to read (surrounded by curly brackets) *r setup, include=FALSE*.  If you use this option and load libraries, readers will find it hard to figure out how commands may have changed meaning by masking.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r setup}}\nknitr::opts_chunk$set(warning=FALSE, message=FALSE, tidy = TRUE)\nset.seed(9119)\n```\n````\n:::\n\n\n# My Notes on Binary GLMs\n\nSuppose we have some variable that we want to explain, say `Churn` that has two mutually exclusive and exhaustive alternatives.  **Customers can either Churn or not**.  Any given customer is conceptualized as a Bernoulli trial, e.g. $\\pi^{y}(1-\\pi)^{1-y}$.  With a willingness to believe that every `Churn` decision is an independently and identically distributed trial in this group of customers, overall churn is a binomial random variable with probability mass function $$P_{y} = {n \\choose y} \\pi^{y}(1-\\pi)^{n-y}$$ where\n\n- $P_{y}$ is the binomial probability of $y$\n- $y$ is the number of successes in $n$ trials\n- $n$ is the number of trials\n- $\\pi$ is the probability of success in any given trial.\n\nThat's just a fancy way of saying that we have a binomial distribution on our hands.  This is known as the **canonical** distribution for binary data because $\\pi$ is a sufficient statistic -- a complete characterization of Churn because it only takes two values.  The challenge is that we wish to formulate a regression model for $\\pi$ which will first require that we to grips with the existence of $\\pi_{i}$.  \n\n# Generalized Linear Models\n\nI need some model that is bounded to zero and one, abstractly, because probabilities are subject to a sum to one constraint.  This is where there is some diversity in the representations; let me explain.  In generalized linear models, there are two keys to the specification: the family and the link.  We have already covered the family; it has to be binomial.\n\nIn the theory of these models, presented by Peter McCullagh and John Nelder in 1989[^1], that link for the probabilities is what ties regression to the binomial distribution; we posit that $(1-\\pi_{i}) = Pr(y_{i}=1|X_{i}) = 1-F(X_{i}\\beta)$ so that $\\pi_{i} = Pr(y_{i}=0|X_{i})= F(X_{i}\\beta)$.  If $F$ is some well-behaved probability distribution, then the aforementioned is valid.  There are a few ways of actually writing that; the $\\pi_{i}$ could be derived from a normal distribution -- called probit --, the distribution that the text focuses on is, the logistic distribution -- the model is named logit--, and there are a few others that are somewhat common: the Cauchy, the log-log, and the complimentary log-log.  The latter two are asymmetric and mirrors one of the other.  What we want to do is to find the estimates of $\\beta$ that maximize the likelihood of the sample we observe.[^2]\n\n## A Probit Model\n\nFirst, a little substitution and some notation.  Let me label the normal probability up to $X_{i}\\beta$ to be $\\Phi(X\\beta)$ and the probability above $X\\beta$ to be $1-\\Phi(X+{i}\\beta)$.  I could substitute this into the binomial and obtain the product for the entire sample -- this is known as the likelihood.\n\n\n$$\\prod^{n}_{i=1} \\Phi(X_{i}\\beta)^{1-y_{i}}(1-\\Phi(X_{i}\\beta))^{y_{i}}$$\n\n\nTaking logs yields:\n\n\n$$\\ln \\mathcal{L} =  \\sum^{n}_{i=1} (1-y_{i})\\ln \\Phi(X_{i}\\beta) + y_{i} \\ln (1-\\Phi(X_{i}\\beta))$$\n\nSo the solution becomes \n\n\n$$\\arg \\max_{\\beta} \\ln \\mathcal{L} =  \\arg \\max_{\\beta} \\sum^{n}_{i=1} (1-y_{i})\\ln \\Phi(X_{i}\\beta) + y_{i} \\ln (1-\\Phi(X_{i}\\beta))$$\n\n\nIn English, we want to find the values of $\\beta$ that maximize the log-likelihod of the entire sample.\n\n### Estimation of a First GLM\n\nNow to another example with the same measure of `Churn`.  The outcome of interest is `Churn`.  The model specification will call `glm`, let me examine `Churn` as a function of `InternetService`, `tenure`, `PhoneService`, `Contract` and `TotalCharges`.  There is one trick to deploying it, the outcome variable must be a `factor` type.  To make the table nice, let me mutate the type to a factor and then we can model it.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r, warning=FALSE, message=FALSE}}\nChurn %<>%\n    mutate(ChurnF = as.factor(Churn))\nChurn %>%\n    select(Churn, ChurnF) %>%\n    mutate(as.numeric(ChurnF)) %>%\n    head()\n```\n````\n\n::: {.cell-output .cell-output-stdout}\n```\n  Churn ChurnF as.numeric(ChurnF)\n1    No     No                  1\n2    No     No                  1\n3   Yes    Yes                  2\n4    No     No                  1\n5   Yes    Yes                  2\n6   Yes    Yes                  2\n```\n:::\n:::\n\n\nNow I want to estimate the model and have a look at the result.  I will put this in a stargazer table.\n\n\n\n````{.cell-code}\n```{{r, results='asis', warning=FALSE, message=FALSE}}\nmy.probit <- glm(ChurnF ~ InternetService + tenure + PhoneService + Contract + TotalCharges,\n    family = binomial(link = \"probit\"), data = Churn)\nstargazer(my.probit, type = \"html\", style = \"apsr\")\n```\n````\n\n\n<table style=\"text-align:center\"><tr><td colspan=\"2\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\"></td><td>ChurnF</td></tr>\n<tr><td colspan=\"2\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\">InternetServiceFiber optic</td><td>0.726<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.053)</td></tr>\n<tr><td style=\"text-align:left\">InternetServiceNo</td><td>-0.458<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.070)</td></tr>\n<tr><td style=\"text-align:left\">tenure</td><td>-0.028<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.003)</td></tr>\n<tr><td style=\"text-align:left\">PhoneServiceYes</td><td>-0.372<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.073)</td></tr>\n<tr><td style=\"text-align:left\">ContractOne year</td><td>-0.489<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.057)</td></tr>\n<tr><td style=\"text-align:left\">ContractTwo year</td><td>-0.866<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.083)</td></tr>\n<tr><td style=\"text-align:left\">TotalCharges</td><td>0.0001<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.00003)</td></tr>\n<tr><td style=\"text-align:left\">Constant</td><td>0.131<sup>*</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.069)</td></tr>\n<tr><td style=\"text-align:left\">N</td><td>7,032</td></tr>\n<tr><td style=\"text-align:left\">Log Likelihood</td><td>-3,019.439</td></tr>\n<tr><td style=\"text-align:left\">AIC</td><td>6,054.878</td></tr>\n<tr><td colspan=\"2\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td colspan=\"2\" style=\"text-align:left\"><sup>*</sup>p < .1; <sup>**</sup>p < .05; <sup>***</sup>p < .01</td></tr>\n</table>\n\n\nWe can do `astrology` on the tables; read the stars. Fiber optic customers are more likely to Churn and those without internet service are less likely to Churn but both conditions are compared to a third category absorbed into the Constant.  What is that category?\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\njanitor::tabyl(Churn$InternetService)\n```\n````\n\n::: {.cell-output .cell-output-stdout}\n```\n Churn$InternetService    n   percent\n                   DSL 2421 0.3437456\n           Fiber optic 3096 0.4395854\n                    No 1526 0.2166690\n```\n:::\n:::\n\n\n`DSL` subscribers.  It is the first in alphabetical order.  *That is the default option.*  That also means that the constant captures those on Month-to-month contracts and without phone service -- the omitted category for each.  So what do these `coefficients` show?  \n\nThe slopes represent the effect of a one-unit change in $x$ on the underlying distribution for the probabilities.  Unless one has intuition for those distributions, they come across as nonsensical.  In the table above, let me take the example of tenure.  For each unit of tenure [another month having been a customer], the normal variable $Z \\sim N(0,1)$ decreases by 0.028.  But what that means depends on whether we are going from 0 to -0.028 or from -2 to -2.028.  Remember the standard normal has about 95% of probability between -2 and 2 and has a modal/most common value at zero.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\ndata.frame(Z = rnorm(10000)) %>%\n    ggplot(.) + aes(x = Z) + geom_density() + theme_minimal() + geom_vline(aes(xintercept = -2.028),\n    color = \"red\") + geom_vline(aes(xintercept = -2), color = \"red\") + geom_vline(aes(xintercept = -0.028),\n    color = \"blue\") + geom_vline(aes(xintercept = 0), color = \"blue\")\n```\n````\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nThe associated probabilities in that example would be either small or nearly trivial even over 1000s of customers.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\npnorm(-2) - pnorm(-2.028)\npnorm(0) - pnorm(-0.028)\n```\n````\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.001470008\n[1] 0.01116892\n```\n:::\n:::\n\n\nIt seems like most scholars I run across don't actually know this; they tend to stick to stars (That's why your book plays with odds and logistic regression but I want to start here because y'all have never seen the logistic distribution, except on my poster....)  Before that, here's another way of showing the actual estimated slopes even if their intuition is hard.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\nlibrary(jtools)\nplot_summs(my.probit, inner_ci_level = 0.95)\n```\n````\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nI can make that plot better.  \n\n### A Scaled Coefficient Plot\n\nRemember scaling from last time and linear models?  No rules against that.  The two metric variables -- `tenure` and `TotalCharges` -- are now the change in Z for a **one standard deviation** change in the relevant variable.  That's 2267 dollars for `TotalCharges` and 24.6 months for `tenure`.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\nChurn %>%\n    skim(TotalCharges, tenure)\n```\n````\n\n::: {.cell-output-display}\nTable: Data summary\n\n|                         |           |\n|:------------------------|:----------|\n|Name                     |Piped data |\n|Number of rows           |7043       |\n|Number of columns        |23         |\n|_______________________  |           |\n|Column type frequency:   |           |\n|numeric                  |2          |\n|________________________ |           |\n|Group variables          |None       |\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate|    mean|      sd|   p0|    p25|     p50|     p75|   p100|hist  |\n|:-------------|---------:|-------------:|-------:|-------:|----:|------:|-------:|-------:|------:|:-----|\n|TotalCharges  |        11|             1| 2283.30| 2266.77| 18.8| 401.45| 1397.47| 3794.74| 8684.8|▇▂▂▂▁ |\n|tenure        |         0|             1|   32.37|   24.56|  0.0|   9.00|   29.00|   55.00|   72.0|▇▃▃▃▆ |\n:::\n:::\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\nChurn %>%\n    mutate(tenure = scale(tenure), TotalCharges = scale(TotalCharges)) %>%\n    glm(ChurnF ~ InternetService + tenure + PhoneService + Contract + TotalCharges,\n        family = binomial(link = \"probit\"), data = .) %>%\n    plot_summs(., inner_ci_level = 0.95)\n```\n````\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\nWhat the plot makes clear is that basically all of the predictors are deemed important in Churn decisions by conventional standards as all have a very low probability of no relationship/zero slope.  But that's as far as we can get with these unless our audience shares this intuition for probability distributions.\n\n### The Trouble with Non-linear Models\n\nI should be clear that the model does have lines; they are just lines inside of a nonlinear function -- the F.  The `generalized linear` part means that the interpretation of any one factor will depend on the values of the others.  We will have to usually want to generate hypothetical data to understand what is really going on.  After a presentation of the remaining models, I will return to my preferred method of interpretation.\n\n## Logistic Regression\n\nThe logistic distribution is the focus of the textbook chapter.  To respecify the model using that, the only change in syntax is the `link`, we need it to be `link=\"logit\"` which is the default.\n\nThe logistic function is given by:\n\n\n$$\\Lambda = \\frac{e^{X\\beta}}{1+e^{X\\beta}}$$\n\n\nBut the rest is the same; it takes the very general representation and provides a specific probability function for $F$:\n\n\n$$\\arg \\max_{\\beta} \\ln \\mathcal{L} =  \\arg \\max_{\\beta} \\sum^{n}_{i=1} (1-y_{i})\\ln \\Lambda(X_{i}\\beta) + y_{i} \\ln (1-\\Lambda(X_{i}\\beta))$$\n\n\nOne of the advantages of using the logistic distribution is that you can analytically solve it with only categorical variables.  The other is the interpretation of the estimates; the slope is an increment in the log-odds, e.g. $\\ln (\\frac{\\pi_{y=1}}{1-\\pi_{y=1}})$.\n\n\n\n````{.cell-code}\n```{{r, results='asis', warning=FALSE, message=FALSE}}\nmy.logit <- glm(ChurnF ~ InternetService + tenure + PhoneService + Contract + TotalCharges,\n    family = binomial(link = \"logit\"), data = Churn)\nstargazer(my.logit, my.probit, type = \"html\", style = \"apsr\")\n```\n````\n\n\n<table style=\"text-align:center\"><tr><td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\"></td><td colspan=\"2\">ChurnF</td></tr>\n<tr><td style=\"text-align:left\"></td><td>logistic</td><td>probit</td></tr>\n<tr><td style=\"text-align:left\"></td><td>(1)</td><td>(2)</td></tr>\n<tr><td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\">InternetServiceFiber optic</td><td>1.172<sup>***</sup></td><td>0.726<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.091)</td><td>(0.053)</td></tr>\n<tr><td style=\"text-align:left\">InternetServiceNo</td><td>-0.765<sup>***</sup></td><td>-0.458<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.127)</td><td>(0.070)</td></tr>\n<tr><td style=\"text-align:left\">tenure</td><td>-0.063<sup>***</sup></td><td>-0.028<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.006)</td><td>(0.003)</td></tr>\n<tr><td style=\"text-align:left\">PhoneServiceYes</td><td>-0.714<sup>***</sup></td><td>-0.372<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.126)</td><td>(0.073)</td></tr>\n<tr><td style=\"text-align:left\">ContractOne year</td><td>-0.874<sup>***</sup></td><td>-0.489<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.103)</td><td>(0.057)</td></tr>\n<tr><td style=\"text-align:left\">ContractTwo year</td><td>-1.781<sup>***</sup></td><td>-0.866<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.172)</td><td>(0.083)</td></tr>\n<tr><td style=\"text-align:left\">TotalCharges</td><td>0.0004<sup>***</sup></td><td>0.0001<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.0001)</td><td>(0.00003)</td></tr>\n<tr><td style=\"text-align:left\">Constant</td><td>0.373<sup>***</sup></td><td>0.131<sup>*</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.117)</td><td>(0.069)</td></tr>\n<tr><td style=\"text-align:left\">N</td><td>7,032</td><td>7,032</td></tr>\n<tr><td style=\"text-align:left\">Log Likelihood</td><td>-3,004.328</td><td>-3,019.439</td></tr>\n<tr><td style=\"text-align:left\">AIC</td><td>6,024.656</td><td>6,054.878</td></tr>\n<tr><td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td colspan=\"3\" style=\"text-align:left\"><sup>*</sup>p < .1; <sup>**</sup>p < .05; <sup>***</sup>p < .01</td></tr>\n</table>\n\n\nIf we see these in a side by side comparison, it is obvious that the `logistic` version is bigger in absolute value across the board.  So what do these mean in terms of actual odds of Churn or not?\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\nexp(my.logit$coefficients)\n```\n````\n\n::: {.cell-output .cell-output-stdout}\n```\n               (Intercept) InternetServiceFiber optic \n                 1.4521830                  3.2282235 \n         InternetServiceNo                     tenure \n                 0.4652907                  0.9393570 \n           PhoneServiceYes           ContractOne year \n                 0.4898382                  0.4173338 \n          ContractTwo year               TotalCharges \n                 0.1685345                  1.0003559 \n```\n:::\n:::\n\nAll else equal,\n\n- The odds of Churning with Fiber optics, as opposed to DSL, increase by 223%.\n- The odds of Churning with No internet, as opposed to DSL, decrease by 53.5% .\n- The odds of Churning with No phone service, as opposed to Phone service, are 51% lower.\n- The odds of Churning decrease by 4% per unit tenure [month].\n- The odds of Churning increase by 0.04% per dollar of total charges.\n- The odds of Churning decrease under contracts.  Compared to none, about 83% lower odds under a two-year contract and 58% lower odds under a one-year contract.\n\nIf you choose to work with odds, then the suggestion to exponentiate the confidence intervals for the odds-ratios is sound.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\nexp(confint(my.logit))\n```\n````\n\n::: {.cell-output .cell-output-stdout}\n```\n                               2.5 %    97.5 %\n(Intercept)                1.1535576 1.8288933\nInternetServiceFiber optic 2.7035580 3.8611249\nInternetServiceNo          0.3619193 0.5951803\ntenure                     0.9284409 0.9500587\nPhoneServiceYes            0.3822153 0.6276984\nContractOne year           0.3400210 0.5101487\nContractTwo year           0.1190988 0.2338910\nTotalCharges               1.0002350 1.0004795\n```\n:::\n:::\n\n\nThere are diagnostics that can be applied to these models.  The various pseudo-$r^2$ measures.  This model fit is neither terrible nor good.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\nlibrary(DescTools)\nDescTools::PseudoR2(my.logit, which = c(\"McFadden\", \"CoxSnell\", \"Nagelkerke\", \"Tjur\"))\n```\n````\n\n::: {.cell-output .cell-output-stdout}\n```\n  McFadden   CoxSnell Nagelkerke       Tjur \n 0.2621401  0.2618213  0.3817196  0.2798359 \n```\n:::\n:::\n\n\nTaking advantage of the book's example, I first need to clean up the data, there are a few missing values.  Then let me estimate the regression and diagnose it.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\nChurn.CC <- Churn %>%\n    select(ChurnF, InternetService, tenure, PhoneService, Contract, TotalCharges) %>%\n    filter(!is.na(TotalCharges))\nmy.logit.CC <- glm(ChurnF ~ InternetService + tenure + PhoneService + Contract +\n    TotalCharges, family = binomial(link = \"logit\"), data = Churn.CC)\nlibrary(LogisticDx)\n# get range of goodness-of-fit diagnostics\nmodel_diagnostics <- LogisticDx::gof(my.logit.CC, plotROC = TRUE)\n```\n````\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\nOne very common plot for binary logistic regression is the ROC: the Receiver Operating Curve.  It plots specificity against sensitivity.  Specificity is the ability, in this case, to correctly identify non-Churners[few false positives is highly specific]; sensitivity is the ability of the test to correctly identify Churners [few false negatives is highly sensitive].  A useful mnemonic is that the presence of the letter `f` in specificity is a reminder that the False test results are False for the condition, while the `t` in sensitivity is True test results are True for the condition.  Now, turning to the actual provided diagnostics, what all is in there?  `?gof` for example.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n# returns a list\nnames(model_diagnostics)\nmodel_diagnostics$gof\n```\n````\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"ct\"    \"chiSq\" \"ctHL\"  \"gof\"   \"R2\"    \"auc\"  \n         test  stat       val df         pVal\n1:         HL chiSq 31.832686  8 9.979186e-05\n2:        mHL     F 16.436041  9 4.896337e-27\n3:       OsRo     Z  3.282456 NA 1.029070e-03\n4: SstPgeq0.5     Z  6.656484 NA 2.804560e-11\n5:   SstPl0.5     Z  5.983933 NA 2.178133e-09\n6:    SstBoth chiSq 80.116227  2 4.008504e-18\n7: SllPgeq0.5 chiSq 45.280618  1 1.707304e-11\n8:   SllPl0.5 chiSq 29.794958  1 4.802393e-08\n9:    SllBoth chiSq 54.741365  2 1.297369e-12\n```\n:::\n:::\n\n\nThis is not a very good model.  It fails all the tests.  We need to add more predictors; I have those but let's keep it simple for now.  Let me look at two more.\n\n## Other Binomial GLMs\n\nThe others\n\n\n\n````{.cell-code}\n```{{r, results='asis', warning=FALSE, message=FALSE}}\nmy.cauchit <- glm(ChurnF ~ InternetService + tenure + PhoneService + Contract + TotalCharges,\n    family = binomial(link = \"cauchit\"), data = Churn)\nmy.cloglogit <- glm(ChurnF ~ InternetService + tenure + PhoneService + Contract +\n    TotalCharges, family = binomial(link = \"cloglog\"), data = Churn)\nstargazer(my.cauchit, my.cloglogit, my.logit, my.probit, type = \"html\", style = \"apsr\")\n```\n````\n\n\n<table style=\"text-align:center\"><tr><td colspan=\"5\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\"></td><td colspan=\"4\">ChurnF</td></tr>\n<tr><td style=\"text-align:left\"></td><td>glm: binomial</td><td>glm: binomial</td><td>logistic</td><td>probit</td></tr>\n<tr><td style=\"text-align:left\"></td><td>link = cauchit</td><td>link = cloglog</td><td></td><td></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(1)</td><td>(2)</td><td>(3)</td><td>(4)</td></tr>\n<tr><td colspan=\"5\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\">InternetServiceFiber optic</td><td>0.993<sup>***</sup></td><td>0.877<sup>***</sup></td><td>1.172<sup>***</sup></td><td>0.726<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.094)</td><td>(0.071)</td><td>(0.091)</td><td>(0.053)</td></tr>\n<tr><td style=\"text-align:left\">InternetServiceNo</td><td>-0.790<sup>***</sup></td><td>-0.640<sup>***</sup></td><td>-0.765<sup>***</sup></td><td>-0.458<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.174)</td><td>(0.112)</td><td>(0.127)</td><td>(0.070)</td></tr>\n<tr><td style=\"text-align:left\">tenure</td><td>-0.123<sup>***</sup></td><td>-0.059<sup>***</sup></td><td>-0.063<sup>***</sup></td><td>-0.028<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.011)</td><td>(0.005)</td><td>(0.006)</td><td>(0.003)</td></tr>\n<tr><td style=\"text-align:left\">PhoneServiceYes</td><td>-0.705<sup>***</sup></td><td>-0.586<sup>***</sup></td><td>-0.714<sup>***</sup></td><td>-0.372<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.136)</td><td>(0.099)</td><td>(0.126)</td><td>(0.073)</td></tr>\n<tr><td style=\"text-align:left\">ContractOne year</td><td>-1.190<sup>***</sup></td><td>-0.767<sup>***</sup></td><td>-0.874<sup>***</sup></td><td>-0.489<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.166)</td><td>(0.092)</td><td>(0.103)</td><td>(0.057)</td></tr>\n<tr><td style=\"text-align:left\">ContractTwo year</td><td>-7.065<sup>***</sup></td><td>-1.696<sup>***</sup></td><td>-1.781<sup>***</sup></td><td>-0.866<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(1.254)</td><td>(0.161)</td><td>(0.172)</td><td>(0.083)</td></tr>\n<tr><td style=\"text-align:left\">TotalCharges</td><td>0.001<sup>***</sup></td><td>0.0004<sup>***</sup></td><td>0.0004<sup>***</sup></td><td>0.0001<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.0001)</td><td>(0.0001)</td><td>(0.0001)</td><td>(0.00003)</td></tr>\n<tr><td style=\"text-align:left\">Constant</td><td>0.590<sup>***</sup></td><td>-0.024</td><td>0.373<sup>***</sup></td><td>0.131<sup>*</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.122)</td><td>(0.088)</td><td>(0.117)</td><td>(0.069)</td></tr>\n<tr><td style=\"text-align:left\">N</td><td>7,032</td><td>7,032</td><td>7,032</td><td>7,032</td></tr>\n<tr><td style=\"text-align:left\">Log Likelihood</td><td>-2,997.445</td><td>-2,991.982</td><td>-3,004.328</td><td>-3,019.439</td></tr>\n<tr><td style=\"text-align:left\">AIC</td><td>6,010.891</td><td>5,999.964</td><td>6,024.656</td><td>6,054.878</td></tr>\n<tr><td colspan=\"5\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td colspan=\"5\" style=\"text-align:left\"><sup>*</sup>p < .1; <sup>**</sup>p < .05; <sup>***</sup>p < .01</td></tr>\n</table>\n\n\nThe best fit seems to be provided by the `cloglog` distribution which is asymmetric.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\nlibrary(pROC)\npredicted <- predict(my.cloglogit, type = \"response\")\nauc(Churn.CC$ChurnF, predicted, plot = TRUE)\n```\n````\n\n::: {.cell-output .cell-output-stdout}\n```\nArea under the curve: 0.8386\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\n## Residuals\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\nd <- density(residuals(my.logit, \"pearson\"))\nplot(d, main = \"\")\n```\n````\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\nThis is rather poor.\n\n## Predicted Probability\n\nI find that the most straightforward way to interpret them is with plots in the probability metric.  Let me take the example of `tenure`.\n\nI will need to create data for interpretation.  Let's suppose we have a `DSL` user with phone service on a two year contract with average `TotalCharges`.  The last thing I need to know is what values of `tenure` to show.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\nlibrary(skimr)\nChurn %>%\n    filter(InternetService == \"DSL\", PhoneService == \"Yes\", Contract == \"Two year\") %>%\n    skim(tenure, TotalCharges)\n```\n````\n\n::: {.cell-output-display}\nTable: Data summary\n\n|                         |           |\n|:------------------------|:----------|\n|Name                     |Piped data |\n|Number of rows           |467        |\n|Number of columns        |23         |\n|_______________________  |           |\n|Column type frequency:   |           |\n|numeric                  |2          |\n|________________________ |           |\n|Group variables          |None       |\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate|    mean|      sd|    p0|     p25|     p50|     p75|    p100|hist  |\n|:-------------|---------:|-------------:|-------:|-------:|-----:|-------:|-------:|-------:|-------:|:-----|\n|tenure        |         0|          1.00|   60.52|   14.79|   0.0|   55.00|   66.00|   71.00|   72.00|▁▁▁▂▇ |\n|TotalCharges  |         3|          0.99| 4733.47| 1382.51| 130.5| 3867.24| 4913.88| 5879.86| 6859.05|▁▂▅▇▇ |\n:::\n:::\n\n\nNow I can create the data and generate predictions in the probability metric of the response.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\nTenure.Pred <- data.frame(InternetService = \"DSL\", PhoneService = \"Yes\", Contract = \"Two year\",\n    TotalCharges = 4733.5, tenure = seq(0, 72, by = 1))\nTenure.Pred$Prob.Churn <- predict(my.logit, newdata = Tenure.Pred, type = \"response\")\n```\n````\n:::\n\n\nNow let me plot it.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\nggplot(Tenure.Pred) + aes(x = tenure, y = Prob.Churn) + geom_line() + theme_minimal()\n```\n````\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n:::\n\n\nWe could get fancier, too.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\nTenure.Pred.Three <- rbind(data.frame(InternetService = \"DSL\", PhoneService = \"Yes\",\n    Contract = \"Month-to-month\", TotalCharges = 4733.5, tenure = seq(0, 72, by = 1)),\n    data.frame(InternetService = \"DSL\", PhoneService = \"Yes\", Contract = \"One year\",\n        TotalCharges = 4733.5, tenure = seq(0, 72, by = 1)), data.frame(InternetService = \"DSL\",\n        PhoneService = \"Yes\", Contract = \"Two year\", TotalCharges = 4733.5, tenure = seq(0,\n            72, by = 1)))\nTenure.Pred.Three$Prob.Churn <- predict(my.logit, newdata = Tenure.Pred.Three, type = \"response\")\nggplot(Tenure.Pred.Three) + aes(x = tenure, y = Prob.Churn, color = Contract) + geom_line() +\n    theme_minimal() + labs(y = \"Pr(Churn)\")\n```\n````\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n:::\n\n\n# A Better (Businessy) Way of Thinking About all of This\n\nI personally believe that the only real way to assess models for use in **predictive analytics** is to assess them by that criteria.  That doesn't mean fitting inside the extant sample of data, but rather sampling from it and then using the model to predict what is known as a **holdout sample**.  Let me show you what I mean.  In this case, let me use the probit and logit models from before and a 75/25 split.  This means that I will analyse 75 percent and predict the other 25 percent.  I can use `join` style commands to pull it off pretty simply.  I have 7043 rows.  So I want 5283 rows of the original data out of that 7043.\n\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\ntrain <- Churn[sample(c(1:7043), size = 5283, replace = FALSE), ]\ntest <- Churn %>%\n    anti_join(., train)\n```\n````\n:::\n\n\nNow to estimate the model.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\nlibrary(janitor)\nmod.train <- train %>%\n    glm(ChurnF ~ InternetService + tenure + PhoneService + Contract + TotalCharges,\n        family = binomial(link = \"probit\"), data = .)\n```\n````\n:::\n\n\nNow to predict the result on the `test` set that I created.  I will then turn the probabilities into a best guess by whether `Churn` or `No` is more likely.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\ntest$Pred.Probs <- predict(mod.train, newdata = test, type = \"response\")\ntest %>%\n    mutate(Pred.Val = (Pred.Probs > 0.5)) %>%\n    janitor::tabyl(Churn, Pred.Val, show_na = FALSE) %>%\n    adorn_percentages(\"row\")\n```\n````\n\n::: {.cell-output .cell-output-stdout}\n```\n Churn     FALSE      TRUE\n    No 0.8961749 0.1038251\n   Yes 0.5042017 0.4957983\n```\n:::\n:::\n\n\nThen all of the totals.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\ntest %>%\n    mutate(Pred.Val = (Pred.Probs > 0.5)) %>%\n    janitor::tabyl(Churn, Pred.Val, show_na = FALSE) %>%\n    adorn_totals(c(\"row\", \"col\"))\n```\n````\n\n::: {.cell-output .cell-output-stdout}\n```\n Churn FALSE TRUE Total\n    No  1148  133  1281\n   Yes   240  236   476\n Total  1388  369  1757\n```\n:::\n:::\n\n\nNow you might say that the fact we can only get 50 to 55 percent of `Churn='Yes'` with the model, remember that only 26.5 percent of people `Churn` overall so we have improved quite a bit over knowing nothing at all but the raw row probability.  In this specific case, the probability of `Yes` in the test set is shown below.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\ntest %>%\n    tabyl(Churn)\n```\n````\n\n::: {.cell-output .cell-output-stdout}\n```\n Churn    n   percent\n    No 1284 0.7295455\n   Yes  476 0.2704545\n```\n:::\n:::\n\n\n## Quadratic Terms?\n\nWhat would happen if I assume that the effect of `tenure` is not a line but instead has some curvature.\n\n\n\n````{.cell-code}\n```{{r, results='asis', warning=FALSE, message=FALSE}}\nmod.train.SQ <- train %>%\n    glm(ChurnF ~ InternetService + tenure + I(tenure^2) + PhoneService + Contract +\n        TotalCharges, family = binomial(link = \"probit\"), data = .)\nstargazer(mod.train, mod.train.SQ, type = \"html\", style = \"apsr\")\n```\n````\n\n\n<table style=\"text-align:center\"><tr><td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\"></td><td colspan=\"2\">ChurnF</td></tr>\n<tr><td style=\"text-align:left\"></td><td>(1)</td><td>(2)</td></tr>\n<tr><td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\">InternetServiceFiber optic</td><td>0.703<sup>***</sup></td><td>0.761<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.061)</td><td>(0.062)</td></tr>\n<tr><td style=\"text-align:left\">InternetServiceNo</td><td>-0.520<sup>***</sup></td><td>-0.566<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.082)</td><td>(0.083)</td></tr>\n<tr><td style=\"text-align:left\">tenure</td><td>-0.030<sup>***</sup></td><td>-0.043<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.004)</td><td>(0.004)</td></tr>\n<tr><td style=\"text-align:left\">I(tenure2)</td><td></td><td>0.0003<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td></td><td>(0.0001)</td></tr>\n<tr><td style=\"text-align:left\">PhoneServiceYes</td><td>-0.296<sup>***</sup></td><td>-0.262<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.085)</td><td>(0.085)</td></tr>\n<tr><td style=\"text-align:left\">ContractOne year</td><td>-0.467<sup>***</sup></td><td>-0.465<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.067)</td><td>(0.067)</td></tr>\n<tr><td style=\"text-align:left\">ContractTwo year</td><td>-0.927<sup>***</sup></td><td>-1.067<sup>***</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.100)</td><td>(0.105)</td></tr>\n<tr><td style=\"text-align:left\">TotalCharges</td><td>0.0001<sup>***</sup></td><td>0.00005</td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.00004)</td><td>(0.00004)</td></tr>\n<tr><td style=\"text-align:left\">Constant</td><td>0.086</td><td>0.171<sup>**</sup></td></tr>\n<tr><td style=\"text-align:left\"></td><td>(0.081)</td><td>(0.082)</td></tr>\n<tr><td style=\"text-align:left\">N</td><td>5,275</td><td>5,275</td></tr>\n<tr><td style=\"text-align:left\">Log Likelihood</td><td>-2,235.094</td><td>-2,220.230</td></tr>\n<tr><td style=\"text-align:left\">AIC</td><td>4,486.189</td><td>4,458.460</td></tr>\n<tr><td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td colspan=\"3\" style=\"text-align:left\"><sup>*</sup>p < .1; <sup>**</sup>p < .05; <sup>***</sup>p < .01</td></tr>\n</table>\n\n\nAs we can see from the table, the curvature appears to be different from zero though interpreting such a thing ceteris paribus is probably nonsense.  Maybe better to see what happens in the metric of the predicted probability.  Let me recycle the prediction data I used to draw this in the earlier section.  What would the two predictions look like and how do they differ?\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\nTenure.Pred$Prob.Churn.2 <- predict(mod.train, newdata = Tenure.Pred, type = \"response\")\nTenure.Pred$Prob.Churn.Sq <- predict(mod.train.SQ, newdata = Tenure.Pred, type = \"response\")\nggplot(Tenure.Pred) + aes(x = tenure, y = Prob.Churn.Sq) + geom_line() + geom_line(aes(y = Prob.Churn.2),\n    color = \"purple\") + theme_minimal() + labs(y = \"Pr(Churn)\")\n```\n````\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-35-1.png){width=672}\n:::\n:::\n\n\n\n\nNow let's predict for the test set, does it really do better?\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\ntest$Pred.Probs.Sq <- predict(mod.train, newdata = test, type = \"response\")\ntest %>%\n    mutate(Pred.Val.Sq = (Pred.Probs.Sq > 0.5)) %>%\n    janitor::tabyl(Churn, Pred.Val.Sq, show_na = FALSE) %>%\n    adorn_percentages(\"row\")\n```\n````\n\n::: {.cell-output .cell-output-stdout}\n```\n Churn     FALSE      TRUE\n    No 0.8961749 0.1038251\n   Yes 0.5042017 0.4957983\n```\n:::\n:::\n\n\nNot usually.  Such people are really unlikely to Churn no matter what; it only starts at about 0.25.\n\n## A final note: a classification tree\n\nFirst, I will start with a generic classification tree with everything set to the defaults.  Then I will look at a report to refine it.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\nlibrary(rpart)\nlibrary(rpart.plot)\nfit.BT <- rpart(ChurnF ~ InternetService + tenure + PhoneService + Contract + TotalCharges,\n    data = train, method = \"class\")\nrpart.plot(fit.BT)\n```\n````\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-37-1.png){width=672}\n:::\n:::\n\n\nHow well does it fit the test sample?\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\ntest$Churn.No <- predict(fit.BT, newdata = test)[, 1]\ntest$Churn.PredRT <- (test$Churn.No < 0.5)\ntest %>%\n    tabyl(Churn, Churn.PredRT)\n```\n````\n\n::: {.cell-output .cell-output-stdout}\n```\n Churn FALSE TRUE\n    No  1210   74\n   Yes   319  157\n```\n:::\n:::\n\n\nNot very well.  We can alter the tolerance for complexity using some diagnostics about the tree.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\nprintcp(fit.BT)\n```\n````\n\n::: {.cell-output .cell-output-stdout}\n```\n\nClassification tree:\nrpart(formula = ChurnF ~ InternetService + tenure + PhoneService + \n    Contract + TotalCharges, data = train, method = \"class\")\n\nVariables actually used in tree construction:\n[1] Contract        InternetService tenure         \n\nRoot node error: 1393/5283 = 0.26368\n\nn= 5283 \n\n        CP nsplit rel error  xerror     xstd\n1 0.056712      0   1.00000 1.00000 0.022991\n2 0.010000      3   0.78177 0.78823 0.021172\n```\n:::\n:::\n\nThe option `cp` controls a complexity parameter that keeps the tree from overfitting the tree.  I want to show a fairly complex one so I will change from the default of 0.01 to 0.0025.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\nfit.BT.2 <- rpart(ChurnF ~ InternetService + tenure + PhoneService + Contract + TotalCharges,\n    data = train, method = \"class\", cp = 0.0025)\nrpart.plot(fit.BT.2, extra = 106)\n```\n````\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-40-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\ntest$Churn.No <- predict(fit.BT.2, newdata = test)[, 1]\ntest$Churn.PredRT2 <- (test$Churn.No < 0.5)\ntest %>%\n    tabyl(Churn, Churn.PredRT2)\n```\n````\n\n::: {.cell-output .cell-output-stdout}\n```\n Churn FALSE TRUE\n    No  1154  130\n   Yes   251  225\n```\n:::\n:::\n\n\nIn this case, we have 1379 correct with the big tree and 1367 right with the smaller tree.\n\n[^1]: McCullagh, P. and Nelder, J.A. (1989) __Generalized Linear Models. 2nd Edition__, Chapman and Hall, London.\n[http://dx.doi.org/10.1007/978-1-4899-3242-6](http://dx.doi.org/10.1007/978-1-4899-3242-6)\n\n[^2]: Full disclosure, I am cheating a bit here.  I don't really want to explain the fitting of generalized linear models as it most often involves iteratively reweighted least squares.  I prefer to motivate them with the far more intuitive likelihood approach though they are not, strictly speaking, identical.",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}