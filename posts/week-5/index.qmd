---
title: "Week 5: Hierarchical Data"
author: "Robert W. Walker"
date: "2022-09-26"
categories: [R]
image: "image.png"
toc: true
execute: 
  echo: fenced
---

The slides [are here.](https://robertwwalker.github.io/xaringan/CMF-Week-5/).

Our fifth class meeting will focus on [Chapter 8](https://peopleanalytics-regression-book.org/gitbook/modeling-explicit-and-latent-hierarchy-in-data.html) of __Handbook of Regression Modeling in People Analytics__. 

## The Skinny

Hierarchical models represent an advance on more standard linear and generalized linear models with the recognition that data have hierarchical forms of organization with varying degrees of freedom for the predictors.  These models can, generically, be combined with techniques that we have already learned to expand the range of our toolkit.  Since we left last week off with ordered models, they will first occupy our attention.

## Ordered Models

My preferred method of thinking about ordered regression involves latent variables.  So what is a latent variable?  It is something that is unobservable, hence latent, and we only observe coarse realizations in the form of qualitative categories.  Consider the example from [Li in the *Journal of Politics*](https://www.journals.uchicago.edu/doi/abs/10.1111/j.1468-2508.2006.00370.x).

![Li Abstract](./data/Screen Shot 2022-09-19 at 9.59.09 AM.png)



## The Outcome

The outcome is summed from six individual types of incentives.  They are explained here.

![Tax Incentives to FDI](./data/Screen Shot 2022-09-19 at 10.09.08 AM.png)

and

![Tax Incentives Part 2](./data/Screen Shot 2022-09-19 at 10.10.30 AM.png)

## Inputs

There are two parts to the data description for the inputs.

![Part 1](./data/Screen Shot 2022-09-19 at 10.01.12 AM.png)

and ![Part 2](./data/Screen Shot 2022-09-19 at 10.01.32 AM.png)

and there is a further description of other variables that are deployed.

![Controls: Part 1](./data/Screen Shot 2022-09-19 at 10.17.41 AM.png)

and

![Controls: Part 2](./data/Screen Shot 2022-09-19 at 10.18.53 AM.png)

## The Data

This should give us an idea of what is going on.  The data come in Stata format; we can read these via the `foreign` or `haven` libraries in R.

```{r}
library(MASS); library(foreign)
Li.Data <- read.dta("./data/li-replication.dta")
table(Li.Data$generosityg)
Li.Data$generositygF <- as.factor(Li.Data$generosityg)
```
It is worthwhile to notice that the top of the scale is rather sparse.

There is also a concern about FDI and economies of scale.  The following is a plot of the relationship between FDI and size of the economy in the sample.

![FD-Size](./data/size-fdi.png)

Without careful attention to normalization, China is a clear `x-y` outlier.

## Motivating the Model

Suppose there is some unobserved continuous variable, call it $y^{*}$ that measures the willingness/utility to be derived from tax incentives to FDI.  Unfortunately, this latent quantity is unobservable; we instead observe how many incentives are offered and posit that the number of incentives is a manifestation of increasing utility with unknown points of separation -- cutpoints -- that separate these latent utilities into a mutually exclusive and exhaustive partition.  In a simplified example, consider this.

```{r}
plot(density(rlogis(1000)))
abline(v=c(-3,-2,-1,0,2,4))
```

So anything below -3 is zero incentives; anything between -3 and -2 is one incentive, ... , and anything above 4 should be all six incentives.  What we have is a regression problem but the outcome is unobserved and takes the form of a logistic random variable.  Indeed, one could write the equation as:

$$y^{*} = X\beta + \epsilon$$
where $\epsilon$ is assumed to have a logistic distribution but this is otherwise just a linear regression.  Indeed, the direct interpretation of the slopes is the effect of a one-unit change in X on that logistic random variable.

## What to Replicate

The table of estimates is presented in the paper; I will copy it here.

![Results Table](./data/Screen Shot 2022-09-19 at 10.24.19 AM.png)

I will choose two of a few models estimated in the paper.  First, let us have a look at Model 1.

```{r}
li.mod1 <- polr(generositygF ~ law00log + transition, data=Li.Data)
summary(li.mod1)
```

We can read these by stars.  There is nothing that is clearly different from zero as a slope or 1 as an odds-ratio.  The authors deploy a common strategy for adjusting standard errors that, in this case, is necessary to find a relationship with statistical confidence.  That's a diversion.  To the story.  In general, the sign of the rule of law indicator is negative, so as rule of law increases, incentives decrease though we cannot rule out no effect.  Transitions also have a negative sign; regime changes have no clear influence on incentives.  There is additional information that is commonly given short-shrift.  What do the cutpoints separating the categories imply?  Let's think this through recongizing that the estimates have an underlying t/normal distribution.  `4|5` is within one standard error of both `3|4` and `5|6`.  The model cannot really tell these values apart.  Things do improve in the lower part of the scale but we should note that this is where the vast majority of the data are actually observed.

### Odds Ratios

Next, I will turn the estimates into odds-ratios by exponentiating the estimates.

```{r}
exp(li.mod1$coefficients)
```

For Kawika, this is one of the many cases that I am familiar with where `robust` is necessary to find something.  Note neither effect can be differentiated from zero with much confidence at all.  To further examine the claims, I will also replicate the right-most column.

## Column 4 Estimates

```{r}
li.mod4 <- polr(generositygF ~ law00log + transition + fdiinf + democfdi + democ + autocfdi2 + autocfdir + reggengl + reggengl2 + gdppclog + gdplog, data=Li.Data)
summary(li.mod4)
```

Measured via odds-ratios, we can obtain those:

```{r}
exp(li.mod4$coefficients)
```

## Diagnostics and Commentary

Goodness of Fit:

```{r}
DescTools::PseudoR2(
  li.mod1, 
  which = c("McFadden", "CoxSnell", "Nagelkerke", "AIC")
)
DescTools::PseudoR2(
  li.mod4, 
  which = c("McFadden", "CoxSnell", "Nagelkerke", "AIC")
)
```
The last model is clearly better than the first by any of these measures.  That said, there are a lot of additional predictors that add much complexity to the model and the difference in AIC is not very large.

What about the others?

```{r}
# lipsitz test 
# generalhoslem::lipsitz.test(li.mod1)
# generalhoslem::lipsitz.test(li.mod4)
```

They fail to work because of sparseness.

### Testing Proportional-Odds

The text cites a test that owes to Brant on examining proportionality.  It turns out that I know a bit about this; I published a purely theoretical stats paper showing that it is not at all clear what the alternative hypothesis embodied in this test actually means because the only model with a proper probability distribution for $y^{*}$ is this proportional-odds model.

I will follow the text with this caveat in mind:

```{r}
brant::brant(li.mod1)
```

### How Well Does the Model Predict?

```{r}
Mat.Fit <- data.frame(fitted(li.mod4))
library(tidyverse)
Mat.Fit$pred.val <- rep(-999, 51)
Mat.Fit$pred.val[Mat.Fit$X0 > Mat.Fit$X1 & Mat.Fit$X0 > Mat.Fit$X2 & Mat.Fit$X0 > Mat.Fit$X3 & Mat.Fit$X0 > Mat.Fit$X4 & Mat.Fit$X0 > Mat.Fit$X5 & Mat.Fit$X0 > Mat.Fit$X6] <- 0
Mat.Fit$pred.val[Mat.Fit$X1 > Mat.Fit$X0 & Mat.Fit$X1 > Mat.Fit$X2 & Mat.Fit$X1 > Mat.Fit$X3 & Mat.Fit$X1 > Mat.Fit$X4 & Mat.Fit$X1 > Mat.Fit$X5 & Mat.Fit$X1 > Mat.Fit$X6] <- 1
Mat.Fit$pred.val[Mat.Fit$X2 > Mat.Fit$X0 & Mat.Fit$X2 > Mat.Fit$X1 & Mat.Fit$X2 > Mat.Fit$X3 & Mat.Fit$X2 > Mat.Fit$X4 & Mat.Fit$X2 > Mat.Fit$X5 & Mat.Fit$X2 > Mat.Fit$X6] <- 2
Mat.Fit$pred.val[Mat.Fit$X3 > Mat.Fit$X0 & Mat.Fit$X3 > Mat.Fit$X1 & Mat.Fit$X3 > Mat.Fit$X2 & Mat.Fit$X3 > Mat.Fit$X4 & Mat.Fit$X3 > Mat.Fit$X5 & Mat.Fit$X3 > Mat.Fit$X6] <- 3
Mat.Fit$pred.val[Mat.Fit$X5 > Mat.Fit$X0 & Mat.Fit$X5 > Mat.Fit$X1 & Mat.Fit$X5 > Mat.Fit$X2 & Mat.Fit$X5 > Mat.Fit$X3 & Mat.Fit$X5 > Mat.Fit$X4 & Mat.Fit$X5 > Mat.Fit$X6] <- 5
Mat.Fit$pred.val[Mat.Fit$X6 > Mat.Fit$X0 & Mat.Fit$X6 > Mat.Fit$X1 & Mat.Fit$X6 > Mat.Fit$X2 & Mat.Fit$X6 > Mat.Fit$X3 & Mat.Fit$X6 > Mat.Fit$X4 & Mat.Fit$X6 > Mat.Fit$X5] <- 6
Pred.Data <- Li.Data[c(1:28,30:41,43:53),]
table(Pred.Data$generosityg,Mat.Fit$pred.val)
```

So `6+12+4+1` or 23 of 51 are correctly predicted with a rather big and complicated model.

### On AIC

The AIC [and BIC] are built around the idea of likelihood presented last time.  The formal definition, [which is correct on Wikipedia](https://en.wikipedia.org/wiki/Akaike_information_criterion) explains the following:

![AIC](./data/Screen Shot 2022-09-19 at 12.06.24 PM.png)

## Hierarchical Models and SEM


### Hierarchical Models

To examine a hierarchical model, I am going to choose some interesting data on popularity.  A description appears below; these data come from an Intro to Multilevel Analysis.

![Popularity Data](./data/Screen Shot 2022-09-26 at 2.13.53 PM.png)

Though the data are technically ordered, this feature is not exploited to build a hierarchical ordered regression model, though it could be done.  Instead, the outcome of interest is an average of *ordered scales*.

### Load the data

```{r}
library(tidyverse)
library(haven)
popular2data <- read_sav(file ="https://github.com/MultiLevelAnalysis/Datasets-third-edition-Multilevel-book/blob/master/chapter%202/popularity/SPSS/popular2.sav?raw=true")
popular2data <- popular2data %>% dplyr::select(pupil, class, extrav, sex, texp, popular)
```


### A Summary

```{r}
summary(popular2data)
head(popular2data)
```

### A plot of the relationship of interest

```{r}
ggplot(data    = popular2data,
       aes(x   = extrav,
           y   = popular,
           col = class))+ #to add the colours for different classes
  geom_point(size     = 0.8,
             alpha    = .8,
             position = "jitter")+ #to add some random noise for plotting purposes
  theme_minimal()+
  theme(legend.position = "none")+
  scale_color_gradientn(colours = rainbow(100))+
  labs(title    = "Popularity vs. Extraversion",
       subtitle = "add colours for different classes",
       x = "Extroversion",
       y = "Average Popularity")
```

### With the lines

```{r}
ggplot(data      = popular2data,
       aes(x     = extrav,
           y     = popular,
           col   = class,
           group = class))+ #to add the colours for different classes
  geom_point(size     = 1.2,
             alpha    = .8,
             position = "jitter")+ #to add some random noise for plotting purposes
  theme_minimal()+
  theme(legend.position = "none")+
  scale_color_gradientn(colours = rainbow(100))+
  geom_smooth(method = lm,
              se     = FALSE,
              size   = .5, 
              alpha  = .8)+ # to add regression line
  labs(title    = "Popularity vs. Extraversion",
       subtitle = "add colours for different classes and regression lines",
       x = "Extroversion",
       y = "Average Popularity")
```

### A regression

```{r}
ggplot(data = popular2data, 
       aes(x   = extrav,
           y   = popular, 
           col = as.factor(sex)))+
  geom_point(size     = 1, 
             alpha    = .7, 
             position = "jitter")+
  geom_smooth(method   = lm,
              se       = T, 
              size     = 1.5, 
              linetype = 1, 
              alpha    = .7)+
  theme_minimal()+
  labs(title    = "Popularity and Extraversion for 2 Genders", 
       subtitle = "The linear relationship between the two is similar for both genders")+
  scale_color_manual(name   =" Gender",
                     labels = c("Boys", "Girls"),
                     values = c("lightblue", "pink"))
```
A model with random intercepts

```{r, warning=FALSE, message=FALSE}
library(lme4)
options(scipen=7)
library(lmerTest)
model1 <- lmer(formula = popular ~ 1 + sex + extrav + (1|class), 
               data    = popular2data)
summary(model1)
```

Though in this case, we probably do not need them but p-values can be obtained from `lmerTest`.  The standard `lme4` summary does not have them.

Now let's add a **second-level** predictor.  Teacher experience does not vary within a given classroom, only across the 100 classrooms.  Let's look at this model.

```{r}
model2 <- lmer(popular ~ 1 + sex + extrav + texp + (1 | class), data=popular2data)
summary(model2)
```

More experienced teachers lead to higher reported average popularity.

### Random slopes

```{r, message=FALSE, warning=FALSE}
model3 <- lmer(formula = popular ~ 1 + sex + extrav + texp + (1 + sex + extrav | class),
               data    = popular2data, control=lmerControl(optCtrl=list(maxfun=100000) ))
summary(model3)
```
### Examining the Model

```{r}
ranova(model3)
```

The random effect associated with `sex` is not close to significance.

### A Crossed-Effects Model

```{r}
model5<-lmer(formula = popular ~ 1 + sex + extrav + texp+ extrav*texp + (1 + extrav | class), 
             data    = popular2data)
summary(model5)
```
### A Picture

```{r}
ggplot(data = popular2data,
       aes(x = extrav, 
           y = popular, 
           col = as.factor(texp)))+
  viridis::scale_color_viridis(discrete = TRUE)+
  geom_point(size     = .7,
             alpha    = .8, 
             position = "jitter")+
  geom_smooth(method = lm,
              se     = FALSE,
              size   = 1,
              alpha  = .4)+
  theme_minimal()+
  labs(title    = "Interaction btw. Experience and Extraversion", 
       subtitle = "The relationship changes", 
       col      = "Years of\nTeacher\nExperience")
```


## Structural Equations Models

A few weeks ago, Jack mentioned the use of principal components as a means for combining collinear variables.  There is a more general language for describing models of this sort.  The following example will play off of work I am currently finishing up with Elliot Maltz and a co-author.

First, the data.

```{r}
library(lavaan)
load(url("https://github.com/robertwwalker/ChoiceAndForecasting/raw/main/posts/week-5/data/EMData.RData"))
```

There is a ton of data in here.  Let me pay particular attention to specific parts we are interested in.

### Agentic

```{r}
names(EMData)[[76]]
table(EMData.Anonymous$...76)
names(EMData)[[77]]
table(EMData.Anonymous$...77)
names(EMData)[[78]]
table(EMData.Anonymous$...78)
names(EMData)[[79]]
table(EMData.Anonymous$...79)
AB <- cfa('Agentic =~ ...76 + ...77 + ...78 + ...79', data=EMData.Anonymous, ordered = TRUE)
summary(AB, fit.measures = TRUE, standardized = TRUE)
```


### Communal

```{r}
names(EMData)[[80]]
table(EMData.Anonymous$...80)
names(EMData)[[81]]
table(EMData.Anonymous$...81)
names(EMData)[[84]]
table(EMData.Anonymous$...84)
CB <- cfa('Communal =~ ...80 + ...81 + ...84', data=EMData.Anonymous, ordered = TRUE)
summary(CB, fit.measures = TRUE, standardized = TRUE)
```


### Mentoring

```{r}
names(EMData)[[13]]
table(EMData.Anonymous$...13)
names(EMData)[[14]]
table(EMData.Anonymous$...14)
names(EMData)[[15]]
table(EMData.Anonymous$...15)
M <- cfa('Mentoring =~ ...13 + ...14 + ...15', data=EMData.Anonymous, ordered = TRUE)
summary(M, fit.measures = TRUE, standardized = TRUE)
```


### Social Influence

```{r}
names(EMData)[[37]]
table(EMData.Anonymous$...37)
names(EMData)[[38]]
table(EMData.Anonymous$...38)
names(EMData)[[39]]
table(EMData.Anonymous$...39)
SI <- cfa('Social.Influence =~ ...37 + ...38 + ...39', data=EMData.Anonymous, ordered = TRUE)
summary(SI, fit.measures = TRUE, standardized = TRUE)
```

```{r}
library(lavaanPlot)
lavaanPlot(model = SI, node_options = list(shape = "box", fontname = "Helvetica"), edge_options = list(color = "grey"), coefs = TRUE, covs = TRUE)
```


### An SEM

Now let me combine those `measurement` models to produce a set of two structural equations.  I wish to explain income and employment given these factors.

```{r, warning=FALSE, message=FALSE}
names(EMData)[c(5,59)]
Struct <- sem('Agentic =~ ...76 + ...77 + ...78 + ...79
          Communal =~ ...80 + ...81 + ...84
          Mentoring =~ ...13 + ...14 + ...15
          Social.Influence =~ ...37 + ...38 + ...39
          ...59 ~ Agentic + Communal + Mentoring + Social.Influence
          ...5 ~ Agentic + Communal + Mentoring + Social.Influence', data=EMData.Anonymous, ordered = c("...13","...14", "...15", "...80","...81", "...84", "...76","...77", "...78", "...79","...37", "...38", "...39"))
summary(Struct, fit.measures=TRUE, standardized=TRUE)
```

```{r}
lavaanPlot(model=Struct, node_options = list(shape = "box", fontname = "Helvetica"), edge_options = list(color = "grey"), coefs = TRUE, covs = TRUE)
```

