---
title: "A Small Thread on Smart Prediction"
author: "Robert W. Walker"
date: "2022-09-06"
categories: [R]
image: "image.png"
---

# Linear Regression

A linear regression example.  The data can be loaded from the web.

```{r}
# if needed, download ugtests data
url <- "http://peopleanalytics-regression-book.org/data/ugtests.csv"
ugtests <- read.csv(url)
str(ugtests)
```

There are 975 individuals graduating in the past three years from the biology department of a large academic institution.  We have data on four examinations: 

- a first year exam ranging from 0 to 100 (Yr1)
- a second year exam ranging from 0 to 200 (Yr2)
- a third year exam ranging from 0 to 200 (Yr3)
- a Final year exam ranging from 0 to 300 (Final)

```{r}
library(skimr); library(kableExtra); library(tidyverse)
skim(ugtests) %>% dplyr::filter(skim_type=="numeric") %>% kable()
```

## A Visual of the Data

```{r}
library(GGally)
# display a pairplot of all four columns of data
GGally::ggpairs(ugtests)
```
```{r}
my.lm <- lm(Final ~ Yr1 + Yr2 + Yr3, data=ugtests)
summary(my.lm)
```

## Basic In Sample Prediction

```{r}
ugtests %>% mutate(Fitted.Value = predict(my.lm)) %>% kable() %>% scroll_box(height="300px")
```

## Out of Sample Prediction

Suppose we have a student with a 55 on `Yr1`, a 95 on `Yr2` and a 110 on `Yr3`.

First, what would their data look like?

```{r}
Hypothetical.Student <- data.frame(Yr1=55, Yr2=95, Yr3=110)
Hypothetical.Student
```

Let's predict, first the single best guess -- the fitted value.

The equation we sought estimates for was:

```{r}
library(equatiomatic)
equatiomatic::extract_eq(my.lm)
```


And it was estimated to be

```{r}
equatiomatic::extract_eq(my.lm, use_coefs = TRUE, coef_digits = 4)
```

Our best guess for `Hypothetical.Student` must then be:

```{r}
14.145 + 0.076*55 + 0.4313*95 + 0.8657*110
```

What does `predict` produce?

```{r}
predict(my.lm, newdata = Hypothetical.Student)
```

The same thing except that mine by hand was restricted to four digits.

In exact form, we could produce, using matrix multiplication,

```{r}
c(summary(my.lm)$coefficients[,1])
c(1,Hypothetical.Student)
c(1,55,95,110)%*%c(summary(my.lm)$coefficients[,1])
```
Because that is all predict does.

### Confidence Interval

To produce confidence intervals, we can add the `interval` option.  For an interval of the predicted average, we have:

```{r}
predict(my.lm, newdata = Hypothetical.Student, interval="confidence")
```

### Prediction Interval

An interval of the predicted range of data, we have

```{r}
predict(my.lm, newdata = Hypothetical.Student, interval="prediction")
```

## Smart Prediction

One great thing about R is **smart prediction**.  So what does it do?  Let's try out the centering operations that I used.

```{r}
my.lm <- lm(Final ~ scale(Yr1, scale=FALSE) + scale(Yr2, scale=FALSE) + scale(Yr3, scale=FALSE), data=ugtests)
summary(my.lm)
```

```{r}
equatiomatic::extract_eq(my.lm, use_coefs = TRUE, coef_digits = 4)
```

Only the intercept is impacted; the original intercept was the expected `Final` for a student that had all zeroes [possible but a very poor performance] and they'd have expected a 14.15 [the original intercept].  After the centering operation for each predictor, the average student [mean scores on Yr1, Yr2, and Yr3] could expect a score of 148.96205 -- the intercept from the regression on centered data.

Now let's predict our `Hypothetical.Student`.

```{r}
predict(my.lm, newdata=Hypothetical.Student)
```

The result is the same.

```{r}
predict(my.lm, newdata=Hypothetical.Student, interval="confidence")
```

```{r}
predict(my.lm, newdata=Hypothetical.Student, interval="prediction")
```

This works because R knows that each variable was `centered`, the mean was subtracted because that is what scale does when the [unfortunately named] argument `scale` inside the function `scale` is set to FALSE `=>` we are not creating z-scores [the default is $\frac{x_{i} - \overline{x}}{sd(x)}$] but are just taking $x_{i} - \overline{x}$ or **centering the data**.

## Smart Prediction is Deployed Throughout R

Let me try a regression where Yr1, Yr2, and Yr3 are allowed to have effects with curvature using each term and its square.  

```{r}
my.lm.Sq <- lm(Final ~ Yr1 + Yr1^2 + Yr2 + Yr2^2 + Yr3 + Yr3^2, data=ugtests)
summary(my.lm.Sq)
```

Unfortunately, that did not actually include the squared terms, we still only have three lines.  We could use this:

```{r}
my.lm.Sq <- lm(Final ~ Yr1 + Yr1*Yr1 + Yr2 + Yr2*Yr2 + Yr3 + Yr3*Yr3, data=ugtests)
summary(my.lm.Sq)
```

But that also does not work.  The key is to give $R$ an object for the squared term that treats `Yr1`, `Yr2`, and `Yr3` as base terms to be squared; the function for this is `I`.

```{r}
my.lm.Sq <- lm(Final ~ Yr1 + I(Yr1^2) + Yr2 + I(Yr2^2) + Yr3 + I(Yr3^2), data=ugtests)
summary(my.lm.Sq)
```

Does the curvature improve fit?  We can use an F test.

```{r}
anova(my.lm, my.lm.Sq)
```

We cannot tell the two models apart so the squared terms do not improve the model.  That wasn't really the goal, it was to illustrate `Smart Prediction`.

What would we expect, given this new model, for our hypothetical student?

```{r}
predict(my.lm.Sq, newdata=Hypothetical.Student)
```
I will forego the intervals but it just works because R knows to square each of `Yr1`, `Yr2`, and `Yr3`.

## An Interaction Term

Suppose that `Yr1` and `Yr2` have an interactive effect.  We could use the `I()` construct but we do not have to.  The regression would be:

```{r}
my.lm.Int <- lm(Final ~ Yr1 + Yr2 + Yr1*Yr2 + Yr3, data=ugtests)
summary(my.lm.Int)
my.lm.I <- lm(Final ~ Yr1 + Yr2 + I(Yr1*Yr2) + Yr3, data=ugtests)
summary(my.lm.I)
```

It is worth noting that this does not improve the fit of the model.

```{r}
anova(my.lm, my.lm.I)
anova(my.lm, my.lm.Int)
```


The same holds for both:

```{r}
predict(my.lm.I, newdata=Hypothetical.Student)
predict(my.lm.Int, newdata=Hypothetical.Student)
```

## An Addendum on Diagnosing Quadratics

Let me generate some fake data to showcase use cases for quadratic functions of x as determinants of y.

### The True Model

I will generate $y$ according to the following equation.

$$y = x + 2*x^2 + \epsilon$$

where x is a sequence from -5 to 5 and $\epsilon$ is Normal(0,1).

```{r}
library(magrittr)
fake.df <- data.frame(x=seq(-5,5, by=0.05))
fake.df$y <- fake.df$x + 2*fake.df$x^2 + rnorm(201)
```

### A Plot

Let me plot x and y and include the estimated regression line [that does not include the squared term].

```{r}
fake.df %>% ggplot() + aes(x=x, y=y) + geom_point() + geom_smooth(method="lm")
```

To be transparent, here is the regression.

```{r}
summary(lm(y~x, data=fake.df))
```

Note just looking at the table would suggest we conclude that $y$ is a linear function of $x$ and this is, at best, partially true.  It is actually a quadratic function of $x$.  The fit is not very good, though.

What do the residuals look like?

```{r}
fake.df %<>% mutate(resid = lm(y~x, fake.df)$residuals)
fake.df %>% ggplot() + aes(x=x, y=resid) + geom_point() + theme_minimal() + labs(y="Residuals from linear regression with only x")
```

This is *a* characteristic pattern of a quadratic, *a* because the inflection point is at zero and $x$ can be both positive or negative.  Real world data is almost always a bit messier.  Nevertheless, I digress.  Let's look at the regression estimates including a squared term.

```{r}
summary(lm(y~x+I(x^2), fake.df))
```

Notice both the linear and the squared term are statistically different from zero and that the linear term has a much smaller standard error because it is far more precisely estimated.  What do the residuals now look like?

```{r}
fake.df %<>% mutate(resid.sq = lm(y~x+I(x^2), fake.df)$residuals)
fake.df %>% ggplot() + aes(x=x, y=resid.sq) + geom_point()
```

These are basically random with respect to $x$.

I should also point out that the residuals are well behaved now; they were not in the previous case.

```{r}
library(gvlma)
gvlma(lm(y~x, data=fake.df))
```

Versus

```{r}
gvlma(lm(y~x+I(x^2), data=fake.df))
```

